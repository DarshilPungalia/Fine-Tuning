{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2bf7d1a1",
      "metadata": {
        "id": "2bf7d1a1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import RecursiveUrlLoader, WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_transformers import BeautifulSoupTransformer\n",
        "from bs4 import BeautifulSoup\n",
        "import pickle\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "2b01fb93",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "cTXr_4EOvaet",
      "metadata": {
        "id": "cTXr_4EOvaet"
      },
      "outputs": [],
      "source": [
        "def bs4_extractor(html: str) -> str:\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    return soup.get_text()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "972dfa04",
      "metadata": {
        "id": "972dfa04"
      },
      "outputs": [],
      "source": [
        "loader  = RecursiveUrlLoader('https://python.langchain.com/api_reference/',\n",
        "                             max_depth=17,\n",
        "                             extractor=bs4_extractor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "bc4fcdb3",
      "metadata": {
        "id": "bc4fcdb3"
      },
      "outputs": [],
      "source": [
        "page = loader.lazy_load()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51d1d2a4",
      "metadata": {},
      "source": [
        "## Preprocessing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "JhKBHAGx2Owp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "JhKBHAGx2Owp",
        "outputId": "7207c0b7-13dd-4d63-f42f-9614533cb6cc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ibm'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base = ' https://python.langchain.com/api_reference/ibm/index.html'\n",
        "\n",
        "base_class = base.split('/')[4]\n",
        "base_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "14ffab1b",
      "metadata": {
        "id": "14ffab1b"
      },
      "outputs": [],
      "source": [
        "import unicodedata\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.encode('latin1', errors='ignore').decode('utf-8', errors='ignore')\n",
        "\n",
        "    text = unicodedata.normalize('NFKC', text)\n",
        "\n",
        "    text = ''.join(c for c in text if unicodedata.category(c)[0] != 'C')\n",
        "    text = re.sub(r\"LangChain\\s+documentation$\", \"\", text).strip()\n",
        "    text = text.split(\"|\")[0].strip()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "b402cde1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "b402cde1",
        "outputId": "2c950891-79e8-4500-d9ce-12d9c145b156"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'langchain-fireworks: 0.3.0'"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clean_text('langchain-fireworks: 0.3.0 √¢‚Ç¨‚Äù √∞≈∏¬¶≈ì√∞≈∏‚Äù‚Äî LangChain  documentation')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "QHkyjPn_8dUC",
      "metadata": {
        "id": "QHkyjPn_8dUC"
      },
      "outputs": [],
      "source": [
        "def remove_blank_lines(text):\n",
        "    lines = text.splitlines()\n",
        "    while lines and lines[0].strip() == \"\":\n",
        "        lines.pop(0)\n",
        "    while lines and lines[-1].strip() == \"\":\n",
        "        lines.pop()\n",
        "    lines = \"\\n\".join(lines)\n",
        "    return re.sub(r'\\n{3,}', '\\n\\n', lines.strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "f-3iy8-E8AbR",
      "metadata": {
        "id": "f-3iy8-E8AbR"
      },
      "outputs": [],
      "source": [
        "def extract_main_content(text, marker_leading=\"LangChain Python API Reference\", marker_trailing=\"¬© Copyright 2025, LangChain Inc.\"):\n",
        "    main_content = text.split(marker_leading, 1)[-1]\n",
        "    main_content = main_content.split(marker_trailing, 1)[0]\n",
        "    main_content = main_content.strip()\n",
        "    main_content = remove_blank_lines(main_content)\n",
        "    return main_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "ckrL8qnw90Et",
      "metadata": {
        "id": "ckrL8qnw90Et"
      },
      "outputs": [],
      "source": [
        "text = '''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "runnables ‚Äî ü¶úüîó LangChain  documentation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Skip to main content\n",
        "\n",
        "\n",
        "Back to top\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Ctrl+K\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    Reference\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Ctrl+K\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Docs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "GitHub\n",
        "\n",
        "\n",
        "\n",
        "X / Twitter\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Ctrl+K\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    Reference\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Docs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "GitHub\n",
        "\n",
        "\n",
        "\n",
        "X / Twitter\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Section Navigation\n",
        "Base packages\n",
        "\n",
        "Core\n",
        "Langchain\n",
        "agents\n",
        "callbacks\n",
        "chains\n",
        "chat_models\n",
        "embeddings\n",
        "evaluation\n",
        "globals\n",
        "hub\n",
        "indexes\n",
        "memory\n",
        "model_laboratory\n",
        "output_parsers\n",
        "retrievers\n",
        "runnables\n",
        "HubRunnable\n",
        "OpenAIFunction\n",
        "OpenAIFunctionsRouter\n",
        "\n",
        "\n",
        "smith\n",
        "storage\n",
        "\n",
        "\n",
        "Text Splitters\n",
        "Community\n",
        "Experimental\n",
        "\n",
        "Integrations\n",
        "\n",
        "AI21\n",
        "Anthropic\n",
        "AstraDB\n",
        "AWS\n",
        "Azure Ai\n",
        "Azure Dynamic Sessions\n",
        "Cerebras\n",
        "Chroma\n",
        "Cohere\n",
        "Deepseek\n",
        "Elasticsearch\n",
        "Exa\n",
        "Fireworks\n",
        "Google Community\n",
        "Google GenAI\n",
        "Google VertexAI\n",
        "Groq\n",
        "Huggingface\n",
        "IBM\n",
        "Milvus\n",
        "MistralAI\n",
        "MongoDB\n",
        "Neo4J\n",
        "Nomic\n",
        "Nvidia Ai Endpoints\n",
        "Ollama\n",
        "OpenAI\n",
        "Perplexity\n",
        "Pinecone\n",
        "Postgres\n",
        "Prompty\n",
        "Qdrant\n",
        "Redis\n",
        "Sema4\n",
        "Snowflake\n",
        "Sqlserver\n",
        "Standard Tests\n",
        "Tavily\n",
        "Together\n",
        "Unstructured\n",
        "Upstage\n",
        "VoyageAI\n",
        "Weaviate\n",
        "XAI\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "LangChain Python API Reference\n",
        "langchain: 0.3.25\n",
        "runnables\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "runnables#\n",
        "LangChain Runnable and the LangChain Expression Language (LCEL).\n",
        "The LangChain Expression Language (LCEL) offers a declarative method to build\n",
        "production-grade programs that harness the power of LLMs.\n",
        "Programs created using LCEL and LangChain Runnables inherently support\n",
        "synchronous, asynchronous, batch, and streaming operations.\n",
        "Support for async allows servers hosting the LCEL based programs\n",
        "to scale better for higher concurrent loads.\n",
        "Batch operations allow for processing multiple inputs in parallel.\n",
        "Streaming of intermediate outputs, as they‚Äôre being generated, allows for\n",
        "creating more responsive UX.\n",
        "This module contains non-core Runnable classes.\n",
        "Classes\n",
        "\n",
        "\n",
        "runnables.hub.HubRunnable\n",
        "An instance of a runnable stored in the LangChain Hub.\n",
        "\n",
        "runnables.openai_functions.OpenAIFunction\n",
        "A function description for ChatOpenAI\n",
        "\n",
        "runnables.openai_functions.OpenAIFunctionsRouter\n",
        "A runnable that routes to the selected function.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      ¬© Copyright 2025, LangChain Inc.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "text = extract_main_content(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "rO1AjSyV-IVS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rO1AjSyV-IVS",
        "outputId": "4a97667f-f194-4c06-cc3a-9d4db7f9d5db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "langchain: 0.3.25\n",
            "runnables\n",
            "\n",
            "runnables#\n",
            "LangChain Runnable and the LangChain Expression Language (LCEL).\n",
            "The LangChain Expression Language (LCEL) offers a declarative method to build\n",
            "production-grade programs that harness the power of LLMs.\n",
            "Programs created using LCEL and LangChain Runnables inherently support\n",
            "synchronous, asynchronous, batch, and streaming operations.\n",
            "Support for async allows servers hosting the LCEL based programs\n",
            "to scale better for higher concurrent loads.\n",
            "Batch operations allow for processing multiple inputs in parallel.\n",
            "Streaming of intermediate outputs, as they‚Äôre being generated, allows for\n",
            "creating more responsive UX.\n",
            "This module contains non-core Runnable classes.\n",
            "Classes\n",
            "\n",
            "runnables.hub.HubRunnable\n",
            "An instance of a runnable stored in the LangChain Hub.\n",
            "\n",
            "runnables.openai_functions.OpenAIFunction\n",
            "A function description for ChatOpenAI\n",
            "\n",
            "runnables.openai_functions.OpenAIFunctionsRouter\n",
            "A runnable that routes to the selected function.\n"
          ]
        }
      ],
      "source": [
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3404155",
      "metadata": {},
      "source": [
        "## Dataset Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "ef6364c3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef6364c3",
        "outputId": "3b9b822b-4449-42fa-8cf0-529dd362d325"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing documents: 775it [04:01,  3.20it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "documentation = defaultdict(list)\n",
        "\n",
        "for i, document in enumerate(tqdm(page, desc=\"Processing documents\")):\n",
        "    content = document.page_content\n",
        "    content = extract_main_content(content)\n",
        "    base_class = document.metadata['source']\n",
        "    class_list = base_class.split('/')\n",
        "    root_topic, topic = class_list[4], class_list[-1].replace('.html', '')\n",
        "    if root_topic == '':\n",
        "        root_topic = 'Langchain'\n",
        "    documentation[root_topic].append(f\"\\n\\n## Class Objects: {topic}\\n{content}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "CYSjDLb6_ehg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CYSjDLb6_ehg",
        "outputId": "6dcbf44e-2d54-4ae3-f3ee-8482ccd20b90"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(len(documentation['google_genai']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "Vcif5OivDRoL",
      "metadata": {
        "id": "Vcif5OivDRoL"
      },
      "outputs": [],
      "source": [
        "documentation = {\n",
        "    topic: \"\\n\".join(contents)\n",
        "    for topic, contents in documentation.items()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "R1TvDgToDe0k",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1TvDgToDe0k",
        "outputId": "be5b2b40-6a1f-464d-aaee-11bd1b331aff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'str'>\n",
            "144531\n"
          ]
        }
      ],
      "source": [
        "print(type(documentation['google_genai']))\n",
        "print(len(documentation['google_genai']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "f6FKC-o6Hkn1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6FKC-o6Hkn1",
        "outputId": "65b2c912-1ec9-4e93-ab17-aba86dfb8747"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['Langchain', 'perplexity', 'google_genai', 'ollama', 'chroma', 'pinecone', 'reference.html', 'nvidia_ai_endpoints', 'upstage', 'aws', 'anthropic', 'fireworks', '_static', 'cerebras', 'search.html', 'sqlserver', 'redis', '_modules', 'prompty', 'text_splitters', 'standard_tests', 'mistralai', 'mongodb', 'together', 'groq', 'cohere', 'experimental', 'nomic', 'openai', 'azure_dynamic_sessions', 'postgres', 'milvus', 'snowflake', 'neo4j', 'xai', 'unstructured', 'qdrant', 'tavily', 'astradb', 'community', 'ibm', 'core', 'google_vertexai', 'azure_ai', 'huggingface', 'elasticsearch', 'google_community', 'langchain', 'weaviate', 'ai21', 'deepseek', 'exa', 'voyageai', 'index.html', 'sema4'])"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documentation.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "WAHsJOmgD3c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "WAHsJOmgD3c8",
        "outputId": "750589c6-73c5-45e9-c429-fe414ff0ab3d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\n## Class Objects: search\\nSearch - ü¶úüîó LangChain  documentation\\n\\nSkip to main content\\n\\nBack to top\\n\\nCtrl+K\\n\\n    Reference\\n  \\n\\nCtrl+K\\n\\nDocs\\n\\nGitHub\\n\\nX / Twitter\\n\\nCtrl+K\\n\\n    Reference\\n  \\n\\nDocs\\n\\nGitHub\\n\\nX / Twitter\\n\\nSearch\\n\\nError\\nPlease activate JavaScript to enable the search functionality.\\n\\nCtrl+K'"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documentation.pop('_static')\n",
        "documentation.pop('index.html')\n",
        "documentation.pop('search.html')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "Eg3TkVlwIb-8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eg3TkVlwIb-8",
        "outputId": "664c8e23-f650-445b-ebc0-98ad6fac9be8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['Langchain', 'perplexity', 'google_genai', 'ollama', 'chroma', 'pinecone', 'reference.html', 'nvidia_ai_endpoints', 'upstage', 'aws', 'anthropic', 'fireworks', 'cerebras', 'sqlserver', 'redis', '_modules', 'prompty', 'text_splitters', 'standard_tests', 'mistralai', 'mongodb', 'together', 'groq', 'cohere', 'experimental', 'nomic', 'openai', 'azure_dynamic_sessions', 'postgres', 'milvus', 'snowflake', 'neo4j', 'xai', 'unstructured', 'qdrant', 'tavily', 'astradb', 'community', 'ibm', 'core', 'google_vertexai', 'azure_ai', 'huggingface', 'elasticsearch', 'google_community', 'langchain', 'weaviate', 'ai21', 'deepseek', 'exa', 'voyageai', 'sema4'])"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documentation.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "162dd756",
      "metadata": {},
      "source": [
        "## Saving Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "-A1_pIA77RxW",
      "metadata": {
        "id": "-A1_pIA77RxW"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "try:\n",
        "    dataset= open('LangDataset.pickle', 'wb')\n",
        "    pickle.dump(documentation, dataset)\n",
        "    dataset.close()\n",
        "\n",
        "except:\n",
        "    print(\"Something went wrong\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c079e1f",
      "metadata": {},
      "source": [
        "## Function Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "qFz5u2DyJgDf",
      "metadata": {
        "id": "qFz5u2DyJgDf"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "grouped = defaultdict(list)\n",
        "\n",
        "for base_class, content in dataset.items():\n",
        "    class_list = base_class.split('/')\n",
        "    root_topic, topic = class_list[4], class_list[-1]\n",
        "    grouped[root_topic].append(f\"\\n## Class Objects: {topic}\\n{content}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "sBZ-pRzI39Fv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBZ-pRzI39Fv",
        "outputId": "aff5f485-09aa-46e0-d94e-aaa7d52ac427"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "defaultdict(list,\n",
              "            {'': ['\\n## Class Objects: \\nabc'],\n",
              "             'ibm': ['\\n## Class Objects: index.html\\nabc',\n",
              "              '\\n## Class Objects: index\\nbcd']})"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grouped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "zWVWzBLU3_wy",
      "metadata": {
        "id": "zWVWzBLU3_wy"
      },
      "outputs": [],
      "source": [
        "combined_dataset = {\n",
        "    topic: \"\\n\".join(contents)\n",
        "    for topic, contents in grouped.items()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "w-CawZJV4s0O",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-CawZJV4s0O",
        "outputId": "7971d825-19c4-4ccc-d65b-226b45f950c2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'': '\\n## Class Objects: \\nabc',\n",
              " 'ibm': '\\n## Class Objects: index.html\\nabc\\n\\n## Class Objects: index\\nbcd'}"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "combined_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "bWvq3fbR4utK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWvq3fbR4utK",
        "outputId": "34621957-1431-4097-de69-6d62ec6d0eb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "My name\n",
            "\n",
            "is\n",
            "\n",
            "\n",
            "\n",
            "DArshil\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Pungalia\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "text = '''My name\n",
        "\n",
        "is\n",
        "\n",
        "\n",
        "\n",
        "DArshil\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Pungalia\n",
        "'''\n",
        "print(text)\n",
        "text = re.sub(r'\\n{3,}', '\\n\\n', text.strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "783d6d95",
      "metadata": {},
      "outputs": [],
      "source": [
        "def loadData(file):\n",
        "    dbfile = open(file, 'rb')\n",
        "    db = pickle.load(dbfile)\n",
        "\n",
        "    return db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a569ada7",
      "metadata": {},
      "outputs": [],
      "source": [
        "data = loadData(r'datasets\\LangDatasetChunked.pickle')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "528863c0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### Instruction: Learn about the perplexity LangChain API.\n",
            "\n",
            "### Part 2 - Module:index(chunk1)\n",
            "\n",
            "langchain-perplexity: 0.1.1\n",
            "\n",
            "langchain-perplexity: 0.1.1#\n",
            "This package provides the Perplexity integration for LangChain.\n",
            "\n",
            "chat_models#\n",
            "Classes\n",
            "\n",
            "chat_models.ChatPerplexity\n",
            "Perplexity AI Chat models API.\n"
          ]
        }
      ],
      "source": [
        "print(data[0]['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "634514e4",
      "metadata": {},
      "source": [
        "## Instruction-Response Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "fbf32fc4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def loadData(file):\n",
        "    dbfile = open(file, 'rb')\n",
        "    db = pickle.load(dbfile)\n",
        "\n",
        "    return db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "83155353",
      "metadata": {},
      "outputs": [],
      "source": [
        "data = loadData(r'datasets\\LangDatasetChunked.pickle')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "adbdd986",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1006\n",
            "<class 'list'>\n"
          ]
        }
      ],
      "source": [
        "print(len(data))\n",
        "print(type(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "fecc63c3",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'text': '### Question: What is index in perplexity ?\\n\\n### Part 2 - Module: index (chunk1)\\n\\nlangchain-perplexity: 0.1.1\\n\\nlangchain-perplexity: 0.1.1#\\nThis package provides the Perplexity integration for LangChain.\\n\\nchat_models#\\nClasses\\n\\nchat_models.ChatPerplexity\\nPerplexity AI Chat models API.'}"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "b7075672",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    temperature=0.5\n",
        ")\n",
        "\n",
        "parser = JsonOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "8d0113e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "template = '''You are a dataset generator for instruction fine-tuning. \n",
        "\n",
        "Given a topic and a block of documentation text, generate 5 to 8 unique and diverse instruction-response pairs as possible. Each instruction should be a meaningful question related to the content ‚Äî including documentation lookups, implementation usage, code examples, best practices, configuration, and edge cases. Vary the phrasing and difficulty.\n",
        "\n",
        "Each pair must be output as a JSON object with the following format:\n",
        "\"instruction\": \"<question>\",\n",
        "\"response\": \"<answer>\"\n",
        "\n",
        "Instructions must be clear and natural having both code related and basic doc questions, and answers should be accurate and concise, optionally including code if relevant.\n",
        "\n",
        "### Documentation:{text}\n",
        "\n",
        "Now generate a diverse list of instruction-response pairs in JSON format: '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "3784ae43",
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=['text'],\n",
        "    partial_variables={'format_instruction': parser.get_format_instructions()})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "a68c87c2",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['text'], input_types={}, partial_variables={'format_instruction': 'Return a JSON object.'}, template='You are a dataset generator for instruction fine-tuning. \\n\\nGiven a topic and a block of documentation text, generate 5 to 8 unique and diverse instruction-response pairs as possible. Each instruction should be a meaningful question related to the content ‚Äî including documentation lookups, implementation usage, code examples, best practices, configuration, and edge cases. Vary the phrasing and difficulty.\\n\\nEach pair must be output as a JSON object with the following format:\\n\"instruction\": \"<question>\",\\n\"response\": \"<answer>\"\\n\\nInstructions must be clear and natural having both code related and basic doc questions, and answers should be accurate and concise, optionally including code if relevant.\\n\\n### Documentation:{text}\\n\\nNow generate a diverse list of instruction-response pairs in JSON format: ')"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "544d5dd4",
      "metadata": {},
      "outputs": [],
      "source": [
        "chain = prompt | model | parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca027ca8",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "instruction_key_pairs = []\n",
        "failed_entries = []\n",
        "\n",
        "for i, text in enumerate(tqdm(data, desc=\"Processing documents\", total=1006)):\n",
        "    try:\n",
        "        pairs = chain.invoke({'text': text['text']})\n",
        "        instruction_key_pairs.append(pairs)\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping index {i} due to error: {e}\")\n",
        "        failed_entries.append((i, text['text'], str(e)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "7fa06301",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5754"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tuning_examples = 0\n",
        "\n",
        "for i in instruction_key_pairs:\n",
        "    tuning_examples += len(i)\n",
        "\n",
        "tuning_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "9f06aabd",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'instruction': 'What is the purpose of the `ChatPerplexity` class in the `langchain_perplexity` module?',\n",
              "  'response': \"The `ChatPerplexity` class provides an interface to interact with Perplexity AI's chat models API. It allows you to send messages to a specified model and receive responses, similar to using OpenAI's chat models.\"},\n",
              " {'instruction': 'How do I set up the Perplexity AI API key to use the `ChatPerplexity` class?',\n",
              "  'response': 'You need to set the `PPLX_API_KEY` environment variable to your Perplexity AI API key. For example, in a terminal, you can use the command `export PPLX_API_KEY=your_api_key`.'},\n",
              " {'instruction': 'What are some key parameters I can configure when initializing a `ChatPerplexity` instance?',\n",
              "  'response': 'You can configure parameters such as `model` (the name of the model to use, e.g., \"llama-3.1-sonar-small-128k-online\"), `temperature` (sampling temperature), `max_tokens` (maximum number of tokens to generate), and `streaming` (whether to stream results).'},\n",
              " {'instruction': 'Provide an example of how to invoke the `ChatPerplexity` model with a system and user message.',\n",
              "  'response': '```python\\nmessages = [(\"system\", \"You are a chatbot.\"), (\"user\", \"Hello!\")]\\nllm.invoke(messages)\\n```'},\n",
              " {'instruction': 'How can I stream the results from the `ChatPerplexity` model?',\n",
              "  'response': \"You can use the `stream` method. Here's an example:\\n\\n```python\\nfor chunk in llm.stream(messages):\\n    print(chunk.content)\\n```\"},\n",
              " {'instruction': 'How can I access the token usage information from a `ChatPerplexity` response?',\n",
              "  'response': 'After invoking the model, you can access the token usage metadata using `response.usage_metadata`.'},\n",
              " {'instruction': 'How can I pass Perplexity-specific parameters like `search_recency_filter` when invoking the model?',\n",
              "  'response': 'You can use the `extra_body` parameter in the `invoke` method. For example:\\n\\n```python\\nllm.invoke(messages, extra_body={\"search_recency_filter\": \"week\"})\\n```'},\n",
              " {'instruction': 'What is the purpose of the `astream_events` method, and what kind of events does it generate?',\n",
              "  'response': 'The `astream_events` method generates a stream of events that provide real-time information about the progress of the Runnable, including events from intermediate results. Events include `on_chain_start`, `on_chain_stream`, `on_chain_end`, `on_llm_start`, `on_llm_stream`, `on_llm_end`, and custom events.'}]"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "instruction_key_pairs[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "80a4f2bd",
      "metadata": {},
      "outputs": [],
      "source": [
        "instruction_key_pairs_combined = [pair for topic in instruction_key_pairs for pair in topic]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "bbed9d56",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5754"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(instruction_key_pairs_combined)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "599a6aa6",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'instruction': 'Explain how to use `with_alisteners` to bind asynchronous lifecycle listeners to a Runnable. Provide an example.',\n",
              "  'response': 'The `with_alisteners` method allows you to attach asynchronous functions that are called at the start, end, or on error of a Runnable\\'s execution. This is useful for tasks like logging, monitoring, or updating a UI. Here\\'s an example:\\n\\n```python\\nfrom langchain_core.runnables import RunnableLambda, Runnable\\nfrom datetime import datetime, timezone\\nimport time\\nimport asyncio\\n\\ndef format_t(timestamp: float) -> str:\\n    return datetime.fromtimestamp(timestamp, tz=timezone.utc).isoformat()\\n\\nasync def test_runnable(time_to_sleep : int):\\n    print(f\"Runnable[{time_to_sleep}s]: starts at {format_t(time.time())}\")\\n    await asyncio.sleep(time_to_sleep)\\n    print(f\"Runnable[{time_to_sleep}s]: ends at {format_t(time.time())}\")\\n\\nasync def fn_start(run_obj : Runnable):\\n    print(f\"on start callback starts at {format_t(time.time())}\")\\n    await asyncio.sleep(3)\\n    print(f\"on start callback ends at {format_t(time.time())}\")\\n\\nasync def fn_end(run_obj : Runnable):\\n    print(f\"on end callback starts at {format_t(time.time())}\")\\n    await asyncio.sleep(2)\\n    print(f\"on end callback ends at {format_t(time.time())}\")\\n\\nrunnable = RunnableLambda(test_runnable).with_alisteners(\\n    on_start=fn_start,\\n    on_end=fn_end\\n)\\n\\nasync def concurrent_runs():\\n    await asyncio.gather(runnable.ainvoke(2), runnable.ainvoke(3))\\n\\nasyncio.run(concurrent_runs())\\n```\\nThis example defines asynchronous functions `fn_start` and `fn_end` that are called before and after the `test_runnable` function, respectively.'},\n",
              " {'instruction': 'How can I configure a Runnable to retry automatically if it encounters an exception? What parameters can I adjust?',\n",
              "  'response': 'You can use the `with_retry` method to create a new Runnable that retries the original Runnable on exceptions.  You can specify the exception types to retry on using the `retry_if_exception_type` parameter, the maximum number of attempts with `stop_after_attempt`, and whether to use exponential jitter for the wait time between retries with `wait_exponential_jitter`. For example:\\n\\n```python\\nfrom langchain_core.runnables import RunnableLambda\\n\\ncount = 0\\ndef _lambda(x: int) -> None:\\n    global count\\n    count = count + 1\\n    if x == 1:\\n        raise ValueError(\"x is 1\")\\n    else:\\n        pass\\n\\nrunnable = RunnableLambda(_lambda)\\ntry:\\n    runnable.with_retry(\\n        stop_after_attempt=2,\\n        retry_if_exception_type=(ValueError,),\\n    ).invoke(1)\\nexcept ValueError:\\n    pass\\nassert (count == 2)\\n```\\nThis code retries the `_lambda` function if a `ValueError` is raised, up to a maximum of 2 attempts.'},\n",
              " {'instruction': 'Explain the purpose of `with_fallbacks` and provide a scenario where it would be useful.',\n",
              "  'response': 'The `with_fallbacks` method allows you to specify a sequence of Runnables to try if the original Runnable fails. This is useful for creating more robust and resilient chains that can handle unexpected errors or unavailable resources.  For example, if a primary language model is unavailable, you can fallback to a secondary model.\\n\\n```python\\nfrom typing import Iterator\\nfrom langchain_core.runnables import RunnableGenerator\\n\\ndef _generate_immediate_error(input: Iterator) -> Iterator[str]:\\n    raise ValueError()\\n    yield \"\"\\n\\ndef _generate(input: Iterator) -> Iterator[str]:\\n    yield from \"foo bar\"\\n\\nrunnable = RunnableGenerator(_generate_immediate_error).with_fallbacks(\\n    [RunnableGenerator(_generate)]\\n)\\nprint(\\'\\'.join(runnable.stream({})))\\n#foo bar\\n```\\nIn this example, `_generate_immediate_error` always raises a `ValueError`. The `with_fallbacks` method ensures that `_generate` is executed instead, providing a fallback mechanism.'},\n",
              " {'instruction': 'What are the possible return types of a Runnable created using `langchain_core.runnables.base.Runnable` when `include_raw` is False and the schema is a Pydantic class?',\n",
              "  'response': 'If `include_raw` is False and the schema is a Pydantic class, the Runnable outputs an instance of that Pydantic class (i.e., a Pydantic object).'},\n",
              " {'instruction': 'Explain the purpose of the `with_types` method in `langchain_core.runnables.base.Runnable` and provide a usage example.',\n",
              "  'response': 'The `with_types` method binds input and output types to a Runnable, returning a new Runnable with the specified types. This can be useful for type checking and ensuring that the Runnable is used with the correct data types. For example: `runnable.with_types(input_type=str, output_type=dict)`.'}]"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "instruction_key_pairs_combined[20:25]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "1f1938bf",
      "metadata": {},
      "outputs": [],
      "source": [
        "upload = loadData(r'datasets\\InsReDataset.pickle')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "b152438b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6739\n"
          ]
        }
      ],
      "source": [
        "print(len(upload)+len(instruction_key_pairs_combined))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "42896ee9",
      "metadata": {},
      "outputs": [],
      "source": [
        "for ins in upload:\n",
        "    instruction_key_pairs_combined.append(ins)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "15df9206",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6739"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(instruction_key_pairs_combined)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "0d79b7f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "save = open(r'datasets/InsReBroader.pickle', 'wb')\n",
        "pickle.dump(instruction_key_pairs_combined, save)\n",
        "save.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
