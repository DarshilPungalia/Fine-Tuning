{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2bf7d1a1",
      "metadata": {
        "id": "2bf7d1a1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import RecursiveUrlLoader, WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_transformers import BeautifulSoupTransformer\n",
        "from bs4 import BeautifulSoup\n",
        "import pickle\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "2b01fb93",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "cTXr_4EOvaet",
      "metadata": {
        "id": "cTXr_4EOvaet"
      },
      "outputs": [],
      "source": [
        "def bs4_extractor(html: str) -> str:\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    return soup.get_text()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "972dfa04",
      "metadata": {
        "id": "972dfa04"
      },
      "outputs": [],
      "source": [
        "loader  = RecursiveUrlLoader('https://python.langchain.com/api_reference/',\n",
        "                             max_depth=17,\n",
        "                             extractor=bs4_extractor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "bc4fcdb3",
      "metadata": {
        "id": "bc4fcdb3"
      },
      "outputs": [],
      "source": [
        "page = loader.lazy_load()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51d1d2a4",
      "metadata": {},
      "source": [
        "## Preprocessing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "JhKBHAGx2Owp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "JhKBHAGx2Owp",
        "outputId": "7207c0b7-13dd-4d63-f42f-9614533cb6cc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ibm'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base = ' https://python.langchain.com/api_reference/ibm/index.html'\n",
        "\n",
        "base_class = base.split('/')[4]\n",
        "base_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "14ffab1b",
      "metadata": {
        "id": "14ffab1b"
      },
      "outputs": [],
      "source": [
        "import unicodedata\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.encode('latin1', errors='ignore').decode('utf-8', errors='ignore')\n",
        "\n",
        "    text = unicodedata.normalize('NFKC', text)\n",
        "\n",
        "    text = ''.join(c for c in text if unicodedata.category(c)[0] != 'C')\n",
        "    text = re.sub(r\"LangChain\\s+documentation$\", \"\", text).strip()\n",
        "    text = text.split(\"|\")[0].strip()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "b402cde1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "b402cde1",
        "outputId": "2c950891-79e8-4500-d9ce-12d9c145b156"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'langchain-fireworks: 0.3.0'"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clean_text('langchain-fireworks: 0.3.0 √¢‚Ç¨‚Äù √∞≈∏¬¶≈ì√∞≈∏‚Äù‚Äî LangChain  documentation')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "QHkyjPn_8dUC",
      "metadata": {
        "id": "QHkyjPn_8dUC"
      },
      "outputs": [],
      "source": [
        "def remove_blank_lines(text):\n",
        "    lines = text.splitlines()\n",
        "    while lines and lines[0].strip() == \"\":\n",
        "        lines.pop(0)\n",
        "    while lines and lines[-1].strip() == \"\":\n",
        "        lines.pop()\n",
        "    lines = \"\\n\".join(lines)\n",
        "    return re.sub(r'\\n{3,}', '\\n\\n', lines.strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "f-3iy8-E8AbR",
      "metadata": {
        "id": "f-3iy8-E8AbR"
      },
      "outputs": [],
      "source": [
        "def extract_main_content(text, marker_leading=\"LangChain Python API Reference\", marker_trailing=\"¬© Copyright 2025, LangChain Inc.\"):\n",
        "    main_content = text.split(marker_leading, 1)[-1]\n",
        "    main_content = main_content.split(marker_trailing, 1)[0]\n",
        "    main_content = main_content.strip()\n",
        "    main_content = remove_blank_lines(main_content)\n",
        "    return main_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "ckrL8qnw90Et",
      "metadata": {
        "id": "ckrL8qnw90Et"
      },
      "outputs": [],
      "source": [
        "text = '''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "runnables ‚Äî ü¶úüîó LangChain  documentation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Skip to main content\n",
        "\n",
        "\n",
        "Back to top\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Ctrl+K\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    Reference\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Ctrl+K\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Docs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "GitHub\n",
        "\n",
        "\n",
        "\n",
        "X / Twitter\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Ctrl+K\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    Reference\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Docs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "GitHub\n",
        "\n",
        "\n",
        "\n",
        "X / Twitter\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Section Navigation\n",
        "Base packages\n",
        "\n",
        "Core\n",
        "Langchain\n",
        "agents\n",
        "callbacks\n",
        "chains\n",
        "chat_models\n",
        "embeddings\n",
        "evaluation\n",
        "globals\n",
        "hub\n",
        "indexes\n",
        "memory\n",
        "model_laboratory\n",
        "output_parsers\n",
        "retrievers\n",
        "runnables\n",
        "HubRunnable\n",
        "OpenAIFunction\n",
        "OpenAIFunctionsRouter\n",
        "\n",
        "\n",
        "smith\n",
        "storage\n",
        "\n",
        "\n",
        "Text Splitters\n",
        "Community\n",
        "Experimental\n",
        "\n",
        "Integrations\n",
        "\n",
        "AI21\n",
        "Anthropic\n",
        "AstraDB\n",
        "AWS\n",
        "Azure Ai\n",
        "Azure Dynamic Sessions\n",
        "Cerebras\n",
        "Chroma\n",
        "Cohere\n",
        "Deepseek\n",
        "Elasticsearch\n",
        "Exa\n",
        "Fireworks\n",
        "Google Community\n",
        "Google GenAI\n",
        "Google VertexAI\n",
        "Groq\n",
        "Huggingface\n",
        "IBM\n",
        "Milvus\n",
        "MistralAI\n",
        "MongoDB\n",
        "Neo4J\n",
        "Nomic\n",
        "Nvidia Ai Endpoints\n",
        "Ollama\n",
        "OpenAI\n",
        "Perplexity\n",
        "Pinecone\n",
        "Postgres\n",
        "Prompty\n",
        "Qdrant\n",
        "Redis\n",
        "Sema4\n",
        "Snowflake\n",
        "Sqlserver\n",
        "Standard Tests\n",
        "Tavily\n",
        "Together\n",
        "Unstructured\n",
        "Upstage\n",
        "VoyageAI\n",
        "Weaviate\n",
        "XAI\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "LangChain Python API Reference\n",
        "langchain: 0.3.25\n",
        "runnables\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "runnables#\n",
        "LangChain Runnable and the LangChain Expression Language (LCEL).\n",
        "The LangChain Expression Language (LCEL) offers a declarative method to build\n",
        "production-grade programs that harness the power of LLMs.\n",
        "Programs created using LCEL and LangChain Runnables inherently support\n",
        "synchronous, asynchronous, batch, and streaming operations.\n",
        "Support for async allows servers hosting the LCEL based programs\n",
        "to scale better for higher concurrent loads.\n",
        "Batch operations allow for processing multiple inputs in parallel.\n",
        "Streaming of intermediate outputs, as they‚Äôre being generated, allows for\n",
        "creating more responsive UX.\n",
        "This module contains non-core Runnable classes.\n",
        "Classes\n",
        "\n",
        "\n",
        "runnables.hub.HubRunnable\n",
        "An instance of a runnable stored in the LangChain Hub.\n",
        "\n",
        "runnables.openai_functions.OpenAIFunction\n",
        "A function description for ChatOpenAI\n",
        "\n",
        "runnables.openai_functions.OpenAIFunctionsRouter\n",
        "A runnable that routes to the selected function.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      ¬© Copyright 2025, LangChain Inc.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "text = extract_main_content(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "rO1AjSyV-IVS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rO1AjSyV-IVS",
        "outputId": "4a97667f-f194-4c06-cc3a-9d4db7f9d5db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "langchain: 0.3.25\n",
            "runnables\n",
            "\n",
            "runnables#\n",
            "LangChain Runnable and the LangChain Expression Language (LCEL).\n",
            "The LangChain Expression Language (LCEL) offers a declarative method to build\n",
            "production-grade programs that harness the power of LLMs.\n",
            "Programs created using LCEL and LangChain Runnables inherently support\n",
            "synchronous, asynchronous, batch, and streaming operations.\n",
            "Support for async allows servers hosting the LCEL based programs\n",
            "to scale better for higher concurrent loads.\n",
            "Batch operations allow for processing multiple inputs in parallel.\n",
            "Streaming of intermediate outputs, as they‚Äôre being generated, allows for\n",
            "creating more responsive UX.\n",
            "This module contains non-core Runnable classes.\n",
            "Classes\n",
            "\n",
            "runnables.hub.HubRunnable\n",
            "An instance of a runnable stored in the LangChain Hub.\n",
            "\n",
            "runnables.openai_functions.OpenAIFunction\n",
            "A function description for ChatOpenAI\n",
            "\n",
            "runnables.openai_functions.OpenAIFunctionsRouter\n",
            "A runnable that routes to the selected function.\n"
          ]
        }
      ],
      "source": [
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3404155",
      "metadata": {},
      "source": [
        "## Dataset Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "ef6364c3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef6364c3",
        "outputId": "3b9b822b-4449-42fa-8cf0-529dd362d325"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing documents: 775it [04:01,  3.20it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "documentation = defaultdict(list)\n",
        "\n",
        "for i, document in enumerate(tqdm(page, desc=\"Processing documents\")):\n",
        "    content = document.page_content\n",
        "    content = extract_main_content(content)\n",
        "    base_class = document.metadata['source']\n",
        "    class_list = base_class.split('/')\n",
        "    root_topic, topic = class_list[4], class_list[-1].replace('.html', '')\n",
        "    if root_topic == '':\n",
        "        root_topic = 'Langchain'\n",
        "    documentation[root_topic].append(f\"\\n\\n## Class Objects: {topic}\\n{content}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "CYSjDLb6_ehg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CYSjDLb6_ehg",
        "outputId": "6dcbf44e-2d54-4ae3-f3ee-8482ccd20b90"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(len(documentation['google_genai']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "Vcif5OivDRoL",
      "metadata": {
        "id": "Vcif5OivDRoL"
      },
      "outputs": [],
      "source": [
        "documentation = {\n",
        "    topic: \"\\n\".join(contents)\n",
        "    for topic, contents in documentation.items()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "R1TvDgToDe0k",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1TvDgToDe0k",
        "outputId": "be5b2b40-6a1f-464d-aaee-11bd1b331aff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'str'>\n",
            "144531\n"
          ]
        }
      ],
      "source": [
        "print(type(documentation['google_genai']))\n",
        "print(len(documentation['google_genai']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "f6FKC-o6Hkn1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6FKC-o6Hkn1",
        "outputId": "65b2c912-1ec9-4e93-ab17-aba86dfb8747"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['Langchain', 'perplexity', 'google_genai', 'ollama', 'chroma', 'pinecone', 'reference.html', 'nvidia_ai_endpoints', 'upstage', 'aws', 'anthropic', 'fireworks', '_static', 'cerebras', 'search.html', 'sqlserver', 'redis', '_modules', 'prompty', 'text_splitters', 'standard_tests', 'mistralai', 'mongodb', 'together', 'groq', 'cohere', 'experimental', 'nomic', 'openai', 'azure_dynamic_sessions', 'postgres', 'milvus', 'snowflake', 'neo4j', 'xai', 'unstructured', 'qdrant', 'tavily', 'astradb', 'community', 'ibm', 'core', 'google_vertexai', 'azure_ai', 'huggingface', 'elasticsearch', 'google_community', 'langchain', 'weaviate', 'ai21', 'deepseek', 'exa', 'voyageai', 'index.html', 'sema4'])"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documentation.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "WAHsJOmgD3c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "WAHsJOmgD3c8",
        "outputId": "750589c6-73c5-45e9-c429-fe414ff0ab3d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\n## Class Objects: search\\nSearch - ü¶úüîó LangChain  documentation\\n\\nSkip to main content\\n\\nBack to top\\n\\nCtrl+K\\n\\n    Reference\\n  \\n\\nCtrl+K\\n\\nDocs\\n\\nGitHub\\n\\nX / Twitter\\n\\nCtrl+K\\n\\n    Reference\\n  \\n\\nDocs\\n\\nGitHub\\n\\nX / Twitter\\n\\nSearch\\n\\nError\\nPlease activate JavaScript to enable the search functionality.\\n\\nCtrl+K'"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documentation.pop('_static')\n",
        "documentation.pop('index.html')\n",
        "documentation.pop('search.html')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "Eg3TkVlwIb-8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eg3TkVlwIb-8",
        "outputId": "664c8e23-f650-445b-ebc0-98ad6fac9be8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['Langchain', 'perplexity', 'google_genai', 'ollama', 'chroma', 'pinecone', 'reference.html', 'nvidia_ai_endpoints', 'upstage', 'aws', 'anthropic', 'fireworks', 'cerebras', 'sqlserver', 'redis', '_modules', 'prompty', 'text_splitters', 'standard_tests', 'mistralai', 'mongodb', 'together', 'groq', 'cohere', 'experimental', 'nomic', 'openai', 'azure_dynamic_sessions', 'postgres', 'milvus', 'snowflake', 'neo4j', 'xai', 'unstructured', 'qdrant', 'tavily', 'astradb', 'community', 'ibm', 'core', 'google_vertexai', 'azure_ai', 'huggingface', 'elasticsearch', 'google_community', 'langchain', 'weaviate', 'ai21', 'deepseek', 'exa', 'voyageai', 'sema4'])"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documentation.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "162dd756",
      "metadata": {},
      "source": [
        "## Saving Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "-A1_pIA77RxW",
      "metadata": {
        "id": "-A1_pIA77RxW"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "try:\n",
        "    dataset= open('LangDataset.pickle', 'wb')\n",
        "    pickle.dump(documentation, dataset)\n",
        "    dataset.close()\n",
        "\n",
        "except:\n",
        "    print(\"Something went wrong\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c079e1f",
      "metadata": {},
      "source": [
        "## Function Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "qFz5u2DyJgDf",
      "metadata": {
        "id": "qFz5u2DyJgDf"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "grouped = defaultdict(list)\n",
        "\n",
        "for base_class, content in dataset.items():\n",
        "    class_list = base_class.split('/')\n",
        "    root_topic, topic = class_list[4], class_list[-1]\n",
        "    grouped[root_topic].append(f\"\\n## Class Objects: {topic}\\n{content}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "sBZ-pRzI39Fv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBZ-pRzI39Fv",
        "outputId": "aff5f485-09aa-46e0-d94e-aaa7d52ac427"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "defaultdict(list,\n",
              "            {'': ['\\n## Class Objects: \\nabc'],\n",
              "             'ibm': ['\\n## Class Objects: index.html\\nabc',\n",
              "              '\\n## Class Objects: index\\nbcd']})"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grouped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "zWVWzBLU3_wy",
      "metadata": {
        "id": "zWVWzBLU3_wy"
      },
      "outputs": [],
      "source": [
        "combined_dataset = {\n",
        "    topic: \"\\n\".join(contents)\n",
        "    for topic, contents in grouped.items()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "w-CawZJV4s0O",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-CawZJV4s0O",
        "outputId": "7971d825-19c4-4ccc-d65b-226b45f950c2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'': '\\n## Class Objects: \\nabc',\n",
              " 'ibm': '\\n## Class Objects: index.html\\nabc\\n\\n## Class Objects: index\\nbcd'}"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "combined_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "bWvq3fbR4utK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWvq3fbR4utK",
        "outputId": "34621957-1431-4097-de69-6d62ec6d0eb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "My name\n",
            "\n",
            "is\n",
            "\n",
            "\n",
            "\n",
            "DArshil\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Pungalia\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "text = '''My name\n",
        "\n",
        "is\n",
        "\n",
        "\n",
        "\n",
        "DArshil\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Pungalia\n",
        "'''\n",
        "print(text)\n",
        "text = re.sub(r'\\n{3,}', '\\n\\n', text.strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "783d6d95",
      "metadata": {},
      "outputs": [],
      "source": [
        "def loadData(file):\n",
        "    dbfile = open(file, 'rb')\n",
        "    db = pickle.load(dbfile)\n",
        "\n",
        "    return db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a569ada7",
      "metadata": {},
      "outputs": [],
      "source": [
        "data = loadData(r'datasets\\LangDatasetChunked.pickle')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "528863c0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### Instruction: Learn about the perplexity LangChain API.\n",
            "\n",
            "### Part 2 - Module:index(chunk1)\n",
            "\n",
            "langchain-perplexity: 0.1.1\n",
            "\n",
            "langchain-perplexity: 0.1.1#\n",
            "This package provides the Perplexity integration for LangChain.\n",
            "\n",
            "chat_models#\n",
            "Classes\n",
            "\n",
            "chat_models.ChatPerplexity\n",
            "Perplexity AI Chat models API.\n"
          ]
        }
      ],
      "source": [
        "print(data[0]['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "634514e4",
      "metadata": {},
      "source": [
        "## Instruction-Response Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "fbf32fc4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def loadData(file):\n",
        "    dbfile = open(file, 'rb')\n",
        "    db = pickle.load(dbfile)\n",
        "\n",
        "    return db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "83155353",
      "metadata": {},
      "outputs": [],
      "source": [
        "data = loadData(r'datasets\\LangDatasetBetter.pickle')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "adbdd986",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "52\n",
            "<class 'dict'>\n"
          ]
        }
      ],
      "source": [
        "print(len(data))\n",
        "print(type(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "fecc63c3",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['Langchain', 'perplexity', 'google_genai', 'ollama', 'chroma', 'pinecone', 'reference.html', 'nvidia_ai_endpoints', 'upstage', 'aws', 'anthropic', 'fireworks', 'cerebras', 'sqlserver', 'redis', '_modules', 'prompty', 'text_splitters', 'standard_tests', 'mistralai', 'mongodb', 'together', 'groq', 'cohere', 'experimental', 'nomic', 'openai', 'azure_dynamic_sessions', 'postgres', 'milvus', 'snowflake', 'neo4j', 'xai', 'unstructured', 'qdrant', 'tavily', 'astradb', 'community', 'ibm', 'core', 'google_vertexai', 'azure_ai', 'huggingface', 'elasticsearch', 'google_community', 'langchain', 'weaviate', 'ai21', 'deepseek', 'exa', 'voyageai', 'sema4'])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "b7075672",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    temperature=0.5\n",
        ")\n",
        "\n",
        "parser = JsonOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "8d0113e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "template = '''You are a dataset generator for instruction fine-tuning. \n",
        "\n",
        "Given a topic and a block of documentation text, generate as many unique and diverse instruction-response pairs as possible. Each instruction should be a meaningful question related to the content ‚Äî including documentation lookups, implementation usage, code examples, best practices, configuration, and edge cases. Vary the phrasing and difficulty.\n",
        "\n",
        "Each pair must be output as a JSON object with the following format:\n",
        "\"instruction\": \"<question>\",\n",
        "\"response\": \"<answer>\"\n",
        "\n",
        "Instructions must be clear and natural, and answers should be accurate and concise, optionally including code if relevant.\n",
        "\n",
        "### Topic: {topic}\n",
        "### Documentation:{text}\n",
        "\n",
        "Now generate a diverse list of instruction-response pairs in JSON format: '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "3784ae43",
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=['topic', 'text'],\n",
        "    partial_variables={'format_instruction': parser.get_format_instructions()})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "a68c87c2",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['text', 'topic'], input_types={}, partial_variables={'format_instruction': 'Return a JSON object.'}, template='You are a dataset generator for instruction fine-tuning. \\n\\nGiven a topic and a block of documentation text, generate as many unique and diverse instruction-response pairs as possible. Each instruction should be a meaningful question related to the content ‚Äî including documentation lookups, implementation usage, code examples, best practices, configuration, and edge cases. Vary the phrasing and difficulty.\\n\\nEach pair must be output as a JSON object with the following format:\\n\"instruction\": \"<question>\",\\n\"response\": \"<answer>\"\\n\\nInstructions must be clear and natural, and answers should be accurate and concise, optionally including code if relevant.\\n\\n### Topic: {topic}\\n### Documentation:{text}\\n\\nNow generate a diverse list of instruction-response pairs in JSON format: ')"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "544d5dd4",
      "metadata": {},
      "outputs": [],
      "source": [
        "chain = prompt | model | parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "ca027ca8",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing documents: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 52/52 [07:58<00:00,  9.20s/it]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "instruction_key_pairs = []\n",
        "\n",
        "for i, (key, value) in enumerate(tqdm(data.items(), desc=\"Processing documents\", total=len(data))):\n",
        "    pairs = chain.invoke({'topic': key, 'text': value})\n",
        "    instruction_key_pairs.append(pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "7fa06301",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "985"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tuning_examples = 0\n",
        "\n",
        "for i in instruction_key_pairs:\n",
        "    tuning_examples += len(i)\n",
        "\n",
        "tuning_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "80a4f2bd",
      "metadata": {},
      "outputs": [],
      "source": [
        "instruction_key_pairs_combined = [pair for topic in instruction_key_pairs for pair in topic]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "bbed9d56",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "985"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(instruction_key_pairs_combined)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "599a6aa6",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'instruction': 'What is the purpose of the LangChain Python API Reference?',\n",
              "  'response': 'It serves as a reference for all langchain-x packages.'},\n",
              " {'instruction': 'Where can I find user guides for LangChain?',\n",
              "  'response': 'User guides are available at https://python.langchain.com.'},\n",
              " {'instruction': 'Where is the legacy API reference hosted?',\n",
              "  'response': 'The legacy API reference is hosted on ReadTheDocs at https://api.python.langchain.com/.'},\n",
              " {'instruction': 'What version of langchain-core is documented in this reference?',\n",
              "  'response': 'langchain-core: 0.3.60'},\n",
              " {'instruction': 'What version of the main Langchain package is documented?',\n",
              "  'response': 'langchain: 0.3.25'},\n",
              " {'instruction': 'What is the version number for langchain-text-splitters?',\n",
              "  'response': 'langchain-text-splitters: 0.3.8'},\n",
              " {'instruction': 'What version of langchain-community is available?',\n",
              "  'response': 'langchain-community: 0.3.24'},\n",
              " {'instruction': 'What is the version number of langchain-experimental?',\n",
              "  'response': 'langchain-experimental: 0.3.5rc1'},\n",
              " {'instruction': 'What is the version of langchain-openai documented here?',\n",
              "  'response': 'langchain-openai 0.3.17'},\n",
              " {'instruction': 'What version of langchain-anthropic is included in this reference?',\n",
              "  'response': 'langchain-anthropic 0.3.13'},\n",
              " {'instruction': 'What version of langchain-google-vertexai is documented?',\n",
              "  'response': 'langchain-google-vertexai 2.0.24'},\n",
              " {'instruction': 'What version of langchain-aws is available?',\n",
              "  'response': 'langchain-aws 0.2.23'},\n",
              " {'instruction': 'What version of langchain-huggingface is documented?',\n",
              "  'response': 'langchain-huggingface 0.2.0'},\n",
              " {'instruction': 'What version of langchain-mistralai is referenced?',\n",
              "  'response': 'langchain-mistralai 0.2.10'},\n",
              " {'instruction': \"Where can I find the documentation for the 'core' package?\",\n",
              "  'response': 'core/index.html'},\n",
              " {'instruction': \"Where can I find the documentation for the main 'Langchain' package?\",\n",
              "  'response': 'langchain/index.html'},\n",
              " {'instruction': \"Where can I find the documentation for 'langchain-text-splitters'?\",\n",
              "  'response': 'text_splitters/index.html'},\n",
              " {'instruction': \"Where can I find the documentation for 'langchain-community'?\",\n",
              "  'response': 'community/index.html'},\n",
              " {'instruction': \"Where can I find the documentation for 'langchain-experimental'?\",\n",
              "  'response': 'experimental/index.html'},\n",
              " {'instruction': \"Where can I find the documentation for 'langchain-openai'?\",\n",
              "  'response': 'openai/index.html'},\n",
              " {'instruction': \"Where can I find the documentation for 'langchain-anthropic'?\",\n",
              "  'response': 'anthropic/index.html'},\n",
              " {'instruction': \"Where can I find the documentation for 'langchain-google-vertexai'?\",\n",
              "  'response': 'google_vertexai/index.html'},\n",
              " {'instruction': \"Where can I find the documentation for 'langchain-aws'?\",\n",
              "  'response': 'aws/index.html'},\n",
              " {'instruction': \"Where can I find the documentation for 'langchain-huggingface'?\",\n",
              "  'response': 'huggingface/index.html'},\n",
              " {'instruction': \"Where can I find the documentation for 'langchain-mistralai'?\",\n",
              "  'response': 'mistralai/index.html'},\n",
              " {'instruction': 'What is the primary purpose of the `langchain-nvidia-ai-endpoints` module?',\n",
              "  'response': \"The module integrates NVIDIA's AI Foundation Models into the LangChain framework, providing tools for conversational AI and semantic embeddings.\"},\n",
              " {'instruction': 'How can I install the `langchain-nvidia-ai-endpoints` module?',\n",
              "  'response': 'You can install it using pip: `pip install langchain-nvidia-ai-endpoints`.'},\n",
              " {'instruction': 'Explain the functionality of the `ChatNVIDIA` class.',\n",
              "  'response': \"The `ChatNVIDIA` class is used to interact with NVIDIA's Foundation chat models, like 'Mistral', for conversational AI applications.\"},\n",
              " {'instruction': 'What are `NVIDIAEmbeddings` used for?',\n",
              "  'response': \"`NVIDIAEmbeddings` are used to generate semantic embeddings from text using NVIDIA's AI models, enabling tasks like semantic analysis and text similarity assessment.\"},\n",
              " {'instruction': 'Provide an example of how to use `ChatNVIDIA` to generate a response.',\n",
              "  'response': '```python\\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\\nai_chat_model = ChatNVIDIA(model=\"meta/llama2-70b\")\\nresponse = ai_chat_model.invoke(\"Tell me about the LangChain integration.\")\\n```'},\n",
              " {'instruction': 'Show me how to create embeddings using `NVIDIAEmbeddings`.',\n",
              "  'response': '```python\\nfrom langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\\nembed_model = NVIDIAEmbeddings(model=\"nvolveqa_40k\")\\nembedding_output = embed_model.embed_query(\"Exploring AI capabilities.\")\\n```'},\n",
              " {'instruction': 'What is the purpose of the `UsageCallbackHandler` class in the `callbacks` module?',\n",
              "  'response': 'The `UsageCallbackHandler` tracks OpenAI info, including token usage and cost.'},\n",
              " {'instruction': 'How can I get the cost in USD for a given model and number of tokens?',\n",
              "  'response': 'Use the `get_token_cost_for_model` function in the `callbacks` module.'},\n",
              " {'instruction': 'What does the `standardize_model_name` function do?',\n",
              "  'response': 'It standardizes the model name to a format that can be used in the OpenAI API.'},\n",
              " {'instruction': 'What is the `NVIDIARerank` class used for?',\n",
              "  'response': 'The `NVIDIARerank` class is a LangChain Document Compressor that uses the NVIDIA NeMo Retriever Reranking API to rerank documents based on a query.'},\n",
              " {'instruction': 'How do I connect to a self-hosted NVIDIA NIM for reranking?',\n",
              "  'response': 'You can connect to a self-hosted model using the `base_url` argument, like this: `ranker = NVIDIARerank(base_url=\"http://localhost:8000/v1\")`.'},\n",
              " {'instruction': 'What are the arguments for the `NVIDIARerank` class?',\n",
              "  'response': 'The arguments include `model` (the model to use), `nvidia_api_key` or `api_key` (API key), `base_url` (base URL of the NIM), and `truncate` (truncation strategy).'},\n",
              " {'instruction': 'How can I find the available models for `NVIDIARerank`?',\n",
              "  'response': 'Use the `get_available_models()` class method.'},\n",
              " {'instruction': 'What does the `compress_documents` method of `NVIDIARerank` do?',\n",
              "  'response': 'It compresses a sequence of documents using the NVIDIA NeMo Retriever Reranking API, returning a sequence of compressed documents.'},\n",
              " {'instruction': 'What is the purpose of the `NVIDIA` class in the `llm` module?',\n",
              "  'response': 'The `NVIDIA` class is a LangChain LLM that uses the Completions API with NVIDIA NIMs.'},\n",
              " {'instruction': 'How do I provide the API key for the `NVIDIA` LLM?',\n",
              "  'response': 'The recommended way is through the `NVIDIA_API_KEY` environment variable.'},\n",
              " {'instruction': 'What additional arguments can be passed to the Completions API via the `NVIDIA` class?',\n",
              "  'response': 'You can pass arguments like `max_tokens`, `stop`, `temperature`, `top_p`, `frequency_penalty`, `presence_penalty`, and `seed`.'},\n",
              " {'instruction': 'How can I use the `bind()` method with the `NVIDIA` class?',\n",
              "  'response': 'You can use `bind()` to set arguments like `max_tokens`, for example: `NVIDIA().bind(max_tokens=512)`.'},\n",
              " {'instruction': 'Explain the purpose of the `stream()` method in the `NVIDIA` class.',\n",
              "  'response': 'The `stream()` method provides a way to get streaming output from the NVIDIA LLM.'},\n",
              " {'instruction': 'How can I save an LLM created with the `NVIDIA` class?',\n",
              "  'response': 'Use the `save()` method, for example: `llm.save(file_path=\"path/llm.yaml\")`.'},\n",
              " {'instruction': 'What is the purpose of the `UsageCallbackHandler` and how do I use it?',\n",
              "  'response': 'The `UsageCallbackHandler` tracks token usage and cost. You can use it with `get_usage_callback` in a context manager to conveniently expose this information.'},\n",
              " {'instruction': 'Explain the parameters of the `get_token_cost_for_model` function.',\n",
              "  'response': 'The parameters are `model_name` (name of the model), `num_tokens` (number of tokens), `price_map` (map of model names to cost per 1000 tokens), and `is_completion` (whether the model is used for completion).'},\n",
              " {'instruction': 'What are the key features provided by the `langchain-nvidia-ai-endpoints` module?',\n",
              "  'response': 'Key features include `Chat Models (ChatNVIDIA)` for conversational AI and `Semantic Embeddings (NVIDIAEmbeddings)` for NLP tasks like semantic analysis.'},\n",
              " {'instruction': 'How can I retrieve a list of available models for the `ChatNVIDIA` class?',\n",
              "  'response': 'You can use the `get_available_models()` class method of the `ChatNVIDIA` class.'},\n",
              " {'instruction': 'What does the `disable_streaming` parameter in `ChatNVIDIA` control?',\n",
              "  'response': \"It controls whether streaming is disabled for the model. It can be set to `True` to always bypass streaming or to `'tool_calling'` to bypass streaming only when the model is called with a `tools` keyword argument.\"},\n",
              " {'instruction': 'Explain the usage of `bind_tools` method of `ChatNVIDIA`.',\n",
              "  'response': 'The `bind_tools` method binds tools to the model, enabling function calling.  It takes a list of tool definitions and optionally a `tool_choice` parameter to control how the model selects tools.'},\n",
              " {'instruction': 'What is the purpose of the `truncate` parameter in `NVIDIAEmbeddings` and what are its possible values?',\n",
              "  'response': \"The `truncate` parameter specifies how to handle input text that exceeds the model's maximum token length. Possible values are 'NONE' (raises an error), 'START' (truncates from the beginning), and 'END' (truncates from the end).\"},\n",
              " {'instruction': 'How do I call the ChatNVIDIA model?',\n",
              "  'response': 'You can call the model by passing a list of messages to the `invoke` method.'},\n",
              " {'instruction': 'How do I set the Upstage API key for the ChatUpstage model?',\n",
              "  'response': \"Set the environment variable `UPSTAGE_API_KEY` with your API key or pass it as a named parameter to the constructor, like this: `model = ChatUpstage(upstage_api_key='YOUR_API_KEY')`.\"},\n",
              " {'instruction': 'What is the purpose of the `UpstageGroundednessCheck` tool?',\n",
              "  'response': \"The `UpstageGroundednessCheck` tool verifies if an assistant's response is grounded in the provided context, ensuring relevance and alignment with the user's initial context. It's useful in retrieval-augmented generation (RAG) to determine if the assistant's message is based on the given context.\"},\n",
              " {'instruction': 'How can I use the `UpstageDocumentParseLoader` to load a PDF and split it by page?',\n",
              "  'response': 'You can load a PDF and split it by page using the `UpstageDocumentParseLoader` like this:\\n\\n```python\\nfrom langchain_upstage import UpstageDocumentParseLoader\\n\\nfile_path = \"/PATH/TO/YOUR/FILE.pdf\"\\nloader = UpstageDocumentParseLoader(file_path, split=\"page\")\\ndocuments = loader.load()\\n```'},\n",
              " {'instruction': 'What are the available options for the `split` parameter in `UpstageDocumentParseLoader` and what do they mean?',\n",
              "  'response': \"The `split` parameter in `UpstageDocumentParseLoader` controls how the document is split. Available options are 'none' (no splitting), 'page' (split by page), and 'element' (split by element).\"},\n",
              " {'instruction': 'How do I use the deprecated `GroundednessCheck` class?',\n",
              "  'response': \"While `GroundednessCheck` is deprecated, you can still use it as you would `UpstageGroundednessCheck`. However, it's recommended to migrate to `UpstageGroundednessCheck` as `GroundednessCheck` will be removed in a future version.\"},\n",
              " {'instruction': 'What is the purpose of the `stream_usage` parameter in `ChatUpstage`?',\n",
              "  'response': 'If `stream_usage` is set to `True` in `ChatUpstage`, an additional message chunk will be generated during streaming that includes usage metadata.'},\n",
              " {'instruction': 'How do I disable streaming in the `ChatUpstage` model?',\n",
              "  'response': 'You can disable streaming by setting the `disable_streaming` parameter to `True` when initializing the `ChatUpstage` model.  For example: `model = ChatUpstage(disable_streaming=True)`.'},\n",
              " {'instruction': 'What does the `get_from_param_or_env` function do in `langchain_upstage.document_parse_parsers`?',\n",
              "  'response': 'The `get_from_param_or_env` function retrieves a value from a parameter or an environment variable, prioritizing the parameter if both are set. It takes a key, an optional parameter name, an optional environment variable key, and an optional default value.'},\n",
              " {'instruction': 'How can I configure a `ChatUpstage` model to use a specific tokenizer?',\n",
              "  'response': \"You can specify the Hugging Face tokenizer name using the `tokenizer_name` parameter when initializing the `ChatUpstage` model. For example: `model = ChatUpstage(tokenizer_name='upstage/solar-pro-tokenizer')`.\"},\n",
              " {'instruction': 'What is the purpose of the `validate_file_path` function in `langchain_upstage.document_parse`?',\n",
              "  'response': 'The `validate_file_path` function checks if a file exists at the given file path. It raises a `FileNotFoundError` if the file does not exist.'},\n",
              " {'instruction': 'How do I use the `UpstageEmbeddings` class to generate embeddings for text?',\n",
              "  'response': 'First, set the `UPSTAGE_API_KEY` environment variable.  Then, you can use the `UpstageEmbeddings` class like this:\\n\\n```python\\nfrom langchain_upstage import UpstageEmbeddings\\n\\nmodel = UpstageEmbeddings(model=\\'solar-embedding-1-large\\')\\nembeddings = model.embed_query(\"Your text here\")\\nprint(embeddings)\\n```'},\n",
              " {'instruction': 'What is the purpose of the `response_format` parameter in the `UpstageGroundednessCheck` tool?',\n",
              "  'response': \"The `response_format` parameter in `UpstageGroundednessCheck` determines the format of the tool's response. If set to 'content', the output is interpreted as the contents of a ToolMessage. If set to 'content_and_artifact', the output is expected to be a two-tuple corresponding to the (content, artifact) of a ToolMessage.\"},\n",
              " {'instruction': 'What is the difference between `invoke` and `ainvoke` methods in the `ChatUpstage` class?',\n",
              "  'response': '`invoke` is the synchronous method to call the model, while `ainvoke` is the asynchronous version. Use `invoke` for synchronous code and `ainvoke` for asynchronous code.'},\n",
              " {'instruction': 'How can I add tags to a `UpstageGroundednessCheck` tool?',\n",
              "  'response': \"You can add tags to a `UpstageGroundednessCheck` tool by passing a list of strings to the `tags` parameter during initialization: `tool = UpstageGroundednessCheck(tags=['my_tag', 'another_tag'])`.\"},\n",
              " {'instruction': 'What parameters can be configured for the `UpstageDocumentParseParser`?',\n",
              "  'response': 'The `UpstageDocumentParseParser` can be configured with parameters such as `api_key`, `base_url`, `model`, `split`, `ocr`, `output_format`, `coordinates`, and `base64_encoding`.'},\n",
              " {'instruction': 'How can I use a custom HTTP client with `ChatUpstage`?',\n",
              "  'response': 'You can pass an `httpx.Client` instance to the `http_client` parameter for sync invocations and an `httpx.AsyncClient` instance to the `http_async_client` parameter for async invocations. You must specify both if you want to customize both sync and async clients.'},\n",
              " {'instruction': 'What is the purpose of the `extra_body` parameter in `ChatUpstage`?',\n",
              "  'response': 'The `extra_body` parameter in `ChatUpstage` allows you to include optional additional JSON properties in the request parameters when making requests to OpenAI compatible APIs, such as vLLM.'},\n",
              " {'instruction': \"What happens if I set `ocr` to 'auto' in `UpstageDocumentParseLoader` and the input is not a PDF?\",\n",
              "  'response': \"If you set `ocr` to 'auto' in `UpstageDocumentParseLoader` and the input is not a PDF, an error will occur. The 'auto' setting only extracts text from PDFs.\"},\n",
              " {'instruction': 'How can I retrieve the number of tokens in a text using the `ChatUpstage` model?',\n",
              "  'response': 'You can use the `get_num_tokens` method of the `ChatUpstage` class. For example: `model = ChatUpstage(); num_tokens = model.get_num_tokens(\"Your text here\")`.'},\n",
              " {'instruction': 'How do I use the `with_structured_output` method with a Pydantic model?',\n",
              "  'response': '```python\\nfrom langchain_upstage import ChatUpstage\\nfrom pydantic import BaseModel\\n\\nclass AnswerWithJustification(BaseModel):\\n    answer: str\\n    justification: str\\n\\nllm = ChatUpstage(model=\"solar-mini\", temperature=0)\\nstructured_llm = llm.with_structured_output(AnswerWithJustification)\\n\\nresult = structured_llm.invoke(\"What weighs more a pound of bricks or a pound of feathers\")\\nprint(result)\\n```'},\n",
              " {'instruction': 'How can I invoke a Bedrock Agent using Langchain?',\n",
              "  'response': 'You can use `BedrockAgentsRunnable` to invoke a Bedrock Agent.  Alternatively, you can invoke a Bedrock Inline Agent as a Runnable using `BedrockInlineAgentsRunnable`.'},\n",
              " {'instruction': 'What is the purpose of the `parse_agent_response` function?',\n",
              "  'response': 'The `parse_agent_response` function parses the raw response received from a Bedrock Agent.'},\n",
              " {'instruction': 'How can I create a question-answering chain against a Neptune graph using openCypher?',\n",
              "  'response': 'You can use the `create_neptune_opencypher_qa_chain` function in the `chains.graph_qa.neptune_cypher` module to create a chain for question-answering against a Neptune graph by generating openCypher statements. You need to provide an LLM and a `BaseNeptuneGraph` object.'},\n",
              " {'instruction': 'What parameters are required to initialize an `InMemoryVectorStore`?',\n",
              "  'response': 'The required parameters for initializing an `InMemoryVectorStore` are `redis_url`, `index_name`, and `embedding`.'},\n",
              " {'instruction': 'How can I connect to an existing `InMemoryVectorStore` index?',\n",
              "  'response': 'You can connect to an existing `InMemoryVectorStore` index using the `from_existing_index` class method, providing the `embedding`, `index_name`, `schema`, and `key_prefix`.'},\n",
              " {'instruction': 'What are the supported search types for `InMemoryVectorStore`?',\n",
              "  'response': \"The supported search types for `InMemoryVectorStore` are 'similarity', 'mmr', and 'similarity_score_threshold'.\"},\n",
              " {'instruction': 'How can I perform a similarity search with a distance threshold in `InMemoryVectorStore`?',\n",
              "  'response': 'You can perform a similarity search with a distance threshold using the `similarity_search` method, setting the `distance_threshold` parameter to your desired value.'},\n",
              " {'instruction': 'How can I add documents to an existing `InMemoryVectorStoreRetriever`?',\n",
              "  'response': 'You can add documents to an existing `InMemoryVectorStoreRetriever` by using the `add_documents` method, passing in a list of `Document` objects.'},\n",
              " {'instruction': 'What is the purpose of the `InMemorySemanticCache` class?',\n",
              "  'response': 'The `InMemorySemanticCache` class provides a cache that uses MemoryDB as a vector-store backend for semantic caching of LLM responses.'},\n",
              " {'instruction': 'How do I configure the `AmazonKnowledgeBasesRetriever`?',\n",
              "  'response': 'To configure the `AmazonKnowledgeBasesRetriever`, you need to provide the `knowledge_base_id`.  Optionally, you can specify the `region_name`, credentials, `retrieval_config`, and other parameters.'},\n",
              " {'instruction': 'What is the purpose of the `create_aws_client` function?',\n",
              "  'response': 'The `create_aws_client` function is a helper function to validate AWS credentials and create an AWS client for a given service.'},\n",
              " {'instruction': 'How can I determine the number of tokens in a text string for Anthropic models?',\n",
              "  'response': 'You can use the `get_num_tokens_anthropic` function to get the number of tokens in a string of text for Anthropic models.'},\n",
              " {'instruction': 'How do I extract Cypher code from a string?',\n",
              "  'response': 'You can use the `extract_cypher` function from `langchain_aws.chains.graph_qa.neptune_cypher` to extract Cypher code from a text string using regular expressions.'},\n",
              " {'instruction': 'How can I create a Neptune SPARQL QA chain?',\n",
              "  'response': 'Use the `create_neptune_sparql_qa_chain` function, providing a `BaseLanguageModel` and a `NeptuneRdfGraph`.'},\n",
              " {'instruction': 'What is the purpose of the `InMemoryDBFilterExpression` class?',\n",
              "  'response': 'The `InMemoryDBFilterExpression` class represents a logical expression of `InMemoryDBFilterFields`, used for creating complex queries against a MemoryDB index.'},\n",
              " {'instruction': 'What is the purpose of the `DistanceStrategy` enum?',\n",
              "  'response': 'The `DistanceStrategy` enum defines the available distance strategies for calculating distances between vectors in the `utilities` module.'},\n",
              " {'instruction': 'How do I drop an `InMemoryVectorStore` index?',\n",
              "  'response': 'You can drop an `InMemoryVectorStore` index using the `drop_index` static method, specifying the `index_name` and whether to `delete_documents`.'},\n",
              " {'instruction': 'How can I write the schema of an `InMemoryVectorStore` to a YAML file?',\n",
              "  'response': 'You can use the `write_schema` method, providing the `path` to the desired YAML file.'},\n",
              " {'instruction': 'How do I use the `AmazonKnowledgeBasesRetriever` in a chain?',\n",
              "  'response': \"You can use the `AmazonKnowledgeBasesRetriever`'s `invoke` or `ainvoke` methods to retrieve relevant documents based on a query. The retriever can then be used as part of a Langchain chain.\"},\n",
              " {'instruction': 'What is the purpose of the `thinking_in_params` function?',\n",
              "  'response': \"The `thinking_in_params` function checks if the 'thinking' parameter is enabled in a request dictionary, which is related to the extended reasoning feature in some models.\"},\n",
              " {'instruction': 'What is the purpose of the `langchain-anthropic` package?',\n",
              "  'response': \"The `langchain-anthropic` package provides integrations for using Anthropic's models within the Langchain framework. It includes classes and functions for chat models, LLMs, tool usage, and output parsing.\"},\n",
              " {'instruction': \"How can I use Anthropic's chat models with Langchain?\",\n",
              "  'response': \"You can use the `ChatAnthropic` class within the `chat_models` module. This class allows you to interact with Anthropic's chat models.\"},\n",
              " {'instruction': 'What is the function of `convert_to_anthropic_tool`?',\n",
              "  'response': 'The `convert_to_anthropic_tool` function, located in the `chat_models` module, converts a tool-like object into an Anthropic tool definition.'},\n",
              " {'instruction': 'What is the purpose of the `AnthropicTool` class?',\n",
              "  'response': 'The `AnthropicTool` class in the `chat_models` module represents an Anthropic tool definition.'},\n",
              " {'instruction': 'How do I generate a system message that describes available tools when using Anthropic models in Langchain?',\n",
              "  'response': 'You can use the `get_system_message` function in the `experimental` module. It takes a list of tools as input and generates a system message that describes them.'},\n",
              " {'instruction': 'What is the role of the `AnthropicLLM` class?',\n",
              "  'response': \"The `AnthropicLLM` class, found in the `llms` module, represents Anthropic's large language model and allows you to interact with it.\"},\n",
              " {'instruction': 'How can I parse the output of tool calls when using Anthropic models?',\n",
              "  'response': \"You can use the `ToolsOutputParser` class in the `output_parsers` module. It's designed to parse the output of tool calls.\"}]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "instruction_key_pairs_combined[200:300]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "1f1938bf",
      "metadata": {},
      "outputs": [],
      "source": [
        "upload = open(r'datasets\\InsReDataset.pickle', 'wb')\n",
        "pickle.dump(instruction_key_pairs_combined, upload)\n",
        "upload.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42896ee9",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
