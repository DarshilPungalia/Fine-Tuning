{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bf7d1a1",
      "metadata": {
        "id": "2bf7d1a1"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import RecursiveUrlLoader, WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_transformers import BeautifulSoupTransformer\n",
        "from bs4 import BeautifulSoup\n",
        "import pickle\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2b01fb93",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "cTXr_4EOvaet",
      "metadata": {
        "id": "cTXr_4EOvaet"
      },
      "outputs": [],
      "source": [
        "def bs4_extractor(html: str) -> str:\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    return soup.get_text()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "972dfa04",
      "metadata": {
        "id": "972dfa04"
      },
      "outputs": [],
      "source": [
        "loader  = RecursiveUrlLoader('https://python.langchain.com/api_reference/',\n",
        "                             max_depth=17,\n",
        "                             extractor=bs4_extractor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "bc4fcdb3",
      "metadata": {
        "id": "bc4fcdb3"
      },
      "outputs": [],
      "source": [
        "page = loader.lazy_load()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51d1d2a4",
      "metadata": {},
      "source": [
        "## Preprocessing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "JhKBHAGx2Owp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "JhKBHAGx2Owp",
        "outputId": "7207c0b7-13dd-4d63-f42f-9614533cb6cc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ibm'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base = ' https://python.langchain.com/api_reference/ibm/index.html'\n",
        "\n",
        "base_class = base.split('/')[4]\n",
        "base_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "14ffab1b",
      "metadata": {
        "id": "14ffab1b"
      },
      "outputs": [],
      "source": [
        "import unicodedata\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.encode('latin1', errors='ignore').decode('utf-8', errors='ignore')\n",
        "\n",
        "    text = unicodedata.normalize('NFKC', text)\n",
        "\n",
        "    text = ''.join(c for c in text if unicodedata.category(c)[0] != 'C')\n",
        "    text = re.sub(r\"LangChain\\s+documentation$\", \"\", text).strip()\n",
        "    text = text.split(\"|\")[0].strip()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "b402cde1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "b402cde1",
        "outputId": "2c950891-79e8-4500-d9ce-12d9c145b156"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'langchain-fireworks: 0.3.0'"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clean_text('langchain-fireworks: 0.3.0 √¢‚Ç¨‚Äù √∞≈∏¬¶≈ì√∞≈∏‚Äù‚Äî LangChain  documentation')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "QHkyjPn_8dUC",
      "metadata": {
        "id": "QHkyjPn_8dUC"
      },
      "outputs": [],
      "source": [
        "def remove_blank_lines(text):\n",
        "    lines = text.splitlines()\n",
        "    while lines and lines[0].strip() == \"\":\n",
        "        lines.pop(0)\n",
        "    while lines and lines[-1].strip() == \"\":\n",
        "        lines.pop()\n",
        "    lines = \"\\n\".join(lines)\n",
        "    return re.sub(r'\\n{3,}', '\\n\\n', lines.strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "f-3iy8-E8AbR",
      "metadata": {
        "id": "f-3iy8-E8AbR"
      },
      "outputs": [],
      "source": [
        "def extract_main_content(text, marker_leading=\"LangChain Python API Reference\", marker_trailing=\"¬© Copyright 2025, LangChain Inc.\"):\n",
        "    main_content = text.split(marker_leading, 1)[-1]\n",
        "    main_content = main_content.split(marker_trailing, 1)[0]\n",
        "    main_content = main_content.strip()\n",
        "    main_content = remove_blank_lines(main_content)\n",
        "    return main_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "ckrL8qnw90Et",
      "metadata": {
        "id": "ckrL8qnw90Et"
      },
      "outputs": [],
      "source": [
        "text = '''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "runnables ‚Äî ü¶úüîó LangChain  documentation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Skip to main content\n",
        "\n",
        "\n",
        "Back to top\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Ctrl+K\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    Reference\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Ctrl+K\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Docs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "GitHub\n",
        "\n",
        "\n",
        "\n",
        "X / Twitter\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Ctrl+K\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    Reference\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Docs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "GitHub\n",
        "\n",
        "\n",
        "\n",
        "X / Twitter\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Section Navigation\n",
        "Base packages\n",
        "\n",
        "Core\n",
        "Langchain\n",
        "agents\n",
        "callbacks\n",
        "chains\n",
        "chat_models\n",
        "embeddings\n",
        "evaluation\n",
        "globals\n",
        "hub\n",
        "indexes\n",
        "memory\n",
        "model_laboratory\n",
        "output_parsers\n",
        "retrievers\n",
        "runnables\n",
        "HubRunnable\n",
        "OpenAIFunction\n",
        "OpenAIFunctionsRouter\n",
        "\n",
        "\n",
        "smith\n",
        "storage\n",
        "\n",
        "\n",
        "Text Splitters\n",
        "Community\n",
        "Experimental\n",
        "\n",
        "Integrations\n",
        "\n",
        "AI21\n",
        "Anthropic\n",
        "AstraDB\n",
        "AWS\n",
        "Azure Ai\n",
        "Azure Dynamic Sessions\n",
        "Cerebras\n",
        "Chroma\n",
        "Cohere\n",
        "Deepseek\n",
        "Elasticsearch\n",
        "Exa\n",
        "Fireworks\n",
        "Google Community\n",
        "Google GenAI\n",
        "Google VertexAI\n",
        "Groq\n",
        "Huggingface\n",
        "IBM\n",
        "Milvus\n",
        "MistralAI\n",
        "MongoDB\n",
        "Neo4J\n",
        "Nomic\n",
        "Nvidia Ai Endpoints\n",
        "Ollama\n",
        "OpenAI\n",
        "Perplexity\n",
        "Pinecone\n",
        "Postgres\n",
        "Prompty\n",
        "Qdrant\n",
        "Redis\n",
        "Sema4\n",
        "Snowflake\n",
        "Sqlserver\n",
        "Standard Tests\n",
        "Tavily\n",
        "Together\n",
        "Unstructured\n",
        "Upstage\n",
        "VoyageAI\n",
        "Weaviate\n",
        "XAI\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "LangChain Python API Reference\n",
        "langchain: 0.3.25\n",
        "runnables\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "runnables#\n",
        "LangChain Runnable and the LangChain Expression Language (LCEL).\n",
        "The LangChain Expression Language (LCEL) offers a declarative method to build\n",
        "production-grade programs that harness the power of LLMs.\n",
        "Programs created using LCEL and LangChain Runnables inherently support\n",
        "synchronous, asynchronous, batch, and streaming operations.\n",
        "Support for async allows servers hosting the LCEL based programs\n",
        "to scale better for higher concurrent loads.\n",
        "Batch operations allow for processing multiple inputs in parallel.\n",
        "Streaming of intermediate outputs, as they‚Äôre being generated, allows for\n",
        "creating more responsive UX.\n",
        "This module contains non-core Runnable classes.\n",
        "Classes\n",
        "\n",
        "\n",
        "runnables.hub.HubRunnable\n",
        "An instance of a runnable stored in the LangChain Hub.\n",
        "\n",
        "runnables.openai_functions.OpenAIFunction\n",
        "A function description for ChatOpenAI\n",
        "\n",
        "runnables.openai_functions.OpenAIFunctionsRouter\n",
        "A runnable that routes to the selected function.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      ¬© Copyright 2025, LangChain Inc.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "text = extract_main_content(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "rO1AjSyV-IVS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rO1AjSyV-IVS",
        "outputId": "4a97667f-f194-4c06-cc3a-9d4db7f9d5db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "langchain: 0.3.25\n",
            "runnables\n",
            "\n",
            "runnables#\n",
            "LangChain Runnable and the LangChain Expression Language (LCEL).\n",
            "The LangChain Expression Language (LCEL) offers a declarative method to build\n",
            "production-grade programs that harness the power of LLMs.\n",
            "Programs created using LCEL and LangChain Runnables inherently support\n",
            "synchronous, asynchronous, batch, and streaming operations.\n",
            "Support for async allows servers hosting the LCEL based programs\n",
            "to scale better for higher concurrent loads.\n",
            "Batch operations allow for processing multiple inputs in parallel.\n",
            "Streaming of intermediate outputs, as they‚Äôre being generated, allows for\n",
            "creating more responsive UX.\n",
            "This module contains non-core Runnable classes.\n",
            "Classes\n",
            "\n",
            "runnables.hub.HubRunnable\n",
            "An instance of a runnable stored in the LangChain Hub.\n",
            "\n",
            "runnables.openai_functions.OpenAIFunction\n",
            "A function description for ChatOpenAI\n",
            "\n",
            "runnables.openai_functions.OpenAIFunctionsRouter\n",
            "A runnable that routes to the selected function.\n"
          ]
        }
      ],
      "source": [
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3404155",
      "metadata": {},
      "source": [
        "## Dataset Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "ef6364c3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef6364c3",
        "outputId": "3b9b822b-4449-42fa-8cf0-529dd362d325"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing documents: 775it [04:01,  3.20it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "documentation = defaultdict(list)\n",
        "\n",
        "for i, document in enumerate(tqdm(page, desc=\"Processing documents\")):\n",
        "    content = document.page_content\n",
        "    content = extract_main_content(content)\n",
        "    base_class = document.metadata['source']\n",
        "    class_list = base_class.split('/')\n",
        "    root_topic, topic = class_list[4], class_list[-1].replace('.html', '')\n",
        "    if root_topic == '':\n",
        "        root_topic = 'Langchain'\n",
        "    documentation[root_topic].append(f\"\\n\\n## Class Objects: {topic}\\n{content}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "CYSjDLb6_ehg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CYSjDLb6_ehg",
        "outputId": "6dcbf44e-2d54-4ae3-f3ee-8482ccd20b90"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(len(documentation['google_genai']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "Vcif5OivDRoL",
      "metadata": {
        "id": "Vcif5OivDRoL"
      },
      "outputs": [],
      "source": [
        "documentation = {\n",
        "    topic: \"\\n\".join(contents)\n",
        "    for topic, contents in documentation.items()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "R1TvDgToDe0k",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1TvDgToDe0k",
        "outputId": "be5b2b40-6a1f-464d-aaee-11bd1b331aff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'str'>\n",
            "144531\n"
          ]
        }
      ],
      "source": [
        "print(type(documentation['google_genai']))\n",
        "print(len(documentation['google_genai']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "f6FKC-o6Hkn1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6FKC-o6Hkn1",
        "outputId": "65b2c912-1ec9-4e93-ab17-aba86dfb8747"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['Langchain', 'perplexity', 'google_genai', 'ollama', 'chroma', 'pinecone', 'reference.html', 'nvidia_ai_endpoints', 'upstage', 'aws', 'anthropic', 'fireworks', '_static', 'cerebras', 'search.html', 'sqlserver', 'redis', '_modules', 'prompty', 'text_splitters', 'standard_tests', 'mistralai', 'mongodb', 'together', 'groq', 'cohere', 'experimental', 'nomic', 'openai', 'azure_dynamic_sessions', 'postgres', 'milvus', 'snowflake', 'neo4j', 'xai', 'unstructured', 'qdrant', 'tavily', 'astradb', 'community', 'ibm', 'core', 'google_vertexai', 'azure_ai', 'huggingface', 'elasticsearch', 'google_community', 'langchain', 'weaviate', 'ai21', 'deepseek', 'exa', 'voyageai', 'index.html', 'sema4'])"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documentation.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "WAHsJOmgD3c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "WAHsJOmgD3c8",
        "outputId": "750589c6-73c5-45e9-c429-fe414ff0ab3d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\n## Class Objects: search\\nSearch - ü¶úüîó LangChain  documentation\\n\\nSkip to main content\\n\\nBack to top\\n\\nCtrl+K\\n\\n    Reference\\n  \\n\\nCtrl+K\\n\\nDocs\\n\\nGitHub\\n\\nX / Twitter\\n\\nCtrl+K\\n\\n    Reference\\n  \\n\\nDocs\\n\\nGitHub\\n\\nX / Twitter\\n\\nSearch\\n\\nError\\nPlease activate JavaScript to enable the search functionality.\\n\\nCtrl+K'"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documentation.pop('_static')\n",
        "documentation.pop('index.html')\n",
        "documentation.pop('search.html')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "Eg3TkVlwIb-8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eg3TkVlwIb-8",
        "outputId": "664c8e23-f650-445b-ebc0-98ad6fac9be8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['Langchain', 'perplexity', 'google_genai', 'ollama', 'chroma', 'pinecone', 'reference.html', 'nvidia_ai_endpoints', 'upstage', 'aws', 'anthropic', 'fireworks', 'cerebras', 'sqlserver', 'redis', '_modules', 'prompty', 'text_splitters', 'standard_tests', 'mistralai', 'mongodb', 'together', 'groq', 'cohere', 'experimental', 'nomic', 'openai', 'azure_dynamic_sessions', 'postgres', 'milvus', 'snowflake', 'neo4j', 'xai', 'unstructured', 'qdrant', 'tavily', 'astradb', 'community', 'ibm', 'core', 'google_vertexai', 'azure_ai', 'huggingface', 'elasticsearch', 'google_community', 'langchain', 'weaviate', 'ai21', 'deepseek', 'exa', 'voyageai', 'sema4'])"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documentation.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "162dd756",
      "metadata": {},
      "source": [
        "## Saving Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "-A1_pIA77RxW",
      "metadata": {
        "id": "-A1_pIA77RxW"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "try:\n",
        "    dataset= open('LangDataset.pickle', 'wb')\n",
        "    pickle.dump(documentation, dataset)\n",
        "    dataset.close()\n",
        "\n",
        "except:\n",
        "    print(\"Something went wrong\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c079e1f",
      "metadata": {},
      "source": [
        "## Function Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "qFz5u2DyJgDf",
      "metadata": {
        "id": "qFz5u2DyJgDf"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "grouped = defaultdict(list)\n",
        "\n",
        "for base_class, content in dataset.items():\n",
        "    class_list = base_class.split('/')\n",
        "    root_topic, topic = class_list[4], class_list[-1]\n",
        "    grouped[root_topic].append(f\"\\n## Class Objects: {topic}\\n{content}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "sBZ-pRzI39Fv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBZ-pRzI39Fv",
        "outputId": "aff5f485-09aa-46e0-d94e-aaa7d52ac427"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "defaultdict(list,\n",
              "            {'': ['\\n## Class Objects: \\nabc'],\n",
              "             'ibm': ['\\n## Class Objects: index.html\\nabc',\n",
              "              '\\n## Class Objects: index\\nbcd']})"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grouped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "zWVWzBLU3_wy",
      "metadata": {
        "id": "zWVWzBLU3_wy"
      },
      "outputs": [],
      "source": [
        "combined_dataset = {\n",
        "    topic: \"\\n\".join(contents)\n",
        "    for topic, contents in grouped.items()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "w-CawZJV4s0O",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-CawZJV4s0O",
        "outputId": "7971d825-19c4-4ccc-d65b-226b45f950c2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'': '\\n## Class Objects: \\nabc',\n",
              " 'ibm': '\\n## Class Objects: index.html\\nabc\\n\\n## Class Objects: index\\nbcd'}"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "combined_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "bWvq3fbR4utK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWvq3fbR4utK",
        "outputId": "34621957-1431-4097-de69-6d62ec6d0eb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "My name\n",
            "\n",
            "is\n",
            "\n",
            "\n",
            "\n",
            "DArshil\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Pungalia\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "text = '''My name\n",
        "\n",
        "is\n",
        "\n",
        "\n",
        "\n",
        "DArshil\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Pungalia\n",
        "'''\n",
        "print(text)\n",
        "text = re.sub(r'\\n{3,}', '\\n\\n', text.strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "783d6d95",
      "metadata": {},
      "outputs": [],
      "source": [
        "def loadData(file):\n",
        "    dbfile = open(file, 'rb')\n",
        "    db = pickle.load(dbfile)\n",
        "\n",
        "    return db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a569ada7",
      "metadata": {},
      "outputs": [],
      "source": [
        "data = loadData(r'datasets\\LangDatasetChunked.pickle')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "528863c0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### Instruction: Learn about the perplexity LangChain API.\n",
            "\n",
            "### Part 2 - Module:index(chunk1)\n",
            "\n",
            "langchain-perplexity: 0.1.1\n",
            "\n",
            "langchain-perplexity: 0.1.1#\n",
            "This package provides the Perplexity integration for LangChain.\n",
            "\n",
            "chat_models#\n",
            "Classes\n",
            "\n",
            "chat_models.ChatPerplexity\n",
            "Perplexity AI Chat models API.\n"
          ]
        }
      ],
      "source": [
        "print(data[0]['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "634514e4",
      "metadata": {},
      "source": [
        "## Instruction-Response Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "fbf32fc4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def loadData(file):\n",
        "    dbfile = open(file, 'rb')\n",
        "    db = pickle.load(dbfile)\n",
        "\n",
        "    return db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "83155353",
      "metadata": {},
      "outputs": [],
      "source": [
        "data = loadData(r'datasets\\LangDatasetBetter.pickle')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "adbdd986",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "52\n",
            "<class 'dict'>\n"
          ]
        }
      ],
      "source": [
        "print(len(data))\n",
        "print(type(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "fecc63c3",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['Langchain', 'perplexity', 'google_genai', 'ollama', 'chroma', 'pinecone', 'reference.html', 'nvidia_ai_endpoints', 'upstage', 'aws', 'anthropic', 'fireworks', 'cerebras', 'sqlserver', 'redis', '_modules', 'prompty', 'text_splitters', 'standard_tests', 'mistralai', 'mongodb', 'together', 'groq', 'cohere', 'experimental', 'nomic', 'openai', 'azure_dynamic_sessions', 'postgres', 'milvus', 'snowflake', 'neo4j', 'xai', 'unstructured', 'qdrant', 'tavily', 'astradb', 'community', 'ibm', 'core', 'google_vertexai', 'azure_ai', 'huggingface', 'elasticsearch', 'google_community', 'langchain', 'weaviate', 'ai21', 'deepseek', 'exa', 'voyageai', 'sema4'])"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "b7075672",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    temperature=0.3\n",
        ")\n",
        "\n",
        "parser = JsonOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "3784ae43",
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = PromptTemplate(\n",
        "    template='Generate as many unique questions and answer pairs for the given {topic} from the text and output in a JSON format in which \"instuction\" contains the question and \"response\" contains the answer, questions should include both documentation and implementation questions like code etc:\\n{text}',\n",
        "    input_variables=['topic', 'text'],\n",
        "    partial_variables={'format_instruction': parser.get_format_instructions()})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "a68c87c2",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['text', 'topic'], input_types={}, partial_variables={'format_instruction': 'Return a JSON object.'}, template='Generate as many unique questions and answer pairs for the given {topic} from the text and output in a JSON format in which \"instuction\" contains the question and \"response\" contains the answer, questions should include both documentation and implementation questions like code etc:\\n{text}')"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "544d5dd4",
      "metadata": {},
      "outputs": [],
      "source": [
        "chain = prompt | model | parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "ca027ca8",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing documents: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 52/52 [13:26<00:00, 15.51s/it]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "instruction_key_pairs = []\n",
        "\n",
        "for i, (key, value) in enumerate(tqdm(data.items(), desc=\"Processing documents\", total=len(data))):\n",
        "    pairs = chain.invoke({'topic': key, 'text': value})\n",
        "    instruction_key_pairs.append(pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "7fa06301",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1968"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tuning_examples = 0\n",
        "\n",
        "for i in instruction_key_pairs:\n",
        "    tuning_examples += len(i)\n",
        "\n",
        "tuning_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "80a4f2bd",
      "metadata": {},
      "outputs": [],
      "source": [
        "instruction_key_pairs_combined = [pair for topic in instruction_key_pairs for pair in topic]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "bbed9d56",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1968"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(instruction_key_pairs_combined)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "1f1938bf",
      "metadata": {},
      "outputs": [],
      "source": [
        "upload = open(r'datasets\\InsReDataset.pickle', 'wb')\n",
        "pickle.dump(instruction_key_pairs_combined, upload)\n",
        "upload.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bff5d1e",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
