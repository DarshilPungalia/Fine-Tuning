{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "84d95532",
      "metadata": {
        "id": "84d95532"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "from dotenv import load_dotenv\n",
        "import transformers\n",
        "import torch\n",
        "from trl import SFTTrainer\n",
        "from peft import LoraConfig\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import BitsAndBytesConfig, GemmaTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "04e753c5",
      "metadata": {
        "id": "04e753c5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "497226f8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "497226f8",
        "outputId": "035abf9f-60cd-4def-be4f-2f54e5d2ec74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "Tesla T4\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c4b933a0",
      "metadata": {
        "id": "c4b933a0"
      },
      "outputs": [],
      "source": [
        "def loadData(file):\n",
        "    dbfile = open(file, 'rb')\n",
        "    db = pickle.load(dbfile)\n",
        "\n",
        "    return db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c856e44b",
      "metadata": {
        "id": "c856e44b"
      },
      "outputs": [],
      "source": [
        "dataset = loadData('LangDatasetBetter.pickle')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6AsfLoY8AVSt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6AsfLoY8AVSt",
        "outputId": "933776c4-036a-4300-e3e6-ae33d4be6361"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['Langchain', 'perplexity', 'google_genai', 'ollama', 'chroma', 'pinecone', 'reference.html', 'nvidia_ai_endpoints', 'upstage', 'aws', 'anthropic', 'fireworks', 'cerebras', 'sqlserver', 'redis', '_modules', 'prompty', 'text_splitters', 'standard_tests', 'mistralai', 'mongodb', 'together', 'groq', 'cohere', 'experimental', 'nomic', 'openai', 'azure_dynamic_sessions', 'postgres', 'milvus', 'snowflake', 'neo4j', 'xai', 'unstructured', 'qdrant', 'tavily', 'astradb', 'community', 'ibm', 'core', 'google_vertexai', 'azure_ai', 'huggingface', 'elasticsearch', 'google_community', 'langchain', 'weaviate', 'ai21', 'deepseek', 'exa', 'voyageai', 'sema4'])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "x2nd5s-PAvat",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "x2nd5s-PAvat",
        "outputId": "2bda10cc-ded7-44cc-f95c-193dbf6c9d93"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\n## Class Objects: index\\nlangchain-google-genai: 2.1.4\\n\\nlangchain-google-genai: 2.1.4#\\nLangChain Google Generative AI Integration\\nThis module integrates Google’s Generative AI models, specifically the Gemini series, with the LangChain framework. It provides classes for interacting with chat models and generating embeddings, leveraging Google’s advanced AI capabilities.\\nChat Models\\nThe ChatGoogleGenerativeAI class is the primary interface for interacting with Google’s Gemini chat models. It allows users to send and receive messages using a specified Gemini model, suitable for various conversational AI applications.\\nLLMs\\nThe GoogleGenerativeAI class is the primary interface for interacting with Google’s Gemini LLMs. It allows users to generate text using a specified Gemini model.\\nEmbeddings\\nThe GoogleGenerativeAIEmbeddings class provides functionalities to generate embeddings using Google’s models.\\nThese embeddings can be used for a range of NLP tasks, including semantic analysis, similarity comparisons, and more.\\nInstallation\\nTo install the package, use pip:\\n`python\\npip install -U langchain-google-genai\\n`\\n## Using Chat Models\\nAfter setting up your environment with the required API key, you can interact with the Google Gemini models.\\n```python\\nfrom langchain_google_genai import ChatGoogleGenerativeAI\\nllm = ChatGoogleGenerativeAI(model=”gemini-pro”)\\nllm.invoke(“Sing a ballad of LangChain.”)\\n```\\n## Using LLMs\\nThe package also supports generating text with Google’s models.\\n```python\\nfrom langchain_google_genai import GoogleGenerativeAI\\nllm = GoogleGenerativeAI(model=”gemini-pro”)\\nllm.invoke(“Once upon a time, a library called LangChain”)\\n```\\n## Embedding Generation\\nThe package also supports creating embeddings with Google’s models, useful for textual similarity and other NLP applications.\\n```python\\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings\\nembeddings = GoogleGenerativeAIEmbeddings(model=”models/embedding-001”)\\nembeddings.embed_query(“hello, world!”)\\n```\\n\\nchat_models#\\nClasses\\n\\nchat_models.ChatGoogleGenerativeAI\\nGoogle AI chat models integration.\\n\\nchat_models.ChatGoogleGenerativeAIError\\nCustom exception class for errors associated with the Google GenAI API.\\n\\nembeddings#\\nClasses\\n\\nembeddings.GoogleGenerativeAIEmbeddings\\nGoogle Generative AI Embeddings.\\n\\ngenai_aqa#\\nClasses\\n\\ngenai_aqa.AqaInput\\nInput to GenAIAqa.invoke.\\n\\ngenai_aqa.AqaOutput\\nOutput from GenAIAqa.invoke.\\n\\ngenai_aqa.GenAIAqa\\nGoogle\\'s Attributed Question and Answering service.\\n\\ngoogle_vector_store#\\nClasses\\n\\ngoogle_vector_store.DoesNotExistsException(*,\\xa0...)\\n\\ngoogle_vector_store.GoogleVectorStore(*,\\xa0...)\\nGoogle GenerativeAI Vector Store.\\n\\ngoogle_vector_store.ServerSideEmbedding()\\nDo nothing embedding model where the embedding is done by the server.\\n\\nllms#\\nClasses\\n\\nllms.GoogleGenerativeAI\\nGoogle GenerativeAI models.\\n\\n\\n## Class Objects: langchain_google_genai.embeddings.GoogleGenerativeAIEmbeddings\\nlangchain-google-genai: 2.1.4\\nembeddings\\nGoogleGenerativeAIEmbeddings\\n\\nGoogleGenerativeAIEmbeddings#\\n\\nclass langchain_google_genai.embeddings.GoogleGenerativeAIEmbeddings[source]#\\nBases: BaseModel, Embeddings\\nGoogle Generative AI Embeddings.\\nTo use, you must have either:\\n\\nThe GOOGLE_API_KEY environment variable set with your API key, or\\n\\n2. Pass your API key using the google_api_key kwarg\\nto the GoogleGenerativeAIEmbeddings constructor.\\n\\nExample\\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings\\n\\nembeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\\nembeddings.embed_query(\"What\\'s our Q1 revenue?\")\\n\\nCreate a new model by parsing and validating input data from keyword arguments.\\nRaises [ValidationError][pydantic_core.ValidationError] if the input data cannot be\\nvalidated to form a valid model.\\nself is explicitly positional-only to allow self as a field name.\\n\\nparam client_options: Dict | None = None#\\nA dictionary of client options to pass to the Google API client, such as api_endpoint.\\n\\nparam credentials: Any = None#\\nThe default custom credentials (google.auth.credentials.Credentials) to use when making API calls. If not provided, credentials will be ascertained from the GOOGLE_API_KEY envvar\\n\\nparam google_api_key: SecretStr | None [Optional]#\\nThe Google API key to use. If not provided, the GOOGLE_API_KEY environment variable will be used.\\n\\nparam model: str [Required]#\\nThe name of the embedding model to use. Example: models/embedding-001\\n\\nparam request_options: Dict | None = None#\\nA dictionary of request options to pass to the Google API client.Example: {‘timeout’: 10}\\n\\nparam task_type: str | None = None#\\nThe task type. Valid options include: task_type_unspecified, retrieval_query, retrieval_document, semantic_similarity, classification, and clustering\\n\\nparam transport: str | None = None#\\nA string, one of: [rest, grpc, grpc_asyncio].\\n\\nasync aembed_documents(\\n\\ntexts: list[str],\\n\\n) → list[list[float]]#\\nAsynchronous Embed search docs.\\n\\nParameters:\\ntexts (list[str]) – List of text to embed.\\n\\nReturns:\\nList of embeddings.\\n\\nReturn type:\\nlist[list[float]]\\n\\nasync aembed_query(\\n\\ntext: str,\\n\\n) → list[float]#\\nAsynchronous Embed query text.\\n\\nParameters:\\ntext (str) – Text to embed.\\n\\nReturns:\\nEmbedding.\\n\\nReturn type:\\nlist[float]\\n\\nembed_documents(\\n\\ntexts: List[str],\\n*,\\nbatch_size: int = 100,\\ntask_type: str | None = None,\\ntitles: List[str] | None = None,\\noutput_dimensionality: int | None = None,\\n\\n) → List[List[float]][source]#\\nEmbed a list of strings. Google Generative AI currently\\nsets a max batch size of 100 strings.\\n\\nParameters:\\n\\ntexts (List[str]) – List[str] The list of strings to embed.\\nbatch_size (int) – [int] The batch size of embeddings to send to the model\\ntask_type (str | None) – task_type (https://ai.google.dev/api/rest/v1/TaskType)\\ntitles (List[str] | None) – An optional list of titles for texts provided.\\nRETRIEVAL_DOCUMENT. (Only applicable when TaskType is)\\noutput_dimensionality (int | None) – Optional reduced dimension for the output embedding.\\nhttps – //ai.google.dev/api/rest/v1/models/batchEmbedContents#EmbedContentRequest\\n\\nReturns:\\nList of embeddings, one for each text.\\n\\nReturn type:\\nList[List[float]]\\n\\nembed_query(\\n\\ntext: str,\\ntask_type: str | None = None,\\ntitle: str | None = None,\\noutput_dimensionality: int | None = None,\\n\\n) → List[float][source]#\\nEmbed a text, using the non-batch endpoint:\\nhttps://ai.google.dev/api/rest/v1/models/embedContent#EmbedContentRequest\\n\\nParameters:\\n\\ntext (str) – The text to embed.\\ntask_type (str | None) – task_type (https://ai.google.dev/api/rest/v1/TaskType)\\ntitle (str | None) – An optional title for the text.\\nRETRIEVAL_DOCUMENT. (Only applicable when TaskType is)\\noutput_dimensionality (int | None) – Optional reduced dimension for the output embedding.\\n\\nReturns:\\nEmbedding for the text.\\n\\nReturn type:\\nList[float]\\n\\n On this page\\n  \\n\\nGoogleGenerativeAIEmbeddings\\nclient_options\\ncredentials\\ngoogle_api_key\\nmodel\\nrequest_options\\ntask_type\\ntransport\\naembed_documents()\\naembed_query()\\nembed_documents()\\nembed_query()\\n\\n\\n## Class Objects: genai_aqa\\nlangchain-google-genai: 2.1.4\\ngenai_aqa\\n\\ngenai_aqa#\\nGoogle GenerativeAI Attributed Question and Answering (AQA) service.\\nThe GenAI Semantic AQA API is a managed end to end service that allows\\ndevelopers to create responses grounded on specified passages based on\\na user query. For more information visit:\\nhttps://developers.generativeai.google/guide\\nClasses\\n\\ngenai_aqa.AqaInput\\nInput to GenAIAqa.invoke.\\n\\ngenai_aqa.AqaOutput\\nOutput from GenAIAqa.invoke.\\n\\ngenai_aqa.GenAIAqa\\nGoogle\\'s Attributed Question and Answering service.\\n\\n\\n## Class Objects: langchain_google_genai.genai_aqa.GenAIAqa\\nlangchain-google-genai: 2.1.4\\ngenai_aqa\\nGenAIAqa\\n\\nGenAIAqa#\\n\\nclass langchain_google_genai.genai_aqa.GenAIAqa[source]#\\nBases: RunnableSerializable[AqaInput, AqaOutput]\\nGoogle’s Attributed Question and Answering service.\\nGiven a user’s query and a list of passages, Google’s server will return\\na response that is grounded to the provided list of passages. It will not\\nbase the response on parametric memory.\\n\\nanswer_style#\\nkeyword-only argument. See\\ngoogle.ai.generativelanguage.AnswerStyle for details.\\n\\nConstruct a Google Generative AI AQA model.\\nAll arguments are optional.\\n\\nParameters:\\n\\nanswer_style – See\\ngoogle.ai.generativelanguage.GenerateAnswerRequest.AnswerStyle.\\nsafety_settings – See google.ai.generativelanguage.SafetySetting.\\ntemperature – 0.0 to 1.0.\\n\\nNote\\nGenAIAqa implements the standard Runnable Interface. 🏃\\nThe Runnable Interface has additional methods that are available on runnables, such as with_config, with_types, with_retry, assign, bind, get_graph, and more.\\n\\nparam answer_style: int = 1#\\n\\nasync abatch(\\n\\ninputs: list[Input],\\nconfig: RunnableConfig | list[RunnableConfig] | None = None,\\n*,\\nreturn_exceptions: bool = False,\\n**kwargs: Any | None,\\n\\n) → list[Output]#\\nDefault implementation runs ainvoke in parallel using asyncio.gather.\\nThe default implementation of batch works well for IO bound runnables.\\nSubclasses should override this method if they can batch more efficiently;\\ne.g., if the underlying Runnable uses an API which supports a batch mode.\\n\\nParameters:\\n\\ninputs (list[Input]) – A list of inputs to the Runnable.\\nconfig (RunnableConfig | list[RunnableConfig] | None) – A config to use when invoking the Runnable.\\nThe config supports standard keys like ‘tags’, ‘metadata’ for tracing\\npurposes, ‘max_concurrency’ for controlling how much work to do\\nin parallel, and other keys. Please refer to the RunnableConfig\\nfor more details. Defaults to None.\\nreturn_exceptions (bool) – Whether to return exceptions instead of raising them.\\nDefaults to False.\\nkwargs (Any | None) – Additional keyword arguments to pass to the Runnable.\\n\\nReturns:\\nA list of outputs from the Runnable.\\n\\nReturn type:\\nlist[Output]\\n\\nasync abatch_as_completed(\\n\\ninputs: Sequence[Input],\\nconfig: RunnableConfig | Sequence[RunnableConfig] | None = None,\\n*,\\nreturn_exceptions: bool = False,\\n**kwargs: Any | None,\\n\\n) → AsyncIterator[tuple[int, Output | Exception]]#\\nRun ainvoke in parallel on a list of inputs.\\nYields results as they complete.\\n\\nParameters:\\n\\ninputs (Sequence[Input]) – A list of inputs to the Runnable.\\nconfig (RunnableConfig | Sequence[RunnableConfig] | None) – A config to use when invoking the Runnable.\\nThe config supports standard keys like ‘tags’, ‘metadata’ for tracing\\npurposes, ‘max_concurrency’ for controlling how much work to do\\nin parallel, and other keys. Please refer to the RunnableConfig\\nfor more details. Defaults to None. Defaults to None.\\nreturn_exceptions (bool) – Whether to return exceptions instead of raising them.\\nDefaults to False.\\nkwargs (Any | None) – Additional keyword arguments to pass to the Runnable.\\n\\nYields:\\nA tuple of the index of the input and the output from the Runnable.\\n\\nReturn type:\\nAsyncIterator[tuple[int, Output | Exception]]\\n\\nasync ainvoke(\\n\\ninput: Input,\\nconfig: RunnableConfig | None = None,\\n**kwargs: Any,\\n\\n) → Output#\\nDefault implementation of ainvoke, calls invoke from a thread.\\nThe default implementation allows usage of async code even if\\nthe Runnable did not implement a native async version of invoke.\\nSubclasses should override this method if they can run asynchronously.\\n\\nParameters:\\n\\ninput (Input)\\nconfig (RunnableConfig | None)\\nkwargs (Any)\\n\\nReturn type:\\nOutput\\n\\nasync astream(\\n\\ninput: Input,\\nconfig: RunnableConfig | None = None,\\n**kwargs: Any | None,\\n\\n) → AsyncIterator[Output]#\\nDefault implementation of astream, which calls ainvoke.\\nSubclasses should override this method if they support streaming output.\\n\\nParameters:\\n\\ninput (Input) – The input to the Runnable.\\nconfig (RunnableConfig | None) – The config to use for the Runnable. Defaults to None.\\nkwargs (Any | None) – Additional keyword arguments to pass to the Runnable.\\n\\nYields:\\nThe output of the Runnable.\\n\\nReturn type:\\nAsyncIterator[Output]\\n\\nasync astream_events(\\n\\ninput: Any,\\nconfig: RunnableConfig | None = None,\\n*,\\nversion: Literal[\\'v1\\', \\'v2\\'] = \\'v2\\',\\ninclude_names: Sequence[str] | None = None,\\ninclude_types: Sequence[str] | None = None,\\ninclude_tags: Sequence[str] | None = None,\\nexclude_names: Sequence[str] | None = None,\\nexclude_types: Sequence[str] | None = None,\\nexclude_tags: Sequence[str] | None = None,\\n**kwargs: Any,\\n\\n) → AsyncIterator[StreamEvent]#\\nGenerate a stream of events.\\nUse to create an iterator over StreamEvents that provide real-time information\\nabout the progress of the Runnable, including StreamEvents from intermediate\\nresults.\\nA StreamEvent is a dictionary with the following schema:\\n\\nevent: str - Event names are of theformat: on_[runnable_type]_(start|stream|end).\\n\\nname: str - The name of the Runnable that generated the event.\\n\\nrun_id: str - randomly generated ID associated with the given execution ofthe Runnable that emitted the event.\\nA child Runnable that gets invoked as part of the execution of a\\nparent Runnable is assigned its own unique ID.\\n\\nparent_ids: list[str] - The IDs of the parent runnables thatgenerated the event. The root Runnable will have an empty list.\\nThe order of the parent IDs is from the root to the immediate parent.\\nOnly available for v2 version of the API. The v1 version of the API\\nwill return an empty list.\\n\\ntags: Optional[list[str]] - The tags of the Runnable that generatedthe event.\\n\\nmetadata: Optional[dict[str, Any]] - The metadata of the Runnablethat generated the event.\\n\\ndata: dict[str, Any]\\n\\nBelow is a table that illustrates some events that might be emitted by various\\nchains. Metadata fields have been omitted from the table for brevity.\\nChain definitions have been included after the table.\\nATTENTION This reference table is for the V2 version of the schema.\\n\\nevent\\nname\\nchunk\\ninput\\noutput\\n\\non_chat_model_start\\n[model name]\\n\\n{“messages”: [[SystemMessage, HumanMessage]]}\\n\\non_chat_model_stream\\n[model name]\\nAIMessageChunk(content=”hello”)\\n\\non_chat_model_end\\n[model name]\\n\\n{“messages”: [[SystemMessage, HumanMessage]]}\\nAIMessageChunk(content=”hello world”)\\n\\non_llm_start\\n[model name]\\n\\n{‘input’: ‘hello’}\\n\\non_llm_stream\\n[model name]\\n‘Hello’\\n\\non_llm_end\\n[model name]\\n\\n‘Hello human!’\\n\\non_chain_start\\nformat_docs\\n\\non_chain_stream\\nformat_docs\\n“hello world!, goodbye world!”\\n\\non_chain_end\\nformat_docs\\n\\n[Document(…)]\\n“hello world!, goodbye world!”\\n\\non_tool_start\\nsome_tool\\n\\n{“x”: 1, “y”: “2”}\\n\\non_tool_end\\nsome_tool\\n\\n{“x”: 1, “y”: “2”}\\n\\non_retriever_start\\n[retriever name]\\n\\n{“query”: “hello”}\\n\\non_retriever_end\\n[retriever name]\\n\\n{“query”: “hello”}\\n[Document(…), ..]\\n\\non_prompt_start\\n[template_name]\\n\\n{“question”: “hello”}\\n\\non_prompt_end\\n[template_name]\\n\\n{“question”: “hello”}\\nChatPromptValue(messages: [SystemMessage, …])\\n\\nIn addition to the standard events, users can also dispatch custom events (see example below).\\nCustom events will be only be surfaced with in the v2 version of the API!\\nA custom event has following format:\\n\\nAttribute\\nType\\nDescription\\n\\nname\\nstr\\nA user defined name for the event.\\n\\ndata\\nAny\\nThe data associated with the event. This can be anything, though we suggest making it JSON serializable.\\n\\nHere are declarations associated with the standard events shown above:\\nformat_docs:\\ndef format_docs(docs: list[Document]) -> str:\\n    \\'\\'\\'Format the docs.\\'\\'\\'\\n    return \", \".join([doc.page_content for doc in docs])\\n\\nformat_docs = RunnableLambda(format_docs)\\n\\nsome_tool:\\n@tool\\ndef some_tool(x: int, y: str) -> dict:\\n    \\'\\'\\'Some_tool.\\'\\'\\'\\n    return {\"x\": x, \"y\": y}\\n\\nprompt:\\ntemplate = ChatPromptTemplate.from_messages(\\n    [(\"system\", \"You are Cat Agent 007\"), (\"human\", \"{question}\")]\\n).with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})\\n\\nExample:\\nfrom langchain_core.runnables import RunnableLambda\\n\\nasync def reverse(s: str) -> str:\\n    return s[::-1]\\n\\nchain = RunnableLambda(func=reverse)\\n\\nevents = [\\n    event async for event in chain.astream_events(\"hello\", version=\"v2\")\\n]\\n\\n# will produce the following events (run_id, and parent_ids\\n# has been omitted for brevity):\\n[\\n    {\\n        \"data\": {\"input\": \"hello\"},\\n        \"event\": \"on_chain_start\",\\n        \"metadata\": {},\\n        \"name\": \"reverse\",\\n        \"tags\": [],\\n    },\\n    {\\n        \"data\": {\"chunk\": \"olleh\"},\\n        \"event\": \"on_chain_stream\",\\n        \"metadata\": {},\\n        \"name\": \"reverse\",\\n        \"tags\": [],\\n    },\\n    {\\n        \"data\": {\"output\": \"olleh\"},\\n        \"event\": \"on_chain_end\",\\n        \"metadata\": {},\\n        \"name\": \"reverse\",\\n        \"tags\": [],\\n    },\\n]\\n\\nExample: Dispatch Custom Event\\nfrom langchain_core.callbacks.manager import (\\n    adispatch_custom_event,\\n)\\nfrom langchain_core.runnables import RunnableLambda, RunnableConfig\\nimport asyncio\\n\\nasync def slow_thing(some_input: str, config: RunnableConfig) -> str:\\n    \"\"\"Do something that takes a long time.\"\"\"\\n    await asyncio.sleep(1) # Placeholder for some slow operation\\n    await adispatch_custom_event(\\n        \"progress_event\",\\n        {\"message\": \"Finished step 1 of 3\"},\\n        config=config # Must be included for python < 3.10\\n    )\\n    await asyncio.sleep(1) # Placeholder for some slow operation\\n    await adispatch_custom_event(\\n        \"progress_event\",\\n        {\"message\": \"Finished step 2 of 3\"},\\n        config=config # Must be included for python < 3.10\\n    )\\n    await asyncio.sleep(1) # Placeholder for some slow operation\\n    return \"Done\"\\n\\nslow_thing = RunnableLambda(slow_thing)\\n\\nasync for event in slow_thing.astream_events(\"some_input\", version=\"v2\"):\\n    print(event)\\n\\nParameters:\\n\\ninput (Any) – The input to the Runnable.\\nconfig (Optional[RunnableConfig]) – The config to use for the Runnable.\\nversion (Literal[\\'v1\\', \\'v2\\']) – The version of the schema to use either v2 or v1.\\nUsers should use v2.\\nv1 is for backwards compatibility and will be deprecated\\nin 0.4.0.\\nNo default will be assigned until the API is stabilized.\\ncustom events will only be surfaced in v2.\\ninclude_names (Optional[Sequence[str]]) – Only include events from runnables with matching names.\\ninclude_types (Optional[Sequence[str]]) – Only include events from runnables with matching types.\\ninclude_tags (Optional[Sequence[str]]) – Only include events from runnables with matching tags.\\nexclude_names (Optional[Sequence[str]]) – Exclude events from runnables with matching names.\\nexclude_types (Optional[Sequence[str]]) – Exclude events from runnables with matching types.\\nexclude_tags (Optional[Sequence[str]]) – Exclude events from runnables with matching tags.\\nkwargs (Any) – Additional keyword arguments to pass to the Runnable.\\nThese will be passed to astream_log as this implementation\\nof astream_events is built on top of astream_log.\\n\\nYields:\\nAn async stream of StreamEvents.\\n\\nRaises:\\nNotImplementedError – If the version is not v1 or v2.\\n\\nReturn type:\\nAsyncIterator[StreamEvent]\\n\\nbatch(\\n\\ninputs: list[Input],\\nconfig: RunnableConfig | list[RunnableConfig] | None = None,\\n*,\\nreturn_exceptions: bool = False,\\n**kwargs: Any | None,\\n\\n) → list[Output]#\\nDefault implementation runs invoke in parallel using a thread pool executor.\\nThe default implementation of batch works well for IO bound runnables.\\nSubclasses should override this method if they can batch more efficiently;\\ne.g., if the underlying Runnable uses an API which supports a batch mode.\\n\\nParameters:\\n\\ninputs (list[Input])\\nconfig (RunnableConfig | list[RunnableConfig] | None)\\nreturn_exceptions (bool)\\nkwargs (Any | None)\\n\\nReturn type:\\nlist[Output]\\n\\nbatch_as_completed(\\n\\ninputs: Sequence[Input],\\nconfig: RunnableConfig | Sequence[RunnableConfig] | None = None,\\n*,\\nreturn_exceptions: bool = False,\\n**kwargs: Any | None,\\n\\n) → Iterator[tuple[int, Output | Exception]]#\\nRun invoke in parallel on a list of inputs.\\nYields results as they complete.\\n\\nParameters:\\n\\ninputs (Sequence[Input])\\nconfig (RunnableConfig | Sequence[RunnableConfig] | None)\\nreturn_exceptions (bool)\\nkwargs (Any | None)\\n\\nReturn type:\\nIterator[tuple[int, Output | Exception]]\\n\\nbind(\\n\\n**kwargs: Any,\\n\\n) → Runnable[Input, Output]#\\nBind arguments to a Runnable, returning a new Runnable.\\nUseful when a Runnable in a chain requires an argument that is not\\nin the output of the previous Runnable or included in the user input.\\n\\nParameters:\\nkwargs (Any) – The arguments to bind to the Runnable.\\n\\nReturns:\\nA new Runnable with the arguments bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\nExample:\\nfrom langchain_community.chat_models import ChatOllama\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nllm = ChatOllama(model=\\'llama2\\')\\n\\n# Without bind.\\nchain = (\\n    llm\\n    | StrOutputParser()\\n)\\n\\nchain.invoke(\"Repeat quoted words exactly: \\'One two three four five.\\'\")\\n# Output is \\'One two three four five.\\'\\n\\n# With bind.\\nchain = (\\n    llm.bind(stop=[\"three\"])\\n    | StrOutputParser()\\n)\\n\\nchain.invoke(\"Repeat quoted words exactly: \\'One two three four five.\\'\")\\n# Output is \\'One two\\'\\n\\nconfigurable_alternatives(\\n\\nwhich: ConfigurableField,\\n*,\\ndefault_key: str = \\'default\\',\\nprefix_keys: bool = False,\\n**kwargs: Runnable[Input, Output] | Callable[[], Runnable[Input, Output]],\\n\\n) → RunnableSerializable#\\nConfigure alternatives for Runnables that can be set at runtime.\\n\\nParameters:\\n\\nwhich (ConfigurableField) – The ConfigurableField instance that will be used to select the\\nalternative.\\ndefault_key (str) – The default key to use if no alternative is selected.\\nDefaults to “default”.\\nprefix_keys (bool) – Whether to prefix the keys with the ConfigurableField id.\\nDefaults to False.\\n**kwargs (Runnable[Input, Output] | Callable[[], Runnable[Input, Output]]) – A dictionary of keys to Runnable instances or callables that\\nreturn Runnable instances.\\n\\nReturns:\\nA new Runnable with the alternatives configured.\\n\\nReturn type:\\nRunnableSerializable\\n\\nfrom langchain_anthropic import ChatAnthropic\\nfrom langchain_core.runnables.utils import ConfigurableField\\nfrom langchain_openai import ChatOpenAI\\n\\nmodel = ChatAnthropic(\\n    model_name=\"claude-3-sonnet-20240229\"\\n).configurable_alternatives(\\n    ConfigurableField(id=\"llm\"),\\n    default_key=\"anthropic\",\\n    openai=ChatOpenAI()\\n)\\n\\n# uses the default model ChatAnthropic\\nprint(model.invoke(\"which organization created you?\").content)\\n\\n# uses ChatOpenAI\\nprint(\\n    model.with_config(\\n        configurable={\"llm\": \"openai\"}\\n    ).invoke(\"which organization created you?\").content\\n)\\n\\nconfigurable_fields(\\n\\n**kwargs: ConfigurableField | ConfigurableFieldSingleOption | ConfigurableFieldMultiOption,\\n\\n) → RunnableSerializable#\\nConfigure particular Runnable fields at runtime.\\n\\nParameters:\\n**kwargs (ConfigurableField | ConfigurableFieldSingleOption | ConfigurableFieldMultiOption) – A dictionary of ConfigurableField instances to configure.\\n\\nReturns:\\nA new Runnable with the fields configured.\\n\\nReturn type:\\nRunnableSerializable\\n\\nfrom langchain_core.runnables import ConfigurableField\\nfrom langchain_openai import ChatOpenAI\\n\\nmodel = ChatOpenAI(max_tokens=20).configurable_fields(\\n    max_tokens=ConfigurableField(\\n        id=\"output_token_number\",\\n        name=\"Max tokens in the output\",\\n        description=\"The maximum number of tokens in the output\",\\n    )\\n)\\n\\n# max_tokens = 20\\nprint(\\n    \"max_tokens_20: \",\\n    model.invoke(\"tell me something about chess\").content\\n)\\n\\n# max_tokens = 200\\nprint(\"max_tokens_200: \", model.with_config(\\n    configurable={\"output_token_number\": 200}\\n    ).invoke(\"tell me something about chess\").content\\n)\\n\\ninvoke(\\n\\ninput: AqaInput,\\nconfig: RunnableConfig | None = None,\\n**kwargs: Any,\\n\\n) → AqaOutput[source]#\\nGenerates a grounded response using the provided passages.\\n\\nParameters:\\n\\ninput (AqaInput)\\nconfig (RunnableConfig | None)\\nkwargs (Any)\\n\\nReturn type:\\nAqaOutput\\n\\nstream(\\n\\ninput: Input,\\nconfig: RunnableConfig | None = None,\\n**kwargs: Any | None,\\n\\n) → Iterator[Output]#\\nDefault implementation of stream, which calls invoke.\\nSubclasses should override this method if they support streaming output.\\n\\nParameters:\\n\\ninput (Input) – The input to the Runnable.\\nconfig (RunnableConfig | None) – The config to use for the Runnable. Defaults to None.\\nkwargs (Any | None) – Additional keyword arguments to pass to the Runnable.\\n\\nYields:\\nThe output of the Runnable.\\n\\nReturn type:\\nIterator[Output]\\n\\nwith_alisteners(\\n\\n*,\\non_start: AsyncListener | None = None,\\non_end: AsyncListener | None = None,\\non_error: AsyncListener | None = None,\\n\\n) → Runnable[Input, Output]#\\nBind async lifecycle listeners to a Runnable, returning a new Runnable.\\non_start: Asynchronously called before the Runnable starts running.\\non_end: Asynchronously called after the Runnable finishes running.\\non_error: Asynchronously called if the Runnable throws an error.\\nThe Run object contains information about the run, including its id,\\ntype, input, output, error, start_time, end_time, and any tags or metadata\\nadded to the run.\\n\\nParameters:\\n\\non_start (Optional[AsyncListener]) – Asynchronously called before the Runnable starts running.\\nDefaults to None.\\non_end (Optional[AsyncListener]) – Asynchronously called after the Runnable finishes running.\\nDefaults to None.\\non_error (Optional[AsyncListener]) – Asynchronously called if the Runnable throws an error.\\nDefaults to None.\\n\\nReturns:\\nA new Runnable with the listeners bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\nExample:\\nfrom langchain_core.runnables import RunnableLambda, Runnable\\nfrom datetime import datetime, timezone\\nimport time\\nimport asyncio\\n\\ndef format_t(timestamp: float) -> str:\\n    return datetime.fromtimestamp(timestamp, tz=timezone.utc).isoformat()\\n\\nasync def test_runnable(time_to_sleep : int):\\n    print(f\"Runnable[{time_to_sleep}s]: starts at {format_t(time.time())}\")\\n    await asyncio.sleep(time_to_sleep)\\n    print(f\"Runnable[{time_to_sleep}s]: ends at {format_t(time.time())}\")\\n\\nasync def fn_start(run_obj : Runnable):\\n    print(f\"on start callback starts at {format_t(time.time())}\")\\n    await asyncio.sleep(3)\\n    print(f\"on start callback ends at {format_t(time.time())}\")\\n\\nasync def fn_end(run_obj : Runnable):\\n    print(f\"on end callback starts at {format_t(time.time())}\")\\n    await asyncio.sleep(2)\\n    print(f\"on end callback ends at {format_t(time.time())}\")\\n\\nrunnable = RunnableLambda(test_runnable).with_alisteners(\\n    on_start=fn_start,\\n    on_end=fn_end\\n)\\nasync def concurrent_runs():\\n    await asyncio.gather(runnable.ainvoke(2), runnable.ainvoke(3))\\n\\nasyncio.run(concurrent_runs())\\nResult:\\non start callback starts at 2025-03-01T07:05:22.875378+00:00\\non start callback starts at 2025-03-01T07:05:22.875495+00:00\\non start callback ends at 2025-03-01T07:05:25.878862+00:00\\non start callback ends at 2025-03-01T07:05:25.878947+00:00\\nRunnable[2s]: starts at 2025-03-01T07:05:25.879392+00:00\\nRunnable[3s]: starts at 2025-03-01T07:05:25.879804+00:00\\nRunnable[2s]: ends at 2025-03-01T07:05:27.881998+00:00\\non end callback starts at 2025-03-01T07:05:27.882360+00:00\\nRunnable[3s]: ends at 2025-03-01T07:05:28.881737+00:00\\non end callback starts at 2025-03-01T07:05:28.882428+00:00\\non end callback ends at 2025-03-01T07:05:29.883893+00:00\\non end callback ends at 2025-03-01T07:05:30.884831+00:00\\n\\nwith_config(\\n\\nconfig: RunnableConfig | None = None,\\n**kwargs: Any,\\n\\n) → Runnable[Input, Output]#\\nBind config to a Runnable, returning a new Runnable.\\n\\nParameters:\\n\\nconfig (RunnableConfig | None) – The config to bind to the Runnable.\\nkwargs (Any) – Additional keyword arguments to pass to the Runnable.\\n\\nReturns:\\nA new Runnable with the config bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\nwith_fallbacks(fallbacks: Sequence[Runnable[Input, Output]], *, exceptions_to_handle: tuple[type[BaseException], ...] = (<class \\'Exception\\'>,), exception_key: Optional[str] = None) → RunnableWithFallbacksT[Input, Output]#\\nAdd fallbacks to a Runnable, returning a new Runnable.\\nThe new Runnable will try the original Runnable, and then each fallback\\nin order, upon failures.\\n\\nParameters:\\n\\nfallbacks (Sequence[Runnable[Input, Output]]) – A sequence of runnables to try if the original Runnable fails.\\nexceptions_to_handle (tuple[type[BaseException], ...]) – A tuple of exception types to handle.\\nDefaults to (Exception,).\\nexception_key (Optional[str]) – If string is specified then handled exceptions will be passed\\nto fallbacks as part of the input under the specified key. If None,\\nexceptions will not be passed to fallbacks. If used, the base Runnable\\nand its fallbacks must accept a dictionary as input. Defaults to None.\\n\\nReturns:\\nA new Runnable that will try the original Runnable, and then each\\nfallback in order, upon failures.\\n\\nReturn type:\\nRunnableWithFallbacksT[Input, Output]\\n\\nExample\\nfrom typing import Iterator\\n\\nfrom langchain_core.runnables import RunnableGenerator\\n\\ndef _generate_immediate_error(input: Iterator) -> Iterator[str]:\\n    raise ValueError()\\n    yield \"\"\\n\\ndef _generate(input: Iterator) -> Iterator[str]:\\n    yield from \"foo bar\"\\n\\nrunnable = RunnableGenerator(_generate_immediate_error).with_fallbacks(\\n    [RunnableGenerator(_generate)]\\n    )\\nprint(\\'\\'.join(runnable.stream({}))) #foo bar\\n\\nParameters:\\n\\nfallbacks (Sequence[Runnable[Input, Output]]) – A sequence of runnables to try if the original Runnable fails.\\nexceptions_to_handle (tuple[type[BaseException], ...]) – A tuple of exception types to handle.\\nexception_key (Optional[str]) – If string is specified then handled exceptions will be passed\\nto fallbacks as part of the input under the specified key. If None,\\nexceptions will not be passed to fallbacks. If used, the base Runnable\\nand its fallbacks must accept a dictionary as input.\\n\\nReturns:\\nA new Runnable that will try the original Runnable, and then each\\nfallback in order, upon failures.\\n\\nReturn type:\\nRunnableWithFallbacksT[Input, Output]\\n\\nwith_listeners(\\n\\n*,\\non_start: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None = None,\\non_end: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None = None,\\non_error: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None = None,\\n\\n) → Runnable[Input, Output]#\\nBind lifecycle listeners to a Runnable, returning a new Runnable.\\non_start: Called before the Runnable starts running, with the Run object.\\non_end: Called after the Runnable finishes running, with the Run object.\\non_error: Called if the Runnable throws an error, with the Run object.\\nThe Run object contains information about the run, including its id,\\ntype, input, output, error, start_time, end_time, and any tags or metadata\\nadded to the run.\\n\\nParameters:\\n\\non_start (Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]) – Called before the Runnable starts running. Defaults to None.\\non_end (Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]) – Called after the Runnable finishes running. Defaults to None.\\non_error (Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]) – Called if the Runnable throws an error. Defaults to None.\\n\\nReturns:\\nA new Runnable with the listeners bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\nExample:\\nfrom langchain_core.runnables import RunnableLambda\\nfrom langchain_core.tracers.schemas import Run\\n\\nimport time\\n\\ndef test_runnable(time_to_sleep : int):\\n    time.sleep(time_to_sleep)\\n\\ndef fn_start(run_obj: Run):\\n    print(\"start_time:\", run_obj.start_time)\\n\\ndef fn_end(run_obj: Run):\\n    print(\"end_time:\", run_obj.end_time)\\n\\nchain = RunnableLambda(test_runnable).with_listeners(\\n    on_start=fn_start,\\n    on_end=fn_end\\n)\\nchain.invoke(2)\\n\\nwith_retry(*, retry_if_exception_type: tuple[type[BaseException], ...] = (<class \\'Exception\\'>,), wait_exponential_jitter: bool = True, exponential_jitter_params: Optional[ExponentialJitterParams] = None, stop_after_attempt: int = 3) → Runnable[Input, Output]#\\nCreate a new Runnable that retries the original Runnable on exceptions.\\n\\nParameters:\\n\\nretry_if_exception_type (tuple[type[BaseException], ...]) – A tuple of exception types to retry on.\\nDefaults to (Exception,).\\nwait_exponential_jitter (bool) – Whether to add jitter to the wait\\ntime between retries. Defaults to True.\\nstop_after_attempt (int) – The maximum number of attempts to make before\\ngiving up. Defaults to 3.\\nexponential_jitter_params (Optional[ExponentialJitterParams]) – Parameters for\\ntenacity.wait_exponential_jitter. Namely: initial, max,\\nexp_base, and jitter (all float values).\\n\\nReturns:\\nA new Runnable that retries the original Runnable on exceptions.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\nExample:\\nfrom langchain_core.runnables import RunnableLambda\\n\\ncount = 0\\n\\ndef _lambda(x: int) -> None:\\n    global count\\n    count = count + 1\\n    if x == 1:\\n        raise ValueError(\"x is 1\")\\n    else:\\n         pass\\n\\nrunnable = RunnableLambda(_lambda)\\ntry:\\n    runnable.with_retry(\\n        stop_after_attempt=2,\\n        retry_if_exception_type=(ValueError,),\\n    ).invoke(1)\\nexcept ValueError:\\n    pass\\n\\nassert (count == 2)\\n\\nwith_types(\\n\\n*,\\ninput_type: type[Input] | None = None,\\noutput_type: type[Output] | None = None,\\n\\n) → Runnable[Input, Output]#\\nBind input and output types to a Runnable, returning a new Runnable.\\n\\nParameters:\\n\\ninput_type (type[Input] | None) – The input type to bind to the Runnable. Defaults to None.\\noutput_type (type[Output] | None) – The output type to bind to the Runnable. Defaults to None.\\n\\nReturns:\\nA new Runnable with the types bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n On this page\\n  \\n\\nGenAIAqa\\nanswer_style\\nanswer_style\\nabatch()\\nabatch_as_completed()\\nainvoke()\\nastream()\\nastream_events()\\nbatch()\\nbatch_as_completed()\\nbind()\\nconfigurable_alternatives()\\nconfigurable_fields()\\ninvoke()\\nstream()\\nwith_alisteners()\\nwith_config()\\nwith_fallbacks()\\nwith_listeners()\\nwith_retry()\\nwith_types()\\n\\n\\n## Class Objects: llms\\nlangchain-google-genai: 2.1.4\\nllms\\n\\nllms#\\nClasses\\n\\nllms.GoogleGenerativeAI\\nGoogle GenerativeAI models.\\n\\n\\n## Class Objects: langchain_google_genai.llms.GoogleGenerativeAI\\nlangchain-google-genai: 2.1.4\\nllms\\nGoogleGenerativeAI\\n\\nGoogleGenerativeAI#\\n\\nclass langchain_google_genai.llms.GoogleGenerativeAI[source]#\\nBases: _BaseGoogleGenerativeAI, BaseLLM\\nGoogle GenerativeAI models.\\nExample\\nfrom langchain_google_genai import GoogleGenerativeAI\\nllm = GoogleGenerativeAI(model=\"gemini-pro\")\\n\\nNeeded for arg validation.\\n\\nNote\\nGoogleGenerativeAI implements the standard Runnable Interface. 🏃\\nThe Runnable Interface has additional methods that are available on runnables, such as with_config, with_types, with_retry, assign, bind, get_graph, and more.\\n\\nparam additional_headers: Dict[str, str] | None = None#\\nA key-value dictionary representing additional headers for the model call\\n\\nparam cache: BaseCache | bool | None = None#\\nWhether to cache the response.\\n\\nIf true, will use the global cache.\\nIf false, will not use a cache\\nIf None, will use the global cache if it’s set, otherwise no cache.\\nIf instance of BaseCache, will use the provided cache.\\n\\nCaching is not currently supported for streaming methods of models.\\n\\nparam callback_manager: BaseCallbackManager | None = None#\\n[DEPRECATED]\\n\\nparam callbacks: Callbacks = None#\\nCallbacks to add to the run trace.\\n\\nparam client_options: Dict | None = None#\\nA dictionary of client options to pass to the Google API client, such as api_endpoint.\\n\\nparam credentials: Any = None#\\nThe default custom credentials (google.auth.credentials.Credentials) to use\\n\\nparam custom_get_token_ids: Callable[[str], list[int]] | None = None#\\nOptional encoder to use for counting tokens.\\n\\nparam google_api_key: SecretStr | None [Optional] (alias \\'api_key\\')#\\nGoogle AI API key.\\nIf not specified will be read from env var GOOGLE_API_KEY.\\n\\nparam max_output_tokens: int | None = None (alias \\'max_tokens\\')#\\nMaximum number of tokens to include in a candidate. Must be greater than zero.\\nIf unset, will default to 64.\\n\\nparam max_retries: int = 6#\\nThe maximum number of retries to make when generating.\\n\\nparam metadata: dict[str, Any] | None = None#\\nMetadata to add to the run trace.\\n\\nparam model: str [Required]#\\nModel name to use.\\nThe name of the model to use.\\nSupported examples:\\n\\ngemini-pro\\nmodels/text-bison-001\\n\\nparam n: int = 1#\\nNumber of chat completions to generate for each prompt. Note that the API may\\nnot return the full n completions if duplicates are generated.\\n\\nparam response_modalities: List[Modality] | None = None#\\nA list of modalities of the response\\n\\nparam safety_settings: Dict[HarmCategory, HarmBlockThreshold] | None = None#\\nThe default safety settings to use for all generations.\\nFor example:\\n\\nfrom google.generativeai.types.safety_types import HarmBlockThreshold, HarmCategory\\n\\nsafety_settings = {HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\\nHarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,\\nHarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\\nHarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\\n\\n}\\n\\nparam tags: list[str] | None = None#\\nTags to add to the run trace.\\n\\nparam temperature: float = 0.7#\\nRun inference with this temperature. Must by in the closed interval\\n[0.0, 2.0].\\n\\nparam thinking_budget: int | None = None#\\nIndicates the thinking budget in tokens.\\n\\nparam timeout: float | None = None#\\nThe maximum number of seconds to wait for a response.\\n\\nparam top_k: int | None = None#\\nDecode using top-k sampling: consider the set of top_k most probable tokens.\\nMust be positive.\\n\\nparam top_p: float | None = None#\\nDecode using nucleus sampling: consider the smallest set of tokens whose\\nprobability sum is at least top_p. Must be in the closed interval [0.0, 1.0].\\n\\nparam transport: str | None = None#\\nA string, one of: [rest, grpc, grpc_asyncio].\\n\\nparam verbose: bool [Optional]#\\nWhether to print out response text.\\n\\n__call__(\\n\\nprompt: str,\\nstop: list[str] | None = None,\\ncallbacks: list[BaseCallbackHandler] | BaseCallbackManager | None = None,\\n*,\\ntags: list[str] | None = None,\\nmetadata: dict[str, Any] | None = None,\\n**kwargs: Any,\\n\\n) → str#\\n\\nDeprecated since version 0.1.7: Use invoke() instead. It will not be removed until langchain-core==1.0.\\n\\nCheck Cache and run the LLM on the given prompt and input.\\n\\nParameters:\\n\\nprompt (str) – The prompt to generate from.\\nstop (list[str] | None) – Stop words to use when generating. Model output is cut off at the\\nfirst occurrence of any of these substrings.\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None) – Callbacks to pass through. Used for executing additional\\nfunctionality, such as logging or streaming, throughout generation.\\ntags (list[str] | None) – List of tags to associate with the prompt.\\nmetadata (dict[str, Any] | None) – Metadata to associate with the prompt.\\n**kwargs (Any) – Arbitrary additional keyword arguments. These are usually passed\\nto the model provider API call.\\n\\nReturns:\\nThe generated text.\\n\\nRaises:\\nValueError – If the prompt is not a string.\\n\\nReturn type:\\nstr\\n\\nasync abatch(\\n\\ninputs: list[PromptValue | str | Sequence[BaseMessage | list[str] | tuple[str, str] | str | dict[str, Any]]],\\nconfig: RunnableConfig | list[RunnableConfig] | None = None,\\n*,\\nreturn_exceptions: bool = False,\\n**kwargs: Any,\\n\\n) → list[str]#\\nDefault implementation runs ainvoke in parallel using asyncio.gather.\\nThe default implementation of batch works well for IO bound runnables.\\nSubclasses should override this method if they can batch more efficiently;\\ne.g., if the underlying Runnable uses an API which supports a batch mode.\\n\\nParameters:\\n\\ninputs (list[PromptValue | str | Sequence[BaseMessage | list[str] | tuple[str, str] | str | dict[str, Any]]]) – A list of inputs to the Runnable.\\nconfig (RunnableConfig | list[RunnableConfig] | None) – A config to use when invoking the Runnable.\\nThe config supports standard keys like ‘tags’, ‘metadata’ for tracing\\npurposes, ‘max_concurrency’ for controlling how much work to do\\nin parallel, and other keys. Please refer to the RunnableConfig\\nfor more details. Defaults to None.\\nreturn_exceptions (bool) – Whether to return exceptions instead of raising them.\\nDefaults to False.\\nkwargs (Any) – Additional keyword arguments to pass to the Runnable.\\n\\nReturns:\\nA list of outputs from the Runnable.\\n\\nReturn type:\\nlist[str]\\n\\nasync abatch_as_completed(\\n\\ninputs: Sequence[Input],\\nconfig: RunnableConfig | Sequence[RunnableConfig] | None = None,\\n*,\\nreturn_exceptions: bool = False,\\n**kwargs: Any | None,\\n\\n) → AsyncIterator[tuple[int, Output | Exception]]#\\nRun ainvoke in parallel on a list of inputs.\\nYields results as they complete.\\n\\nParameters:\\n\\ninputs (Sequence[Input]) – A list of inputs to the Runnable.\\nconfig (RunnableConfig | Sequence[RunnableConfig] | None) – A config to use when invoking the Runnable.\\nThe config supports standard keys like ‘tags’, ‘metadata’ for tracing\\npurposes, ‘max_concurrency’ for controlling how much work to do\\nin parallel, and other keys. Please refer to the RunnableConfig\\nfor more details. Defaults to None. Defaults to None.\\nreturn_exceptions (bool) – Whether to return exceptions instead of raising them.\\nDefaults to False.\\nkwargs (Any | None) – Additional keyword arguments to pass to the Runnable.\\n\\nYields:\\nA tuple of the index of the input and the output from the Runnable.\\n\\nReturn type:\\nAsyncIterator[tuple[int, Output | Exception]]\\n\\nasync ainvoke(\\n\\ninput: PromptValue | str | Sequence[BaseMessage | list[str] | tuple[str, str] | str | dict[str, Any]],\\nconfig: RunnableConfig | None = None,\\n*,\\nstop: list[str] | None = None,\\n**kwargs: Any,\\n\\n) → str#\\nDefault implementation of ainvoke, calls invoke from a thread.\\nThe default implementation allows usage of async code even if\\nthe Runnable did not implement a native async version of invoke.\\nSubclasses should override this method if they can run asynchronously.\\n\\nParameters:\\n\\ninput (PromptValue | str | Sequence[BaseMessage | list[str] | tuple[str, str] | str | dict[str, Any]])\\nconfig (RunnableConfig | None)\\nstop (list[str] | None)\\nkwargs (Any)\\n\\nReturn type:\\nstr\\n\\nasync astream(\\n\\ninput: PromptValue | str | Sequence[BaseMessage | list[str] | tuple[str, str] | str | dict[str, Any]],\\nconfig: RunnableConfig | None = None,\\n*,\\nstop: list[str] | None = None,\\n**kwargs: Any,\\n\\n) → AsyncIterator[str]#\\nDefault implementation of astream, which calls ainvoke.\\nSubclasses should override this method if they support streaming output.\\n\\nParameters:\\n\\ninput (PromptValue | str | Sequence[BaseMessage | list[str] | tuple[str, str] | str | dict[str, Any]]) – The input to the Runnable.\\nconfig (RunnableConfig | None) – The config to use for the Runnable. Defaults to None.\\nkwargs (Any) – Additional keyword arguments to pass to the Runnable.\\nstop (list[str] | None)\\n\\nYields:\\nThe output of the Runnable.\\n\\nReturn type:\\nAsyncIterator[str]\\n\\nasync astream_events(\\n\\ninput: Any,\\nconfig: RunnableConfig | None = None,\\n*,\\nversion: Literal[\\'v1\\', \\'v2\\'] = \\'v2\\',\\ninclude_names: Sequence[str] | None = None,\\ninclude_types: Sequence[str] | None = None,\\ninclude_tags: Sequence[str] | None = None,\\nexclude_names: Sequence[str] | None = None,\\nexclude_types: Sequence[str] | None = None,\\nexclude_tags: Sequence[str] | None = None,\\n**kwargs: Any,\\n\\n) → AsyncIterator[StreamEvent]#\\nGenerate a stream of events.\\nUse to create an iterator over StreamEvents that provide real-time information\\nabout the progress of the Runnable, including StreamEvents from intermediate\\nresults.\\nA StreamEvent is a dictionary with the following schema:\\n\\nevent: str - Event names are of theformat: on_[runnable_type]_(start|stream|end).\\n\\nname: str - The name of the Runnable that generated the event.\\n\\nrun_id: str - randomly generated ID associated with the given execution ofthe Runnable that emitted the event.\\nA child Runnable that gets invoked as part of the execution of a\\nparent Runnable is assigned its own unique ID.\\n\\nparent_ids: list[str] - The IDs of the parent runnables thatgenerated the event. The root Runnable will have an empty list.\\nThe order of the parent IDs is from the root to the immediate parent.\\nOnly available for v2 version of the API. The v1 version of the API\\nwill return an empty list.\\n\\ntags: Optional[list[str]] - The tags of the Runnable that generatedthe event.\\n\\nmetadata: Optional[dict[str, Any]] - The metadata of the Runnablethat generated the event.\\n\\ndata: dict[str, Any]\\n\\nBelow is a table that illustrates some events that might be emitted by various\\nchains. Metadata fields have been omitted from the table for brevity.\\nChain definitions have been included after the table.\\nATTENTION This reference table is for the V2 version of the schema.\\n\\nevent\\nname\\nchunk\\ninput\\noutput\\n\\non_chat_model_start\\n[model name]\\n\\n{“messages”: [[SystemMessage, HumanMessage]]}\\n\\non_chat_model_stream\\n[model name]\\nAIMessageChunk(content=”hello”)\\n\\non_chat_model_end\\n[model name]\\n\\n{“messages”: [[SystemMessage, HumanMessage]]}\\nAIMessageChunk(content=”hello world”)\\n\\non_llm_start\\n[model name]\\n\\n{‘input’: ‘hello’}\\n\\non_llm_stream\\n[model name]\\n‘Hello’\\n\\non_llm_end\\n[model name]\\n\\n‘Hello human!’\\n\\non_chain_start\\nformat_docs\\n\\non_chain_stream\\nformat_docs\\n“hello world!, goodbye world!”\\n\\non_chain_end\\nformat_docs\\n\\n[Document(…)]\\n“hello world!, goodbye world!”\\n\\non_tool_start\\nsome_tool\\n\\n{“x”: 1, “y”: “2”}\\n\\non_tool_end\\nsome_tool\\n\\n{“x”: 1, “y”: “2”}\\n\\non_retriever_start\\n[retriever name]\\n\\n{“query”: “hello”}\\n\\non_retriever_end\\n[retriever name]\\n\\n{“query”: “hello”}\\n[Document(…), ..]\\n\\non_prompt_start\\n[template_name]\\n\\n{“question”: “hello”}\\n\\non_prompt_end\\n[template_name]\\n\\n{“question”: “hello”}\\nChatPromptValue(messages: [SystemMessage, …])\\n\\nIn addition to the standard events, users can also dispatch custom events (see example below).\\nCustom events will be only be surfaced with in the v2 version of the API!\\nA custom event has following format:\\n\\nAttribute\\nType\\nDescription\\n\\nname\\nstr\\nA user defined name for the event.\\n\\ndata\\nAny\\nThe data associated with the event. This can be anything, though we suggest making it JSON serializable.\\n\\nHere are declarations associated with the standard events shown above:\\nformat_docs:\\ndef format_docs(docs: list[Document]) -> str:\\n    \\'\\'\\'Format the docs.\\'\\'\\'\\n    return \", \".join([doc.page_content for doc in docs])\\n\\nformat_docs = RunnableLambda(format_docs)\\n\\nsome_tool:\\n@tool\\ndef some_tool(x: int, y: str) -> dict:\\n    \\'\\'\\'Some_tool.\\'\\'\\'\\n    return {\"x\": x, \"y\": y}\\n\\nprompt:\\ntemplate = ChatPromptTemplate.from_messages(\\n    [(\"system\", \"You are Cat Agent 007\"), (\"human\", \"{question}\")]\\n).with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})\\n\\nExample:\\nfrom langchain_core.runnables import RunnableLambda\\n\\nasync def reverse(s: str) -> str:\\n    return s[::-1]\\n\\nchain = RunnableLambda(func=reverse)\\n\\nevents = [\\n    event async for event in chain.astream_events(\"hello\", version=\"v2\")\\n]\\n\\n# will produce the following events (run_id, and parent_ids\\n# has been omitted for brevity):\\n[\\n    {\\n        \"data\": {\"input\": \"hello\"},\\n        \"event\": \"on_chain_start\",\\n        \"metadata\": {},\\n        \"name\": \"reverse\",\\n        \"tags\": [],\\n    },\\n    {\\n        \"data\": {\"chunk\": \"olleh\"},\\n        \"event\": \"on_chain_stream\",\\n        \"metadata\": {},\\n        \"name\": \"reverse\",\\n        \"tags\": [],\\n    },\\n    {\\n        \"data\": {\"output\": \"olleh\"},\\n        \"event\": \"on_chain_end\",\\n        \"metadata\": {},\\n        \"name\": \"reverse\",\\n        \"tags\": [],\\n    },\\n]\\n\\nExample: Dispatch Custom Event\\nfrom langchain_core.callbacks.manager import (\\n    adispatch_custom_event,\\n)\\nfrom langchain_core.runnables import RunnableLambda, RunnableConfig\\nimport asyncio\\n\\nasync def slow_thing(some_input: str, config: RunnableConfig) -> str:\\n    \"\"\"Do something that takes a long time.\"\"\"\\n    await asyncio.sleep(1) # Placeholder for some slow operation\\n    await adispatch_custom_event(\\n        \"progress_event\",\\n        {\"message\": \"Finished step 1 of 3\"},\\n        config=config # Must be included for python < 3.10\\n    )\\n    await asyncio.sleep(1) # Placeholder for some slow operation\\n    await adispatch_custom_event(\\n        \"progress_event\",\\n        {\"message\": \"Finished step 2 of 3\"},\\n        config=config # Must be included for python < 3.10\\n    )\\n    await asyncio.sleep(1) # Placeholder for some slow operation\\n    return \"Done\"\\n\\nslow_thing = RunnableLambda(slow_thing)\\n\\nasync for event in slow_thing.astream_events(\"some_input\", version=\"v2\"):\\n    print(event)\\n\\nParameters:\\n\\ninput (Any) – The input to the Runnable.\\nconfig (Optional[RunnableConfig]) – The config to use for the Runnable.\\nversion (Literal[\\'v1\\', \\'v2\\']) – The version of the schema to use either v2 or v1.\\nUsers should use v2.\\nv1 is for backwards compatibility and will be deprecated\\nin 0.4.0.\\nNo default will be assigned until the API is stabilized.\\ncustom events will only be surfaced in v2.\\ninclude_names (Optional[Sequence[str]]) – Only include events from runnables with matching names.\\ninclude_types (Optional[Sequence[str]]) – Only include events from runnables with matching types.\\ninclude_tags (Optional[Sequence[str]]) – Only include events from runnables with matching tags.\\nexclude_names (Optional[Sequence[str]]) – Exclude events from runnables with matching names.\\nexclude_types (Optional[Sequence[str]]) – Exclude events from runnables with matching types.\\nexclude_tags (Optional[Sequence[str]]) – Exclude events from runnables with matching tags.\\nkwargs (Any) – Additional keyword arguments to pass to the Runnable.\\nThese will be passed to astream_log as this implementation\\nof astream_events is built on top of astream_log.\\n\\nYields:\\nAn async stream of StreamEvents.\\n\\nRaises:\\nNotImplementedError – If the version is not v1 or v2.\\n\\nReturn type:\\nAsyncIterator[StreamEvent]\\n\\nbatch(\\n\\ninputs: list[PromptValue | str | Sequence[BaseMessage | list[str] | tuple[str, str] | str | dict[str, Any]]],\\nconfig: RunnableConfig | list[RunnableConfig] | None = None,\\n*,\\nreturn_exceptions: bool = False,\\n**kwargs: Any,\\n\\n) → list[str]#\\nDefault implementation runs invoke in parallel using a thread pool executor.\\nThe default implementation of batch works well for IO bound runnables.\\nSubclasses should override this method if they can batch more efficiently;\\ne.g., if the underlying Runnable uses an API which supports a batch mode.\\n\\nParameters:\\n\\ninputs (list[PromptValue | str | Sequence[BaseMessage | list[str] | tuple[str, str] | str | dict[str, Any]]])\\nconfig (RunnableConfig | list[RunnableConfig] | None)\\nreturn_exceptions (bool)\\nkwargs (Any)\\n\\nReturn type:\\nlist[str]\\n\\nbatch_as_completed(\\n\\ninputs: Sequence[Input],\\nconfig: RunnableConfig | Sequence[RunnableConfig] | None = None,\\n*,\\nreturn_exceptions: bool = False,\\n**kwargs: Any | None,\\n\\n) → Iterator[tuple[int, Output | Exception]]#\\nRun invoke in parallel on a list of inputs.\\nYields results as they complete.\\n\\nParameters:\\n\\ninputs (Sequence[Input])\\nconfig (RunnableConfig | Sequence[RunnableConfig] | None)\\nreturn_exceptions (bool)\\nkwargs (Any | None)\\n\\nReturn type:\\nIterator[tuple[int, Output | Exception]]\\n\\nbind(\\n\\n**kwargs: Any,\\n\\n) → Runnable[Input, Output]#\\nBind arguments to a Runnable, returning a new Runnable.\\nUseful when a Runnable in a chain requires an argument that is not\\nin the output of the previous Runnable or included in the user input.\\n\\nParameters:\\nkwargs (Any) – The arguments to bind to the Runnable.\\n\\nReturns:\\nA new Runnable with the arguments bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\nExample:\\nfrom langchain_community.chat_models import ChatOllama\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nllm = ChatOllama(model=\\'llama2\\')\\n\\n# Without bind.\\nchain = (\\n    llm\\n    | StrOutputParser()\\n)\\n\\nchain.invoke(\"Repeat quoted words exactly: \\'One two three four five.\\'\")\\n# Output is \\'One two three four five.\\'\\n\\n# With bind.\\nchain = (\\n    llm.bind(stop=[\"three\"])\\n    | StrOutputParser()\\n)\\n\\nchain.invoke(\"Repeat quoted words exactly: \\'One two three four five.\\'\")\\n# Output is \\'One two\\'\\n\\nconfigurable_alternatives(\\n\\nwhich: ConfigurableField,\\n*,\\ndefault_key: str = \\'default\\',\\nprefix_keys: bool = False,\\n**kwargs: Runnable[Input, Output] | Callable[[], Runnable[Input, Output]],\\n\\n) → RunnableSerializable#\\nConfigure alternatives for Runnables that can be set at runtime.\\n\\nParameters:\\n\\nwhich (ConfigurableField) – The ConfigurableField instance that will be used to select the\\nalternative.\\ndefault_key (str) – The default key to use if no alternative is selected.\\nDefaults to “default”.\\nprefix_keys (bool) – Whether to prefix the keys with the ConfigurableField id.\\nDefaults to False.\\n**kwargs (Runnable[Input, Output] | Callable[[], Runnable[Input, Output]]) – A dictionary of keys to Runnable instances or callables that\\nreturn Runnable instances.\\n\\nReturns:\\nA new Runnable with the alternatives configured.\\n\\nReturn type:\\nRunnableSerializable\\n\\nfrom langchain_anthropic import ChatAnthropic\\nfrom langchain_core.runnables.utils import ConfigurableField\\nfrom langchain_openai import ChatOpenAI\\n\\nmodel = ChatAnthropic(\\n    model_name=\"claude-3-sonnet-20240229\"\\n).configurable_alternatives(\\n    ConfigurableField(id=\"llm\"),\\n    default_key=\"anthropic\",\\n    openai=ChatOpenAI()\\n)\\n\\n# uses the default model ChatAnthropic\\nprint(model.invoke(\"which organization created you?\").content)\\n\\n# uses ChatOpenAI\\nprint(\\n    model.with_config(\\n        configurable={\"llm\": \"openai\"}\\n    ).invoke(\"which organization created you?\").content\\n)\\n\\nconfigurable_fields(\\n\\n**kwargs: ConfigurableField | ConfigurableFieldSingleOption | ConfigurableFieldMultiOption,\\n\\n) → RunnableSerializable#\\nConfigure particular Runnable fields at runtime.\\n\\nParameters:\\n**kwargs (ConfigurableField | ConfigurableFieldSingleOption | ConfigurableFieldMultiOption) – A dictionary of ConfigurableField instances to configure.\\n\\nReturns:\\nA new Runnable with the fields configured.\\n\\nReturn type:\\nRunnableSerializable\\n\\nfrom langchain_core.runnables import ConfigurableField\\nfrom langchain_openai import ChatOpenAI\\n\\nmodel = ChatOpenAI(max_tokens=20).configurable_fields(\\n    max_tokens=ConfigurableField(\\n        id=\"output_token_number\",\\n        name=\"Max tokens in the output\",\\n        description=\"The maximum number of tokens in the output\",\\n    )\\n)\\n\\n# max_tokens = 20\\nprint(\\n    \"max_tokens_20: \",\\n    model.invoke(\"tell me something about chess\").content\\n)\\n\\n# max_tokens = 200\\nprint(\"max_tokens_200: \", model.with_config(\\n    configurable={\"output_token_number\": 200}\\n    ).invoke(\"tell me something about chess\").content\\n)\\n\\nget_num_tokens(text: str) → int[source]#\\nGet the number of tokens present in the text.\\nUseful for checking if an input will fit in a model’s context window.\\n\\nParameters:\\ntext (str) – The string input to tokenize.\\n\\nReturns:\\nThe integer number of tokens in the text.\\n\\nReturn type:\\nint\\n\\nget_num_tokens_from_messages(\\n\\nmessages: list[BaseMessage],\\ntools: Sequence | None = None,\\n\\n) → int#\\nGet the number of tokens in the messages.\\nUseful for checking if an input fits in a model’s context window.\\nNote: the base implementation of get_num_tokens_from_messages ignores\\ntool schemas.\\n\\nParameters:\\n\\nmessages (list[BaseMessage]) – The message inputs to tokenize.\\ntools (Sequence | None) – If provided, sequence of dict, BaseModel, function, or BaseTools\\nto be converted to tool schemas.\\n\\nReturns:\\nThe sum of the number of tokens across the messages.\\n\\nReturn type:\\nint\\n\\nget_token_ids(text: str) → list[int]#\\nReturn the ordered ids of the tokens in a text.\\n\\nParameters:\\ntext (str) – The string input to tokenize.\\n\\nReturns:\\n\\nA list of ids corresponding to the tokens in the text, in order they occurin the text.\\n\\nReturn type:\\nlist[int]\\n\\ninvoke(\\n\\ninput: PromptValue | str | Sequence[BaseMessage | list[str] | tuple[str, str] | str | dict[str, Any]],\\nconfig: RunnableConfig | None = None,\\n*,\\nstop: list[str] | None = None,\\n**kwargs: Any,\\n\\n) → str#\\nTransform a single input into an output.\\n\\nParameters:\\n\\ninput (PromptValue | str | Sequence[BaseMessage | list[str] | tuple[str, str] | str | dict[str, Any]]) – The input to the Runnable.\\nconfig (RunnableConfig | None) – A config to use when invoking the Runnable.\\nThe config supports standard keys like ‘tags’, ‘metadata’ for tracing\\npurposes, ‘max_concurrency’ for controlling how much work to do\\nin parallel, and other keys. Please refer to the RunnableConfig\\nfor more details.\\nstop (list[str] | None)\\nkwargs (Any)\\n\\nReturns:\\nThe output of the Runnable.\\n\\nReturn type:\\nstr\\n\\nsave(\\n\\nfile_path: Path | str,\\n\\n) → None#\\nSave the LLM.\\n\\nParameters:\\nfile_path (Path | str) – Path to file to save the LLM to.\\n\\nRaises:\\nValueError – If the file path is not a string or Path object.\\n\\nReturn type:\\nNone\\n\\nExample:\\n.. code-block:: python\\n\\nllm.save(file_path=”path/llm.yaml”)\\n\\nstream(\\n\\ninput: PromptValue | str | Sequence[BaseMessage | list[str] | tuple[str, str] | str | dict[str, Any]],\\nconfig: RunnableConfig | None = None,\\n*,\\nstop: list[str] | None = None,\\n**kwargs: Any,\\n\\n) → Iterator[str]#\\nDefault implementation of stream, which calls invoke.\\nSubclasses should override this method if they support streaming output.\\n\\nParameters:\\n\\ninput (PromptValue | str | Sequence[BaseMessage | list[str] | tuple[str, str] | str | dict[str, Any]]) – The input to the Runnable.\\nconfig (RunnableConfig | None) – The config to use for the Runnable. Defaults to None.\\nkwargs (Any) – Additional keyword arguments to pass to the Runnable.\\nstop (list[str] | None)\\n\\nYields:\\nThe output of the Runnable.\\n\\nReturn type:\\nIterator[str]\\n\\nwith_alisteners(\\n\\n*,\\non_start: AsyncListener | None = None,\\non_end: AsyncListener | None = None,\\non_error: AsyncListener | None = None,\\n\\n) → Runnable[Input, Output]#\\nBind async lifecycle listeners to a Runnable, returning a new Runnable.\\non_start: Asynchronously called before the Runnable starts running.\\non_end: Asynchronously called after the Runnable finishes running.\\non_error: Asynchronously called if the Runnable throws an error.\\nThe Run object contains information about the run, including its id,\\ntype, input, output, error, start_time, end_time, and any tags or metadata\\nadded to the run.\\n\\nParameters:\\n\\non_start (Optional[AsyncListener]) – Asynchronously called before the Runnable starts running.\\nDefaults to None.\\non_end (Optional[AsyncListener]) – Asynchronously called after the Runnable finishes running.\\nDefaults to None.\\non_error (Optional[AsyncListener]) – Asynchronously called if the Runnable throws an error.\\nDefaults to None.\\n\\nReturns:\\nA new Runnable with the listeners bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\nExample:\\nfrom langchain_core.runnables import RunnableLambda, Runnable\\nfrom datetime import datetime, timezone\\nimport time\\nimport asyncio\\n\\ndef format_t(timestamp: float) -> str:\\n    return datetime.fromtimestamp(timestamp, tz=timezone.utc).isoformat()\\n\\nasync def test_runnable(time_to_sleep : int):\\n    print(f\"Runnable[{time_to_sleep}s]: starts at {format_t(time.time())}\")\\n    await asyncio.sleep(time_to_sleep)\\n    print(f\"Runnable[{time_to_sleep}s]: ends at {format_t(time.time())}\")\\n\\nasync def fn_start(run_obj : Runnable):\\n    print(f\"on start callback starts at {format_t(time.time())}\")\\n    await asyncio.sleep(3)\\n    print(f\"on start callback ends at {format_t(time.time())}\")\\n\\nasync def fn_end(run_obj : Runnable):\\n    print(f\"on end callback starts at {format_t(time.time())}\")\\n    await asyncio.sleep(2)\\n    print(f\"on end callback ends at {format_t(time.time())}\")\\n\\nrunnable = RunnableLambda(test_runnable).with_alisteners(\\n    on_start=fn_start,\\n    on_end=fn_end\\n)\\nasync def concurrent_runs():\\n    await asyncio.gather(runnable.ainvoke(2), runnable.ainvoke(3))\\n\\nasyncio.run(concurrent_runs())\\nResult:\\non start callback starts at 2025-03-01T07:05:22.875378+00:00\\non start callback starts at 2025-03-01T07:05:22.875495+00:00\\non start callback ends at 2025-03-01T07:05:25.878862+00:00\\non start callback ends at 2025-03-01T07:05:25.878947+00:00\\nRunnable[2s]: starts at 2025-03-01T07:05:25.879392+00:00\\nRunnable[3s]: starts at 2025-03-01T07:05:25.879804+00:00\\nRunnable[2s]: ends at 2025-03-01T07:05:27.881998+00:00\\non end callback starts at 2025-03-01T07:05:27.882360+00:00\\nRunnable[3s]: ends at 2025-03-01T07:05:28.881737+00:00\\non end callback starts at 2025-03-01T07:05:28.882428+00:00\\non end callback ends at 2025-03-01T07:05:29.883893+00:00\\non end callback ends at 2025-03-01T07:05:30.884831+00:00\\n\\nwith_config(\\n\\nconfig: RunnableConfig | None = None,\\n**kwargs: Any,\\n\\n) → Runnable[Input, Output]#\\nBind config to a Runnable, returning a new Runnable.\\n\\nParameters:\\n\\nconfig (RunnableConfig | None) – The config to bind to the Runnable.\\nkwargs (Any) – Additional keyword arguments to pass to the Runnable.\\n\\nReturns:\\nA new Runnable with the config bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\nwith_fallbacks(fallbacks: Sequence[Runnable[Input, Output]], *, exceptions_to_handle: tuple[type[BaseException], ...] = (<class \\'Exception\\'>,), exception_key: Optional[str] = None) → RunnableWithFallbacksT[Input, Output]#\\nAdd fallbacks to a Runnable, returning a new Runnable.\\nThe new Runnable will try the original Runnable, and then each fallback\\nin order, upon failures.\\n\\nParameters:\\n\\nfallbacks (Sequence[Runnable[Input, Output]]) – A sequence of runnables to try if the original Runnable fails.\\nexceptions_to_handle (tuple[type[BaseException], ...]) – A tuple of exception types to handle.\\nDefaults to (Exception,).\\nexception_key (Optional[str]) – If string is specified then handled exceptions will be passed\\nto fallbacks as part of the input under the specified key. If None,\\nexceptions will not be passed to fallbacks. If used, the base Runnable\\nand its fallbacks must accept a dictionary as input. Defaults to None.\\n\\nReturns:\\nA new Runnable that will try the original Runnable, and then each\\nfallback in order, upon failures.\\n\\nReturn type:\\nRunnableWithFallbacksT[Input, Output]\\n\\nExample\\nfrom typing import Iterator\\n\\nfrom langchain_core.runnables import RunnableGenerator\\n\\ndef _generate_immediate_error(input: Iterator) -> Iterator[str]:\\n    raise ValueError()\\n    yield \"\"\\n\\ndef _generate(input: Iterator) -> Iterator[str]:\\n    yield from \"foo bar\"\\n\\nrunnable = RunnableGenerator(_generate_immediate_error).with_fallbacks(\\n    [RunnableGenerator(_generate)]\\n    )\\nprint(\\'\\'.join(runnable.stream({}))) #foo bar\\n\\nParameters:\\n\\nfallbacks (Sequence[Runnable[Input, Output]]) – A sequence of runnables to try if the original Runnable fails.\\nexceptions_to_handle (tuple[type[BaseException], ...]) – A tuple of exception types to handle.\\nexception_key (Optional[str]) – If string is specified then handled exceptions will be passed\\nto fallbacks as part of the input under the specified key. If None,\\nexceptions will not be passed to fallbacks. If used, the base Runnable\\nand its fallbacks must accept a dictionary as input.\\n\\nReturns:\\nA new Runnable that will try the original Runnable, and then each\\nfallback in order, upon failures.\\n\\nReturn type:\\nRunnableWithFallbacksT[Input, Output]\\n\\nwith_listeners(\\n\\n*,\\non_start: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None = None,\\non_end: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None = None,\\non_error: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None = None,\\n\\n) → Runnable[Input, Output]#\\nBind lifecycle listeners to a Runnable, returning a new Runnable.\\non_start: Called before the Runnable starts running, with the Run object.\\non_end: Called after the Runnable finishes running, with the Run object.\\non_error: Called if the Runnable throws an error, with the Run object.\\nThe Run object contains information about the run, including its id,\\ntype, input, output, error, start_time, end_time, and any tags or metadata\\nadded to the run.\\n\\nParameters:\\n\\non_start (Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]) – Called before the Runnable starts running. Defaults to None.\\non_end (Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]) – Called after the Runnable finishes running. Defaults to None.\\non_error (Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]) – Called if the Runnable throws an error. Defaults to None.\\n\\nReturns:\\nA new Runnable with the listeners bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\nExample:\\nfrom langchain_core.runnables import RunnableLambda\\nfrom langchain_core.tracers.schemas import Run\\n\\nimport time\\n\\ndef test_runnable(time_to_sleep : int):\\n    time.sleep(time_to_sleep)\\n\\ndef fn_start(run_obj: Run):\\n    print(\"start_time:\", run_obj.start_time)\\n\\ndef fn_end(run_obj: Run):\\n    print(\"end_time:\", run_obj.end_time)\\n\\nchain = RunnableLambda(test_runnable).with_listeners(\\n    on_start=fn_start,\\n    on_end=fn_end\\n)\\nchain.invoke(2)\\n\\nwith_retry(*, retry_if_exception_type: tuple[type[BaseException], ...] = (<class \\'Exception\\'>,), wait_exponential_jitter: bool = True, exponential_jitter_params: Optional[ExponentialJitterParams] = None, stop_after_attempt: int = 3) → Runnable[Input, Output]#\\nCreate a new Runnable that retries the original Runnable on exceptions.\\n\\nParameters:\\n\\nretry_if_exception_type (tuple[type[BaseException], ...]) – A tuple of exception types to retry on.\\nDefaults to (Exception,).\\nwait_exponential_jitter (bool) – Whether to add jitter to the wait\\ntime between retries. Defaults to True.\\nstop_after_attempt (int) – The maximum number of attempts to make before\\ngiving up. Defaults to 3.\\nexponential_jitter_params (Optional[ExponentialJitterParams]) – Parameters for\\ntenacity.wait_exponential_jitter. Namely: initial, max,\\nexp_base, and jitter (all float values).\\n\\nReturns:\\nA new Runnable that retries the original Runnable on exceptions.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\nExample:\\nfrom langchain_core.runnables import RunnableLambda\\n\\ncount = 0\\n\\ndef _lambda(x: int) -> None:\\n    global count\\n    count = count + 1\\n    if x == 1:\\n        raise ValueError(\"x is 1\")\\n    else:\\n         pass\\n\\nrunnable = RunnableLambda(_lambda)\\ntry:\\n    runnable.with_retry(\\n        stop_after_attempt=2,\\n        retry_if_exception_type=(ValueError,),\\n    ).invoke(1)\\nexcept ValueError:\\n    pass\\n\\nassert (count == 2)\\n\\nwith_structured_output(\\n\\nschema: dict | type,\\n**kwargs: Any,\\n\\n) → Runnable[PromptValue | str | Sequence[BaseMessage | list[str] | tuple[str, str] | str | dict[str, Any]], dict | BaseModel]#\\nNot implemented on this class.\\n\\nParameters:\\n\\nschema (dict | type)\\nkwargs (Any)\\n\\nReturn type:\\nRunnable[PromptValue | str | Sequence[BaseMessage | list[str] | tuple[str, str] | str | dict[str, Any]], dict | BaseModel]\\n\\nwith_types(\\n\\n*,\\ninput_type: type[Input] | None = None,\\noutput_type: type[Output] | None = None,\\n\\n) → Runnable[Input, Output]#\\nBind input and output types to a Runnable, returning a new Runnable.\\n\\nParameters:\\n\\ninput_type (type[Input] | None) – The input type to bind to the Runnable. Defaults to None.\\noutput_type (type[Output] | None) – The output type to bind to the Runnable. Defaults to None.\\n\\nReturns:\\nA new Runnable with the types bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n On this page\\n  \\n\\nGoogleGenerativeAI\\nadditional_headers\\ncache\\ncallback_manager\\ncallbacks\\nclient_options\\ncredentials\\ncustom_get_token_ids\\ngoogle_api_key\\nmax_output_tokens\\nmax_retries\\nmetadata\\nmodel\\nn\\nresponse_modalities\\nsafety_settings\\ntags\\ntemperature\\nthinking_budget\\ntimeout\\ntop_k\\ntop_p\\ntransport\\nverbose\\n__call__()\\nabatch()\\nabatch_as_completed()\\nainvoke()\\nastream()\\nastream_events()\\nbatch()\\nbatch_as_completed()\\nbind()\\nconfigurable_alternatives()\\nconfigurable_fields()\\nget_num_tokens()\\nget_num_tokens_from_messages()\\nget_token_ids()\\ninvoke()\\nsave()\\nstream()\\nwith_alisteners()\\nwith_config()\\nwith_fallbacks()\\nwith_listeners()\\nwith_retry()\\nwith_structured_output()\\nwith_types()\\n\\n\\n## Class Objects: chat_models\\nlangchain-google-genai: 2.1.4\\nchat_models\\n\\nchat_models#\\nClasses\\n\\nchat_models.ChatGoogleGenerativeAI\\nGoogle AI chat models integration.\\n\\nchat_models.ChatGoogleGenerativeAIError\\nCustom exception class for errors associated with the Google GenAI API.\\n\\n\\n## Class Objects: langchain_google_genai.chat_models.ChatGoogleGenerativeAIError\\nlangchain-google-genai: 2.1.4\\nchat_models\\nChatGoogleGenerativeAIError\\n\\nChatGoogleGenerativeAIError#\\n\\nclass langchain_google_genai.chat_models.ChatGoogleGenerativeAIError[source]#\\nCustom exception class for errors associated with the Google GenAI API.\\nThis exception is raised when there are specific issues related to the\\nGoogle genai API usage in the ChatGoogleGenerativeAI class, such as unsupported\\nmessage types or roles.\\n\\n On this page\\n  \\n\\nChatGoogleGenerativeAIError\\n\\n\\n## Class Objects: langchain_google_genai.chat_models.ChatGoogleGenerativeAI\\nlangchain-google-genai: 2.1.4\\nchat_models\\nChatGoogleGenerativeAI\\n\\nChatGoogleGenerativeAI#\\n\\nclass langchain_google_genai.chat_models.ChatGoogleGenerativeAI[source]#\\nBases: _BaseGoogleGenerativeAI, BaseChatModel\\nGoogle AI chat models integration.\\n\\nInstantiation:To use, you must have either:\\n\\nThe GOOGLE_API_KEY environment variable set with your API key, or\\n\\n2. Pass your API key using the google_api_key kwarg\\nto the ChatGoogleGenerativeAI constructor.\\n\\nfrom langchain_google_genai import ChatGoogleGenerativeAI\\n\\nllm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\\nllm.invoke(\"Write me a ballad about LangChain\")\\n\\nInvoke:messages = [\\n    (\"system\", \"Translate the user sentence to French.\"),\\n    (\"human\", \"I love programming.\"),\\n]\\nllm.invoke(messages)\\n\\nAIMessage(\\n    content=\"J\\'adore programmer. \\\\n\",\\n    response_metadata={\\'prompt_feedback\\': {\\'block_reason\\': 0, \\'safety_ratings\\': []}, \\'finish_reason\\': \\'STOP\\', \\'safety_ratings\\': [{\\'category\\': \\'HARM_CATEGORY_SEXUALLY_EXPLICIT\\', \\'probability\\': \\'NEGLIGIBLE\\', \\'blocked\\': False}, {\\'category\\': \\'HARM_CATEGORY_HATE_SPEECH\\', \\'probability\\': \\'NEGLIGIBLE\\', \\'blocked\\': False}, {\\'category\\': \\'HARM_CATEGORY_HARASSMENT\\', \\'probability\\': \\'NEGLIGIBLE\\', \\'blocked\\': False}, {\\'category\\': \\'HARM_CATEGORY_DANGEROUS_CONTENT\\', \\'probability\\': \\'NEGLIGIBLE\\', \\'blocked\\': False}]},\\n    id=\\'run-56cecc34-2e54-4b52-a974-337e47008ad2-0\\',\\n    usage_metadata={\\'input_tokens\\': 18, \\'output_tokens\\': 5, \\'total_tokens\\': 23}\\n)\\n\\nStream:for chunk in llm.stream(messages):\\n    print(chunk)\\n\\nAIMessageChunk(content=\\'J\\', response_metadata={\\'finish_reason\\': \\'STOP\\', \\'safety_ratings\\': []}, id=\\'run-e905f4f4-58cb-4a10-a960-448a2bb649e3\\', usage_metadata={\\'input_tokens\\': 18, \\'output_tokens\\': 1, \\'total_tokens\\': 19})\\nAIMessageChunk(content=\"\\'adore programmer. \\\\n\", response_metadata={\\'finish_reason\\': \\'STOP\\', \\'safety_ratings\\': [{\\'category\\': \\'HARM_CATEGORY_SEXUALLY_EXPLICIT\\', \\'probability\\': \\'NEGLIGIBLE\\', \\'blocked\\': False}, {\\'category\\': \\'HARM_CATEGORY_HATE_SPEECH\\', \\'probability\\': \\'NEGLIGIBLE\\', \\'blocked\\': False}, {\\'category\\': \\'HARM_CATEGORY_HARASSMENT\\', \\'probability\\': \\'NEGLIGIBLE\\', \\'blocked\\': False}, {\\'category\\': \\'HARM_CATEGORY_DANGEROUS_CONTENT\\', \\'probability\\': \\'NEGLIGIBLE\\', \\'blocked\\': False}]}, id=\\'run-e905f4f4-58cb-4a10-a960-448a2bb649e3\\', usage_metadata={\\'input_tokens\\': 18, \\'output_tokens\\': 5, \\'total_tokens\\': 23})\\n\\nstream = llm.stream(messages)\\nfull = next(stream)\\nfor chunk in stream:\\n    full += chunk\\nfull\\n\\nAIMessageChunk(\\n    content=\"J\\'adore programmer. \\\\n\",\\n    response_metadata={\\'finish_reason\\': \\'STOPSTOP\\', \\'safety_ratings\\': [{\\'category\\': \\'HARM_CATEGORY_SEXUALLY_EXPLICIT\\', \\'probability\\': \\'NEGLIGIBLE\\', \\'blocked\\': False}, {\\'category\\': \\'HARM_CATEGORY_HATE_SPEECH\\', \\'probability\\': \\'NEGLIGIBLE\\', \\'blocked\\': False}, {\\'category\\': \\'HARM_CATEGORY_HARASSMENT\\', \\'probability\\': \\'NEGLIGIBLE\\', \\'blocked\\': False}, {\\'category\\': \\'HARM_CATEGORY_DANGEROUS_CONTENT\\', \\'probability\\': \\'NEGLIGIBLE\\', \\'blocked\\': False}]},\\n    id=\\'run-3ce13a42-cd30-4ad7-a684-f1f0b37cdeec\\',\\n    usage_metadata={\\'input_tokens\\': 36, \\'output_tokens\\': 6, \\'total_tokens\\': 42}\\n)\\n\\nAsync:await llm.ainvoke(messages)\\n\\n# stream:\\n# async for chunk in (await llm.astream(messages))\\n\\n# batch:\\n# await llm.abatch([messages])\\n\\nContext Caching:Context caching allows you to store and reuse content (e.g., PDFs, images) for faster processing.\\nThe cached_content parameter accepts a cache name created via the Google Generative AI API.\\nBelow are two examples: caching a single file directly and caching multiple files using Part.\\nSingle File Example:\\nThis caches a single file and queries it.\\nfrom google import genai\\nfrom google.genai import types\\nimport time\\nfrom langchain_google_genai import ChatGoogleGenerativeAI\\nfrom langchain_core.messages import HumanMessage\\n\\nclient = genai.Client()\\n\\n# Upload file\\nfile = client.files.upload(file=\"./example_file\")\\nwhile file.state.name == \\'PROCESSING\\':\\n    time.sleep(2)\\n    file = client.files.get(name=file.name)\\n\\n# Create cache\\nmodel = \\'models/gemini-1.5-flash-001\\'\\ncache = client.caches.create(\\n    model=model,\\n    config=types.CreateCachedContentConfig(\\n        display_name=\\'Cached Content\\',\\n        system_instruction=(\\n            \\'You are an expert content analyzer, and your job is to answer \\'\\n            \\'the user\\'s query based on the file you have access to.\\'\\n        ),\\n        contents=[file],\\n        ttl=\"300s\",\\n    )\\n)\\n\\n# Query with LangChain\\nllm = ChatGoogleGenerativeAI(\\n    model=model,\\n    cached_content=cache.name,\\n)\\nmessage = HumanMessage(content=\"Summarize the main points of the content.\")\\nllm.invoke([message])\\n\\nMultiple Files Example:\\nThis caches two files using Part and queries them together.\\nfrom google import genai\\nfrom google.genai.types import CreateCachedContentConfig, Content, Part\\nimport time\\nfrom langchain_google_genai import ChatGoogleGenerativeAI\\nfrom langchain_core.messages import HumanMessage\\n\\nclient = genai.Client()\\n\\n# Upload files\\nfile_1 = client.files.upload(file=\"./file1\")\\nwhile file_1.state.name == \\'PROCESSING\\':\\n    time.sleep(2)\\n    file_1 = client.files.get(name=file_1.name)\\n\\nfile_2 = client.files.upload(file=\"./file2\")\\nwhile file_2.state.name == \\'PROCESSING\\':\\n    time.sleep(2)\\n    file_2 = client.files.get(name=file_2.name)\\n\\n# Create cache with multiple files\\ncontents = [\\n    Content(\\n        role=\"user\",\\n        parts=[\\n            Part.from_uri(file_uri=file_1.uri, mime_type=file_1.mime_type),\\n            Part.from_uri(file_uri=file_2.uri, mime_type=file_2.mime_type),\\n        ],\\n    )\\n]\\nmodel = \"gemini-1.5-flash-001\"\\ncache = client.caches.create(\\n    model=model,\\n    config=CreateCachedContentConfig(\\n        display_name=\\'Cached Contents\\',\\n        system_instruction=(\\n            \\'You are an expert content analyzer, and your job is to answer \\'\\n            \\'the user\\'s query based on the files you have access to.\\'\\n        ),\\n        contents=contents,\\n        ttl=\"300s\",\\n    )\\n)\\n\\n# Query with LangChain\\nllm = ChatGoogleGenerativeAI(\\n    model=model,\\n    cached_content=cache.name,\\n)\\nmessage = HumanMessage(content=\"Provide a summary of the key information across both files.\")\\nllm.invoke([message])\\n\\nTool calling:from pydantic import BaseModel, Field\\n\\nclass GetWeather(BaseModel):\\n    \\'\\'\\'Get the current weather in a given location\\'\\'\\'\\n\\n    location: str = Field(\\n        ..., description=\"The city and state, e.g. San Francisco, CA\"\\n    )\\n\\nclass GetPopulation(BaseModel):\\n    \\'\\'\\'Get the current population in a given location\\'\\'\\'\\n\\n    location: str = Field(\\n        ..., description=\"The city and state, e.g. San Francisco, CA\"\\n    )\\n\\nllm_with_tools = llm.bind_tools([GetWeather, GetPopulation])\\nai_msg = llm_with_tools.invoke(\\n    \"Which city is hotter today and which is bigger: LA or NY?\"\\n)\\nai_msg.tool_calls\\n\\n[{\\'name\\': \\'GetWeather\\',\\n  \\'args\\': {\\'location\\': \\'Los Angeles, CA\\'},\\n  \\'id\\': \\'c186c99f-f137-4d52-947f-9e3deabba6f6\\'},\\n {\\'name\\': \\'GetWeather\\',\\n  \\'args\\': {\\'location\\': \\'New York City, NY\\'},\\n  \\'id\\': \\'cebd4a5d-e800-4fa5-babd-4aa286af4f31\\'},\\n {\\'name\\': \\'GetPopulation\\',\\n  \\'args\\': {\\'location\\': \\'Los Angeles, CA\\'},\\n  \\'id\\': \\'4f92d897-f5e4-4d34-a3bc-93062c92591e\\'},\\n {\\'name\\': \\'GetPopulation\\',\\n  \\'args\\': {\\'location\\': \\'New York City, NY\\'},\\n  \\'id\\': \\'634582de-5186-4e4b-968b-f192f0a93678\\'}]\\n\\nUse Search with Gemini 2:from google.ai.generativelanguage_v1beta.types import Tool as GenAITool\\nllm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-exp\")\\nresp = llm.invoke(\\n    \"When is the next total solar eclipse in US?\",\\n    tools=[GenAITool(google_search={})],\\n)\\n\\nStructured output:from typing import Optional\\n\\nfrom pydantic import BaseModel, Field\\n\\nclass Joke(BaseModel):\\n    \\'\\'\\'Joke to tell user.\\'\\'\\'\\n\\n    setup: str = Field(description=\"The setup of the joke\")\\n    punchline: str = Field(description=\"The punchline to the joke\")\\n    rating: Optional[int] = Field(description=\"How funny the joke is, from 1 to 10\")\\n\\nstructured_llm = llm.with_structured_output(Joke)\\nstructured_llm.invoke(\"Tell me a joke about cats\")\\n\\nJoke(\\n    setup=\\'Why are cats so good at video games?\\',\\n    punchline=\\'They have nine lives on the internet\\',\\n    rating=None\\n)\\n\\nImage input:import base64\\nimport httpx\\nfrom langchain_core.messages import HumanMessage\\n\\nimage_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\\nimage_data = base64.b64encode(httpx.get(image_url).content).decode(\"utf-8\")\\nmessage = HumanMessage(\\n    content=[\\n        {\"type\": \"text\", \"text\": \"describe the weather in this image\"},\\n        {\\n            \"type\": \"image_url\",\\n            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"},\\n        },\\n    ]\\n)\\nai_msg = llm.invoke([message])\\nai_msg.content\\n\\n\\'The weather in this image appears to be sunny and pleasant. The sky is a bright blue with scattered white clouds, suggesting fair weather. The lush green grass and trees indicate a warm and possibly slightly breezy day. There are no signs of rain or storms.\\'\\n\\nToken usage:ai_msg = llm.invoke(messages)\\nai_msg.usage_metadata\\n\\n{\\'input_tokens\\': 18, \\'output_tokens\\': 5, \\'total_tokens\\': 23}\\n\\nResponse metadataai_msg = llm.invoke(messages)\\nai_msg.response_metadata\\n\\n{\\n    \\'prompt_feedback\\': {\\'block_reason\\': 0, \\'safety_ratings\\': []},\\n    \\'finish_reason\\': \\'STOP\\',\\n    \\'safety_ratings\\': [{\\'category\\': \\'HARM_CATEGORY_SEXUALLY_EXPLICIT\\', \\'probability\\': \\'NEGLIGIBLE\\', \\'blocked\\': False}, {\\'category\\': \\'HARM_CATEGORY_HATE_SPEECH\\', \\'probability\\': \\'NEGLIGIBLE\\', \\'blocked\\': False}, {\\'category\\': \\'HARM_CATEGORY_HARASSMENT\\', \\'probability\\': \\'NEGLIGIBLE\\', \\'blocked\\': False}, {\\'category\\': \\'HARM_CATEGORY_DANGEROUS_CONTENT\\', \\'probability\\': \\'NEGLIGIBLE\\', \\'blocked\\': False}]\\n}\\n\\nNeeded for arg validation.\\n\\nNote\\nChatGoogleGenerativeAI implements the standard Runnable Interface. 🏃\\nThe Runnable Interface has additional methods that are available on runnables, such as with_config, with_types, with_retry, assign, bind, get_graph, and more.\\n\\nparam additional_headers: Dict[str, str] | None = None#\\nA key-value dictionary representing additional headers for the model call\\n\\nparam cache: BaseCache | bool | None = None#\\nWhether to cache the response.\\n\\nIf true, will use the global cache.\\nIf false, will not use a cache\\nIf None, will use the global cache if it’s set, otherwise no cache.\\nIf instance of BaseCache, will use the provided cache.\\n\\nCaching is not currently supported for streaming methods of models.\\n\\nparam cached_content: str | None = None#\\nThe name of the cached content used as context to serve the prediction.\\nNote: only used in explicit caching, where users can have control over caching\\n(e.g. what content to cache) and enjoy guaranteed cost savings. Format:\\ncachedContents/{cachedContent}.\\n\\nparam callback_manager: BaseCallbackManager | None = None#\\n\\nDeprecated since version 0.1.7: Use callbacks() instead. It will be removed in pydantic==1.0.\\n\\nCallback manager to add to the run trace.\\n\\nparam callbacks: Callbacks = None#\\nCallbacks to add to the run trace.\\n\\nparam client_options: Dict | None = None#\\nA dictionary of client options to pass to the Google API client, such as api_endpoint.\\n\\nparam convert_system_message_to_human: bool = False#\\nWhether to merge any leading SystemMessage into the following HumanMessage.\\nGemini does not support system messages; any unsupported messages will\\nraise an error.\\n\\nparam credentials: Any = None#\\nThe default custom credentials (google.auth.credentials.Credentials) to use\\n\\nparam custom_get_token_ids: Callable[[str], list[int]] | None = None#\\nOptional encoder to use for counting tokens.\\n\\nparam disable_streaming: bool | Literal[\\'tool_calling\\'] = False#\\nWhether to disable streaming for this model.\\nIf streaming is bypassed, then stream()/astream()/astream_events() will\\ndefer to invoke()/ainvoke().\\n\\nIf True, will always bypass streaming case.\\nIf “tool_calling”, will bypass streaming case only when the model is called\\nwith a tools keyword argument.\\nIf False (default), will always use streaming case if available.\\n\\nparam google_api_key: SecretStr | None [Optional] (alias \\'api_key\\')#\\nGoogle AI API key.\\nIf not specified will be read from env var GOOGLE_API_KEY.\\n\\nparam max_output_tokens: int | None = None (alias \\'max_tokens\\')#\\nMaximum number of tokens to include in a candidate. Must be greater than zero.\\nIf unset, will default to 64.\\n\\nparam max_retries: int = 6#\\nThe maximum number of retries to make when generating.\\n\\nparam metadata: dict[str, Any] | None = None#\\nMetadata to add to the run trace.\\n\\nparam model: str [Required]#\\nModel name to use.\\nThe name of the model to use.\\nSupported examples:\\n\\ngemini-pro\\nmodels/text-bison-001\\n\\nparam model_kwargs: dict[str, Any] [Optional]#\\nHolds any unexpected initialization parameters.\\n\\nparam n: int = 1#\\nNumber of chat completions to generate for each prompt. Note that the API may\\nnot return the full n completions if duplicates are generated.\\n\\nparam rate_limiter: BaseRateLimiter | None = None#\\nAn optional rate limiter to use for limiting the number of requests.\\n\\nparam response_modalities: List[Modality] | None = None#\\nA list of modalities of the response\\n\\nparam safety_settings: Dict[HarmCategory, HarmBlockThreshold] | None = None#\\nThe default safety settings to use for all generations.\\nFor example:\\n\\nfrom google.generativeai.types.safety_types import HarmBlockThreshold, HarmCategory\\n\\nsafety_settings = {HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\\nHarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,\\nHarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\\nHarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\\n\\n}\\n\\nparam tags: list[str] | None = None#\\nTags to add to the run trace.\\n\\nparam temperature: float = 0.7#\\nRun inference with this temperature. Must by in the closed interval\\n[0.0, 2.0].\\n\\nparam thinking_budget: int | None = None#\\nIndicates the thinking budget in tokens.\\n\\nparam timeout: float | None = None#\\nThe maximum number of seconds to wait for a response.\\n\\nparam top_k: int | None = None#\\nDecode using top-k sampling: consider the set of top_k most probable tokens.\\nMust be positive.\\n\\nparam top_p: float | None = None#\\nDecode using nucleus sampling: consider the smallest set of tokens whose\\nprobability sum is at least top_p. Must be in the closed interval [0.0, 1.0].\\n\\nparam transport: str | None = None#\\nA string, one of: [rest, grpc, grpc_asyncio].\\n\\nparam verbose: bool [Optional]#\\nWhether to print out response text.\\n\\n__call__(\\n\\nmessages: list[BaseMessage],\\nstop: list[str] | None = None,\\ncallbacks: list[BaseCallbackHandler] | BaseCallbackManager | None = None,\\n**kwargs: Any,\\n\\n) → BaseMessage#\\n\\nDeprecated since version 0.1.7: Use invoke() instead. It will not be removed until langchain-core==1.0.\\n\\nCall the model.\\n\\nParameters:\\n\\nmessages (list[BaseMessage]) – List of messages.\\nstop (list[str] | None) – Stop words to use when generating. Model output is cut off at the\\nfirst occurrence of any of these substrings.\\ncallbacks (list[BaseCallbackHandler] | BaseCallbackManager | None) – Callbacks to pass through. Used for executing additional\\nfunctionality, such as logging or streaming, throughout generation.\\n**kwargs (Any) – Arbitrary additional keyword arguments. These are usually passed\\nto the model provider API call.\\n\\nReturns:\\nThe model output message.\\n\\nReturn type:\\nBaseMessage\\n\\nasync abatch(\\n\\ninputs: list[Input],\\nconfig: RunnableConfig | list[RunnableConfig] | None = None,\\n*,\\nreturn_exceptions: bool = False,\\n**kwargs: Any | None,\\n\\n) → list[Output]#\\nDefault implementation runs ainvoke in parallel using asyncio.gather.\\nThe default implementation of batch works well for IO bound runnables.\\nSubclasses should override this method if they can batch more efficiently;\\ne.g., if the underlying Runnable uses an API which supports a batch mode.\\n\\nParameters:\\n\\ninputs (list[Input]) – A list of inputs to the Runnable.\\nconfig (RunnableConfig | list[RunnableConfig] | None) – A config to use when invoking the Runnable.\\nThe config supports standard keys like ‘tags’, ‘metadata’ for tracing\\npurposes, ‘max_concurrency’ for controlling how much work to do\\nin parallel, and other keys. Please refer to the RunnableConfig\\nfor more details. Defaults to None.\\nreturn_exceptions (bool) – Whether to return exceptions instead of raising them.\\nDefaults to False.\\nkwargs (Any | None) – Additional keyword arguments to pass to the Runnable.\\n\\nReturns:\\nA list of outputs from the Runnable.\\n\\nReturn type:\\nlist[Output]\\n\\nasync abatch_as_completed(\\n\\ninputs: Sequence[Input],\\nconfig: RunnableConfig | Sequence[RunnableConfig] | None = None,\\n*,\\nreturn_exceptions: bool = False,\\n**kwargs: Any | None,\\n\\n) → AsyncIterator[tuple[int, Output | Exception]]#\\nRun ainvoke in parallel on a list of inputs.\\nYields results as they complete.\\n\\nParameters:\\n\\ninputs (Sequence[Input]) – A list of inputs to the Runnable.\\nconfig (RunnableConfig | Sequence[RunnableConfig] | None) – A config to use when invoking the Runnable.\\nThe config supports standard keys like ‘tags’, ‘metadata’ for tracing\\npurposes, ‘max_concurrency’ for controlling how much work to do\\nin parallel, and other keys. Please refer to the RunnableConfig\\nfor more details. Defaults to None. Defaults to None.\\nreturn_exceptions (bool) – Whether to return exceptions instead of raising them.\\nDefaults to False.\\nkwargs (Any | None) – Additional keyword arguments to pass to the Runnable.\\n\\nYields:\\nA tuple of the index of the input and the output from the Runnable.\\n\\nReturn type:\\nAsyncIterator[tuple[int, Output | Exception]]\\n\\nasync ainvoke(\\n\\ninput: LanguageModelInput,\\nconfig: RunnableConfig | None = None,\\n*,\\nstop: list[str] | None = None,\\n**kwargs: Any,\\n\\n) → BaseMessage#\\nDefault implementation of ainvoke, calls invoke from a thread.\\nThe default implementation allows usage of async code even if\\nthe Runnable did not implement a native async version of invoke.\\nSubclasses should override this method if they can run asynchronously.\\n\\nParameters:\\n\\ninput (LanguageModelInput)\\nconfig (Optional[RunnableConfig])\\nstop (Optional[list[str]])\\nkwargs (Any)\\n\\nReturn type:\\nBaseMessage\\n\\nasync astream(\\n\\ninput: LanguageModelInput,\\nconfig: RunnableConfig | None = None,\\n*,\\nstop: list[str] | None = None,\\n**kwargs: Any,\\n\\n) → AsyncIterator[BaseMessageChunk]#\\nDefault implementation of astream, which calls ainvoke.\\nSubclasses should override this method if they support streaming output.\\n\\nParameters:\\n\\ninput (LanguageModelInput) – The input to the Runnable.\\nconfig (Optional[RunnableConfig]) – The config to use for the Runnable. Defaults to None.\\nkwargs (Any) – Additional keyword arguments to pass to the Runnable.\\nstop (Optional[list[str]])\\n\\nYields:\\nThe output of the Runnable.\\n\\nReturn type:\\nAsyncIterator[BaseMessageChunk]\\n\\nasync astream_events(\\n\\ninput: Any,\\nconfig: RunnableConfig | None = None,\\n*,\\nversion: Literal[\\'v1\\', \\'v2\\'] = \\'v2\\',\\ninclude_names: Sequence[str] | None = None,\\ninclude_types: Sequence[str] | None = None,\\ninclude_tags: Sequence[str] | None = None,\\nexclude_names: Sequence[str] | None = None,\\nexclude_types: Sequence[str] | None = None,\\nexclude_tags: Sequence[str] | None = None,\\n**kwargs: Any,\\n\\n) → AsyncIterator[StreamEvent]#\\nGenerate a stream of events.\\nUse to create an iterator over StreamEvents that provide real-time information\\nabout the progress of the Runnable, including StreamEvents from intermediate\\nresults.\\nA StreamEvent is a dictionary with the following schema:\\n\\nevent: str - Event names are of theformat: on_[runnable_type]_(start|stream|end).\\n\\nname: str - The name of the Runnable that generated the event.\\n\\nrun_id: str - randomly generated ID associated with the given execution ofthe Runnable that emitted the event.\\nA child Runnable that gets invoked as part of the execution of a\\nparent Runnable is assigned its own unique ID.\\n\\nparent_ids: list[str] - The IDs of the parent runnables thatgenerated the event. The root Runnable will have an empty list.\\nThe order of the parent IDs is from the root to the immediate parent.\\nOnly available for v2 version of the API. The v1 version of the API\\nwill return an empty list.\\n\\ntags: Optional[list[str]] - The tags of the Runnable that generatedthe event.\\n\\nmetadata: Optional[dict[str, Any]] - The metadata of the Runnablethat generated the event.\\n\\ndata: dict[str, Any]\\n\\nBelow is a table that illustrates some events that might be emitted by various\\nchains. Metadata fields have been omitted from the table for brevity.\\nChain definitions have been included after the table.\\nATTENTION This reference table is for the V2 version of the schema.\\n\\nevent\\nname\\nchunk\\ninput\\noutput\\n\\non_chat_model_start\\n[model name]\\n\\n{“messages”: [[SystemMessage, HumanMessage]]}\\n\\non_chat_model_stream\\n[model name]\\nAIMessageChunk(content=”hello”)\\n\\non_chat_model_end\\n[model name]\\n\\n{“messages”: [[SystemMessage, HumanMessage]]}\\nAIMessageChunk(content=”hello world”)\\n\\non_llm_start\\n[model name]\\n\\n{‘input’: ‘hello’}\\n\\non_llm_stream\\n[model name]\\n‘Hello’\\n\\non_llm_end\\n[model name]\\n\\n‘Hello human!’\\n\\non_chain_start\\nformat_docs\\n\\non_chain_stream\\nformat_docs\\n“hello world!, goodbye world!”\\n\\non_chain_end\\nformat_docs\\n\\n[Document(…)]\\n“hello world!, goodbye world!”\\n\\non_tool_start\\nsome_tool\\n\\n{“x”: 1, “y”: “2”}\\n\\non_tool_end\\nsome_tool\\n\\n{“x”: 1, “y”: “2”}\\n\\non_retriever_start\\n[retriever name]\\n\\n{“query”: “hello”}\\n\\non_retriever_end\\n[retriever name]\\n\\n{“query”: “hello”}\\n[Document(…), ..]\\n\\non_prompt_start\\n[template_name]\\n\\n{“question”: “hello”}\\n\\non_prompt_end\\n[template_name]\\n\\n{“question”: “hello”}\\nChatPromptValue(messages: [SystemMessage, …])\\n\\nIn addition to the standard events, users can also dispatch custom events (see example below).\\nCustom events will be only be surfaced with in the v2 version of the API!\\nA custom event has following format:\\n\\nAttribute\\nType\\nDescription\\n\\nname\\nstr\\nA user defined name for the event.\\n\\ndata\\nAny\\nThe data associated with the event. This can be anything, though we suggest making it JSON serializable.\\n\\nHere are declarations associated with the standard events shown above:\\nformat_docs:\\ndef format_docs(docs: list[Document]) -> str:\\n    \\'\\'\\'Format the docs.\\'\\'\\'\\n    return \", \".join([doc.page_content for doc in docs])\\n\\nformat_docs = RunnableLambda(format_docs)\\n\\nsome_tool:\\n@tool\\ndef some_tool(x: int, y: str) -> dict:\\n    \\'\\'\\'Some_tool.\\'\\'\\'\\n    return {\"x\": x, \"y\": y}\\n\\nprompt:\\ntemplate = ChatPromptTemplate.from_messages(\\n    [(\"system\", \"You are Cat Agent 007\"), (\"human\", \"{question}\")]\\n).with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})\\n\\nExample:\\nfrom langchain_core.runnables import RunnableLambda\\n\\nasync def reverse(s: str) -> str:\\n    return s[::-1]\\n\\nchain = RunnableLambda(func=reverse)\\n\\nevents = [\\n    event async for event in chain.astream_events(\"hello\", version=\"v2\")\\n]\\n\\n# will produce the following events (run_id, and parent_ids\\n# has been omitted for brevity):\\n[\\n    {\\n        \"data\": {\"input\": \"hello\"},\\n        \"event\": \"on_chain_start\",\\n        \"metadata\": {},\\n        \"name\": \"reverse\",\\n        \"tags\": [],\\n    },\\n    {\\n        \"data\": {\"chunk\": \"olleh\"},\\n        \"event\": \"on_chain_stream\",\\n        \"metadata\": {},\\n        \"name\": \"reverse\",\\n        \"tags\": [],\\n    },\\n    {\\n        \"data\": {\"output\": \"olleh\"},\\n        \"event\": \"on_chain_end\",\\n        \"metadata\": {},\\n        \"name\": \"reverse\",\\n        \"tags\": [],\\n    },\\n]\\n\\nExample: Dispatch Custom Event\\nfrom langchain_core.callbacks.manager import (\\n    adispatch_custom_event,\\n)\\nfrom langchain_core.runnables import RunnableLambda, RunnableConfig\\nimport asyncio\\n\\nasync def slow_thing(some_input: str, config: RunnableConfig) -> str:\\n    \"\"\"Do something that takes a long time.\"\"\"\\n    await asyncio.sleep(1) # Placeholder for some slow operation\\n    await adispatch_custom_event(\\n        \"progress_event\",\\n        {\"message\": \"Finished step 1 of 3\"},\\n        config=config # Must be included for python < 3.10\\n    )\\n    await asyncio.sleep(1) # Placeholder for some slow operation\\n    await adispatch_custom_event(\\n        \"progress_event\",\\n        {\"message\": \"Finished step 2 of 3\"},\\n        config=config # Must be included for python < 3.10\\n    )\\n    await asyncio.sleep(1) # Placeholder for some slow operation\\n    return \"Done\"\\n\\nslow_thing = RunnableLambda(slow_thing)\\n\\nasync for event in slow_thing.astream_events(\"some_input\", version=\"v2\"):\\n    print(event)\\n\\nParameters:\\n\\ninput (Any) – The input to the Runnable.\\nconfig (Optional[RunnableConfig]) – The config to use for the Runnable.\\nversion (Literal[\\'v1\\', \\'v2\\']) – The version of the schema to use either v2 or v1.\\nUsers should use v2.\\nv1 is for backwards compatibility and will be deprecated\\nin 0.4.0.\\nNo default will be assigned until the API is stabilized.\\ncustom events will only be surfaced in v2.\\ninclude_names (Optional[Sequence[str]]) – Only include events from runnables with matching names.\\ninclude_types (Optional[Sequence[str]]) – Only include events from runnables with matching types.\\ninclude_tags (Optional[Sequence[str]]) – Only include events from runnables with matching tags.\\nexclude_names (Optional[Sequence[str]]) – Exclude events from runnables with matching names.\\nexclude_types (Optional[Sequence[str]]) – Exclude events from runnables with matching types.\\nexclude_tags (Optional[Sequence[str]]) – Exclude events from runnables with matching tags.\\nkwargs (Any) – Additional keyword arguments to pass to the Runnable.\\nThese will be passed to astream_log as this implementation\\nof astream_events is built on top of astream_log.\\n\\nYields:\\nAn async stream of StreamEvents.\\n\\nRaises:\\nNotImplementedError – If the version is not v1 or v2.\\n\\nReturn type:\\nAsyncIterator[StreamEvent]\\n\\nbatch(\\n\\ninputs: list[Input],\\nconfig: RunnableConfig | list[RunnableConfig] | None = None,\\n*,\\nreturn_exceptions: bool = False,\\n**kwargs: Any | None,\\n\\n) → list[Output]#\\nDefault implementation runs invoke in parallel using a thread pool executor.\\nThe default implementation of batch works well for IO bound runnables.\\nSubclasses should override this method if they can batch more efficiently;\\ne.g., if the underlying Runnable uses an API which supports a batch mode.\\n\\nParameters:\\n\\ninputs (list[Input])\\nconfig (RunnableConfig | list[RunnableConfig] | None)\\nreturn_exceptions (bool)\\nkwargs (Any | None)\\n\\nReturn type:\\nlist[Output]\\n\\nbatch_as_completed(\\n\\ninputs: Sequence[Input],\\nconfig: RunnableConfig | Sequence[RunnableConfig] | None = None,\\n*,\\nreturn_exceptions: bool = False,\\n**kwargs: Any | None,\\n\\n) → Iterator[tuple[int, Output | Exception]]#\\nRun invoke in parallel on a list of inputs.\\nYields results as they complete.\\n\\nParameters:\\n\\ninputs (Sequence[Input])\\nconfig (RunnableConfig | Sequence[RunnableConfig] | None)\\nreturn_exceptions (bool)\\nkwargs (Any | None)\\n\\nReturn type:\\nIterator[tuple[int, Output | Exception]]\\n\\nbind(\\n\\n**kwargs: Any,\\n\\n) → Runnable[Input, Output]#\\nBind arguments to a Runnable, returning a new Runnable.\\nUseful when a Runnable in a chain requires an argument that is not\\nin the output of the previous Runnable or included in the user input.\\n\\nParameters:\\nkwargs (Any) – The arguments to bind to the Runnable.\\n\\nReturns:\\nA new Runnable with the arguments bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\nExample:\\nfrom langchain_community.chat_models import ChatOllama\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nllm = ChatOllama(model=\\'llama2\\')\\n\\n# Without bind.\\nchain = (\\n    llm\\n    | StrOutputParser()\\n)\\n\\nchain.invoke(\"Repeat quoted words exactly: \\'One two three four five.\\'\")\\n# Output is \\'One two three four five.\\'\\n\\n# With bind.\\nchain = (\\n    llm.bind(stop=[\"three\"])\\n    | StrOutputParser()\\n)\\n\\nchain.invoke(\"Repeat quoted words exactly: \\'One two three four five.\\'\")\\n# Output is \\'One two\\'\\n\\nbind_tools(\\n\\ntools: Sequence[dict[str, Any] | type | Callable[[...], Any] | BaseTool | Tool],\\ntool_config: Dict | _ToolConfigDict | None = None,\\n*,\\ntool_choice: dict | List[str] | str | Literal[\\'auto\\', \\'none\\', \\'any\\'] | Literal[True] | bool | None = None,\\n**kwargs: Any,\\n\\n) → Runnable[PromptValue | str | Sequence[BaseMessage | list[str] | tuple[str, str] | str | dict[str, Any]], BaseMessage][source]#\\nBind tool-like objects to this chat model.\\nAssumes model is compatible with google-generativeAI tool-calling API.\\n\\nParameters:\\n\\ntools (Sequence[dict[str, Any] | type | Callable[[...], Any] | BaseTool | Tool]) – A list of tool definitions to bind to this chat model.\\nCan be a pydantic model, callable, or BaseTool. Pydantic\\nmodels, callables, and BaseTools will be automatically converted to\\ntheir schema dictionary representation.\\n**kwargs (Any) – Any additional parameters to pass to the\\nRunnable constructor.\\ntool_config (Dict | _ToolConfigDict | None)\\ntool_choice (dict | List[str] | str | Literal[\\'auto\\', \\'none\\', \\'any\\'] | ~typing.Literal[True] | bool | None)\\n**kwargs\\n\\nReturn type:\\nRunnable[PromptValue | str | Sequence[BaseMessage | list[str] | tuple[str, str] | str | dict[str, Any]], BaseMessage]\\n\\nconfigurable_alternatives(\\n\\nwhich: ConfigurableField,\\n*,\\ndefault_key: str = \\'default\\',\\nprefix_keys: bool = False,\\n**kwargs: Runnable[Input, Output] | Callable[[], Runnable[Input, Output]],\\n\\n) → RunnableSerializable#\\nConfigure alternatives for Runnables that can be set at runtime.\\n\\nParameters:\\n\\nwhich (ConfigurableField) – The ConfigurableField instance that will be used to select the\\nalternative.\\ndefault_key (str) – The default key to use if no alternative is selected.\\nDefaults to “default”.\\nprefix_keys (bool) – Whether to prefix the keys with the ConfigurableField id.\\nDefaults to False.\\n**kwargs (Runnable[Input, Output] | Callable[[], Runnable[Input, Output]]) – A dictionary of keys to Runnable instances or callables that\\nreturn Runnable instances.\\n\\nReturns:\\nA new Runnable with the alternatives configured.\\n\\nReturn type:\\nRunnableSerializable\\n\\nfrom langchain_anthropic import ChatAnthropic\\nfrom langchain_core.runnables.utils import ConfigurableField\\nfrom langchain_openai import ChatOpenAI\\n\\nmodel = ChatAnthropic(\\n    model_name=\"claude-3-sonnet-20240229\"\\n).configurable_alternatives(\\n    ConfigurableField(id=\"llm\"),\\n    default_key=\"anthropic\",\\n    openai=ChatOpenAI()\\n)\\n\\n# uses the default model ChatAnthropic\\nprint(model.invoke(\"which organization created you?\").content)\\n\\n# uses ChatOpenAI\\nprint(\\n    model.with_config(\\n        configurable={\"llm\": \"openai\"}\\n    ).invoke(\"which organization created you?\").content\\n)\\n\\nconfigurable_fields(\\n\\n**kwargs: ConfigurableField | ConfigurableFieldSingleOption | ConfigurableFieldMultiOption,\\n\\n) → RunnableSerializable#\\nConfigure particular Runnable fields at runtime.\\n\\nParameters:\\n**kwargs (ConfigurableField | ConfigurableFieldSingleOption | ConfigurableFieldMultiOption) – A dictionary of ConfigurableField instances to configure.\\n\\nReturns:\\nA new Runnable with the fields configured.\\n\\nReturn type:\\nRunnableSerializable\\n\\nfrom langchain_core.runnables import ConfigurableField\\nfrom langchain_openai import ChatOpenAI\\n\\nmodel = ChatOpenAI(max_tokens=20).configurable_fields(\\n    max_tokens=ConfigurableField(\\n        id=\"output_token_number\",\\n        name=\"Max tokens in the output\",\\n        description=\"The maximum number of tokens in the output\",\\n    )\\n)\\n\\n# max_tokens = 20\\nprint(\\n    \"max_tokens_20: \",\\n    model.invoke(\"tell me something about chess\").content\\n)\\n\\n# max_tokens = 200\\nprint(\"max_tokens_200: \", model.with_config(\\n    configurable={\"output_token_number\": 200}\\n    ).invoke(\"tell me something about chess\").content\\n)\\n\\nget_num_tokens(text: str) → int[source]#\\nGet the number of tokens present in the text.\\nUseful for checking if an input will fit in a model’s context window.\\n\\nParameters:\\ntext (str) – The string input to tokenize.\\n\\nReturns:\\nThe integer number of tokens in the text.\\n\\nReturn type:\\nint\\n\\nget_num_tokens_from_messages(\\n\\nmessages: list[BaseMessage],\\ntools: Sequence | None = None,\\n\\n) → int#\\nGet the number of tokens in the messages.\\nUseful for checking if an input fits in a model’s context window.\\nNote: the base implementation of get_num_tokens_from_messages ignores\\ntool schemas.\\n\\nParameters:\\n\\nmessages (list[BaseMessage]) – The message inputs to tokenize.\\ntools (Sequence | None) – If provided, sequence of dict, BaseModel, function, or BaseTools\\nto be converted to tool schemas.\\n\\nReturns:\\nThe sum of the number of tokens across the messages.\\n\\nReturn type:\\nint\\n\\nget_token_ids(text: str) → list[int]#\\nReturn the ordered ids of the tokens in a text.\\n\\nParameters:\\ntext (str) – The string input to tokenize.\\n\\nReturns:\\n\\nA list of ids corresponding to the tokens in the text, in order they occurin the text.\\n\\nReturn type:\\nlist[int]\\n\\ninvoke(\\n\\ninput: PromptValue | str | Sequence[BaseMessage | list[str] | tuple[str, str] | str | dict[str, Any]],\\nconfig: RunnableConfig | None = None,\\n*,\\ncode_execution: bool | None = None,\\nstop: list[str] | None = None,\\n**kwargs: Any,\\n\\n) → BaseMessage[source]#\\nEnable code execution. Supported on: gemini-1.5-pro, gemini-1.5-flash,\\ngemini-2.0-flash, and gemini-2.0-pro. When enabled, the model can execute\\ncode to solve problems.\\n\\nParameters:\\n\\ninput (PromptValue | str | Sequence[BaseMessage | list[str] | tuple[str, str] | str | dict[str, Any]])\\nconfig (RunnableConfig | None)\\ncode_execution (bool | None)\\nstop (list[str] | None)\\nkwargs (Any)\\n\\nReturn type:\\nBaseMessage\\n\\nstream(\\n\\ninput: LanguageModelInput,\\nconfig: RunnableConfig | None = None,\\n*,\\nstop: list[str] | None = None,\\n**kwargs: Any,\\n\\n) → Iterator[BaseMessageChunk]#\\nDefault implementation of stream, which calls invoke.\\nSubclasses should override this method if they support streaming output.\\n\\nParameters:\\n\\ninput (LanguageModelInput) – The input to the Runnable.\\nconfig (Optional[RunnableConfig]) – The config to use for the Runnable. Defaults to None.\\nkwargs (Any) – Additional keyword arguments to pass to the Runnable.\\nstop (Optional[list[str]])\\n\\nYields:\\nThe output of the Runnable.\\n\\nReturn type:\\nIterator[BaseMessageChunk]\\n\\nwith_alisteners(\\n\\n*,\\non_start: AsyncListener | None = None,\\non_end: AsyncListener | None = None,\\non_error: AsyncListener | None = None,\\n\\n) → Runnable[Input, Output]#\\nBind async lifecycle listeners to a Runnable, returning a new Runnable.\\non_start: Asynchronously called before the Runnable starts running.\\non_end: Asynchronously called after the Runnable finishes running.\\non_error: Asynchronously called if the Runnable throws an error.\\nThe Run object contains information about the run, including its id,\\ntype, input, output, error, start_time, end_time, and any tags or metadata\\nadded to the run.\\n\\nParameters:\\n\\non_start (Optional[AsyncListener]) – Asynchronously called before the Runnable starts running.\\nDefaults to None.\\non_end (Optional[AsyncListener]) – Asynchronously called after the Runnable finishes running.\\nDefaults to None.\\non_error (Optional[AsyncListener]) – Asynchronously called if the Runnable throws an error.\\nDefaults to None.\\n\\nReturns:\\nA new Runnable with the listeners bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\nExample:\\nfrom langchain_core.runnables import RunnableLambda, Runnable\\nfrom datetime import datetime, timezone\\nimport time\\nimport asyncio\\n\\ndef format_t(timestamp: float) -> str:\\n    return datetime.fromtimestamp(timestamp, tz=timezone.utc).isoformat()\\n\\nasync def test_runnable(time_to_sleep : int):\\n    print(f\"Runnable[{time_to_sleep}s]: starts at {format_t(time.time())}\")\\n    await asyncio.sleep(time_to_sleep)\\n    print(f\"Runnable[{time_to_sleep}s]: ends at {format_t(time.time())}\")\\n\\nasync def fn_start(run_obj : Runnable):\\n    print(f\"on start callback starts at {format_t(time.time())}\")\\n    await asyncio.sleep(3)\\n    print(f\"on start callback ends at {format_t(time.time())}\")\\n\\nasync def fn_end(run_obj : Runnable):\\n    print(f\"on end callback starts at {format_t(time.time())}\")\\n    await asyncio.sleep(2)\\n    print(f\"on end callback ends at {format_t(time.time())}\")\\n\\nrunnable = RunnableLambda(test_runnable).with_alisteners(\\n    on_start=fn_start,\\n    on_end=fn_end\\n)\\nasync def concurrent_runs():\\n    await asyncio.gather(runnable.ainvoke(2), runnable.ainvoke(3))\\n\\nasyncio.run(concurrent_runs())\\nResult:\\non start callback starts at 2025-03-01T07:05:22.875378+00:00\\non start callback starts at 2025-03-01T07:05:22.875495+00:00\\non start callback ends at 2025-03-01T07:05:25.878862+00:00\\non start callback ends at 2025-03-01T07:05:25.878947+00:00\\nRunnable[2s]: starts at 2025-03-01T07:05:25.879392+00:00\\nRunnable[3s]: starts at 2025-03-01T07:05:25.879804+00:00\\nRunnable[2s]: ends at 2025-03-01T07:05:27.881998+00:00\\non end callback starts at 2025-03-01T07:05:27.882360+00:00\\nRunnable[3s]: ends at 2025-03-01T07:05:28.881737+00:00\\non end callback starts at 2025-03-01T07:05:28.882428+00:00\\non end callback ends at 2025-03-01T07:05:29.883893+00:00\\non end callback ends at 2025-03-01T07:05:30.884831+00:00\\n\\nwith_config(\\n\\nconfig: RunnableConfig | None = None,\\n**kwargs: Any,\\n\\n) → Runnable[Input, Output]#\\nBind config to a Runnable, returning a new Runnable.\\n\\nParameters:\\n\\nconfig (RunnableConfig | None) – The config to bind to the Runnable.\\nkwargs (Any) – Additional keyword arguments to pass to the Runnable.\\n\\nReturns:\\nA new Runnable with the config bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\nwith_fallbacks(fallbacks: Sequence[Runnable[Input, Output]], *, exceptions_to_handle: tuple[type[BaseException], ...] = (<class \\'Exception\\'>,), exception_key: Optional[str] = None) → RunnableWithFallbacksT[Input, Output]#\\nAdd fallbacks to a Runnable, returning a new Runnable.\\nThe new Runnable will try the original Runnable, and then each fallback\\nin order, upon failures.\\n\\nParameters:\\n\\nfallbacks (Sequence[Runnable[Input, Output]]) – A sequence of runnables to try if the original Runnable fails.\\nexceptions_to_handle (tuple[type[BaseException], ...]) – A tuple of exception types to handle.\\nDefaults to (Exception,).\\nexception_key (Optional[str]) – If string is specified then handled exceptions will be passed\\nto fallbacks as part of the input under the specified key. If None,\\nexceptions will not be passed to fallbacks. If used, the base Runnable\\nand its fallbacks must accept a dictionary as input. Defaults to None.\\n\\nReturns:\\nA new Runnable that will try the original Runnable, and then each\\nfallback in order, upon failures.\\n\\nReturn type:\\nRunnableWithFallbacksT[Input, Output]\\n\\nExample\\nfrom typing import Iterator\\n\\nfrom langchain_core.runnables import RunnableGenerator\\n\\ndef _generate_immediate_error(input: Iterator) -> Iterator[str]:\\n    raise ValueError()\\n    yield \"\"\\n\\ndef _generate(input: Iterator) -> Iterator[str]:\\n    yield from \"foo bar\"\\n\\nrunnable = RunnableGenerator(_generate_immediate_error).with_fallbacks(\\n    [RunnableGenerator(_generate)]\\n    )\\nprint(\\'\\'.join(runnable.stream({}))) #foo bar\\n\\nParameters:\\n\\nfallbacks (Sequence[Runnable[Input, Output]]) – A sequence of runnables to try if the original Runnable fails.\\nexceptions_to_handle (tuple[type[BaseException], ...]) – A tuple of exception types to handle.\\nexception_key (Optional[str]) – If string is specified then handled exceptions will be passed\\nto fallbacks as part of the input under the specified key. If None,\\nexceptions will not be passed to fallbacks. If used, the base Runnable\\nand its fallbacks must accept a dictionary as input.\\n\\nReturns:\\nA new Runnable that will try the original Runnable, and then each\\nfallback in order, upon failures.\\n\\nReturn type:\\nRunnableWithFallbacksT[Input, Output]\\n\\nwith_listeners(\\n\\n*,\\non_start: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None = None,\\non_end: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None = None,\\non_error: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None = None,\\n\\n) → Runnable[Input, Output]#\\nBind lifecycle listeners to a Runnable, returning a new Runnable.\\non_start: Called before the Runnable starts running, with the Run object.\\non_end: Called after the Runnable finishes running, with the Run object.\\non_error: Called if the Runnable throws an error, with the Run object.\\nThe Run object contains information about the run, including its id,\\ntype, input, output, error, start_time, end_time, and any tags or metadata\\nadded to the run.\\n\\nParameters:\\n\\non_start (Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]) – Called before the Runnable starts running. Defaults to None.\\non_end (Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]) – Called after the Runnable finishes running. Defaults to None.\\non_error (Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]) – Called if the Runnable throws an error. Defaults to None.\\n\\nReturns:\\nA new Runnable with the listeners bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\nExample:\\nfrom langchain_core.runnables import RunnableLambda\\nfrom langchain_core.tracers.schemas import Run\\n\\nimport time\\n\\ndef test_runnable(time_to_sleep : int):\\n    time.sleep(time_to_sleep)\\n\\ndef fn_start(run_obj: Run):\\n    print(\"start_time:\", run_obj.start_time)\\n\\ndef fn_end(run_obj: Run):\\n    print(\"end_time:\", run_obj.end_time)\\n\\nchain = RunnableLambda(test_runnable).with_listeners(\\n    on_start=fn_start,\\n    on_end=fn_end\\n)\\nchain.invoke(2)\\n\\nwith_retry(*, retry_if_exception_type: tuple[type[BaseException], ...] = (<class \\'Exception\\'>,), wait_exponential_jitter: bool = True, exponential_jitter_params: Optional[ExponentialJitterParams] = None, stop_after_attempt: int = 3) → Runnable[Input, Output]#\\nCreate a new Runnable that retries the original Runnable on exceptions.\\n\\nParameters:\\n\\nretry_if_exception_type (tuple[type[BaseException], ...]) – A tuple of exception types to retry on.\\nDefaults to (Exception,).\\nwait_exponential_jitter (bool) – Whether to add jitter to the wait\\ntime between retries. Defaults to True.\\nstop_after_attempt (int) – The maximum number of attempts to make before\\ngiving up. Defaults to 3.\\nexponential_jitter_params (Optional[ExponentialJitterParams]) – Parameters for\\ntenacity.wait_exponential_jitter. Namely: initial, max,\\nexp_base, and jitter (all float values).\\n\\nReturns:\\nA new Runnable that retries the original Runnable on exceptions.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\nExample:\\nfrom langchain_core.runnables import RunnableLambda\\n\\ncount = 0\\n\\ndef _lambda(x: int) -> None:\\n    global count\\n    count = count + 1\\n    if x == 1:\\n        raise ValueError(\"x is 1\")\\n    else:\\n         pass\\n\\nrunnable = RunnableLambda(_lambda)\\ntry:\\n    runnable.with_retry(\\n        stop_after_attempt=2,\\n        retry_if_exception_type=(ValueError,),\\n    ).invoke(1)\\nexcept ValueError:\\n    pass\\n\\nassert (count == 2)\\n\\nwith_structured_output(\\n\\nschema: Dict | Type[BaseModel],\\n*,\\ninclude_raw: bool = False,\\n**kwargs: Any,\\n\\n) → Runnable[PromptValue | str | Sequence[BaseMessage | list[str] | tuple[str, str] | str | dict[str, Any]], Dict | BaseModel][source]#\\nModel wrapper that returns outputs formatted to match the given schema.\\n\\nParameters:\\n\\nschema (Dict | Type[BaseModel]) – \\nThe output schema. Can be passed in as:\\nan OpenAI function/tool schema,\\na JSON Schema,\\na TypedDict class,\\nor a Pydantic class.\\n\\nIf schema is a Pydantic class then the model output will be a\\nPydantic instance of that class, and the model-generated fields will be\\nvalidated by the Pydantic class. Otherwise the model output will be a\\ndict and will not be validated. See langchain_core.utils.function_calling.convert_to_openai_tool()\\nfor more on how to properly specify types and descriptions of\\nschema fields when specifying a Pydantic or TypedDict class.\\n\\ninclude_raw (bool) – If False then only the parsed structured output is returned. If\\nan error occurs during model output parsing it will be raised. If True\\nthen both the raw model response (a BaseMessage) and the parsed model\\nresponse will be returned. If an error occurs during output parsing it\\nwill be caught and returned as well. The final output is always a dict\\nwith keys “raw”, “parsed”, and “parsing_error”.\\nkwargs (Any)\\n\\nReturns:\\nA Runnable that takes same inputs as a langchain_core.language_models.chat.BaseChatModel.\\nIf include_raw is False and schema is a Pydantic class, Runnable outputs\\nan instance of schema (i.e., a Pydantic object).\\nOtherwise, if include_raw is False then Runnable outputs a dict.\\n\\nIf include_raw is True, then Runnable outputs a dict with keys:\\n\"raw\": BaseMessage\\n\"parsed\": None if there was a parsing error, otherwise the type depends on the schema as described above.\\n\"parsing_error\": Optional[BaseException]\\n\\nReturn type:\\nRunnable[PromptValue | str | Sequence[BaseMessage | list[str] | tuple[str, str] | str | dict[str, Any]], Dict | BaseModel]\\n\\nExample: Pydantic schema (include_raw=False):from pydantic import BaseModel\\n\\nclass AnswerWithJustification(BaseModel):\\n    \\'\\'\\'An answer to the user question along with justification for the answer.\\'\\'\\'\\n    answer: str\\n    justification: str\\n\\nllm = ChatModel(model=\"model-name\", temperature=0)\\nstructured_llm = llm.with_structured_output(AnswerWithJustification)\\n\\nstructured_llm.invoke(\"What weighs more a pound of bricks or a pound of feathers\")\\n\\n# -> AnswerWithJustification(\\n#     answer=\\'They weigh the same\\',\\n#     justification=\\'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.\\'\\n# )\\n\\nExample: Pydantic schema (include_raw=True):from pydantic import BaseModel\\n\\nclass AnswerWithJustification(BaseModel):\\n    \\'\\'\\'An answer to the user question along with justification for the answer.\\'\\'\\'\\n    answer: str\\n    justification: str\\n\\nllm = ChatModel(model=\"model-name\", temperature=0)\\nstructured_llm = llm.with_structured_output(AnswerWithJustification, include_raw=True)\\n\\nstructured_llm.invoke(\"What weighs more a pound of bricks or a pound of feathers\")\\n# -> {\\n#     \\'raw\\': AIMessage(content=\\'\\', additional_kwargs={\\'tool_calls\\': [{\\'id\\': \\'call_Ao02pnFYXD6GN1yzc0uXPsvF\\', \\'function\\': {\\'arguments\\': \\'{\"answer\":\"They weigh the same.\",\"justification\":\"Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.\"}\\', \\'name\\': \\'AnswerWithJustification\\'}, \\'type\\': \\'function\\'}]}),\\n#     \\'parsed\\': AnswerWithJustification(answer=\\'They weigh the same.\\', justification=\\'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.\\'),\\n#     \\'parsing_error\\': None\\n# }\\n\\nExample: Dict schema (include_raw=False):from pydantic import BaseModel\\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\\n\\nclass AnswerWithJustification(BaseModel):\\n    \\'\\'\\'An answer to the user question along with justification for the answer.\\'\\'\\'\\n    answer: str\\n    justification: str\\n\\ndict_schema = convert_to_openai_tool(AnswerWithJustification)\\nllm = ChatModel(model=\"model-name\", temperature=0)\\nstructured_llm = llm.with_structured_output(dict_schema)\\n\\nstructured_llm.invoke(\"What weighs more a pound of bricks or a pound of feathers\")\\n# -> {\\n#     \\'answer\\': \\'They weigh the same\\',\\n#     \\'justification\\': \\'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume and density of the two substances differ.\\'\\n# }\\n\\nChanged in version 0.2.26: Added support for TypedDict class.\\n\\nwith_types(\\n\\n*,\\ninput_type: type[Input] | None = None,\\noutput_type: type[Output] | None = None,\\n\\n) → Runnable[Input, Output]#\\nBind input and output types to a Runnable, returning a new Runnable.\\n\\nParameters:\\n\\ninput_type (type[Input] | None) – The input type to bind to the Runnable. Defaults to None.\\noutput_type (type[Output] | None) – The output type to bind to the Runnable. Defaults to None.\\n\\nReturns:\\nA new Runnable with the types bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\nproperty async_client: GenerativeServiceAsyncClient#\\n\\n On this page\\n  \\n\\nChatGoogleGenerativeAI\\nadditional_headers\\ncache\\ncached_content\\ncallback_manager\\ncallbacks\\nclient_options\\nconvert_system_message_to_human\\ncredentials\\ncustom_get_token_ids\\ndisable_streaming\\ngoogle_api_key\\nmax_output_tokens\\nmax_retries\\nmetadata\\nmodel\\nmodel_kwargs\\nn\\nrate_limiter\\nresponse_modalities\\nsafety_settings\\ntags\\ntemperature\\nthinking_budget\\ntimeout\\ntop_k\\ntop_p\\ntransport\\nverbose\\n__call__()\\nabatch()\\nabatch_as_completed()\\nainvoke()\\nastream()\\nastream_events()\\nbatch()\\nbatch_as_completed()\\nbind()\\nbind_tools()\\nconfigurable_alternatives()\\nconfigurable_fields()\\nget_num_tokens()\\nget_num_tokens_from_messages()\\nget_token_ids()\\ninvoke()\\nstream()\\nwith_alisteners()\\nwith_config()\\nwith_fallbacks()\\nwith_listeners()\\nwith_retry()\\nwith_structured_output()\\nwith_types()\\nasync_client\\n\\n\\n## Class Objects: google_vector_store\\nlangchain-google-genai: 2.1.4\\ngoogle_vector_store\\n\\ngoogle_vector_store#\\nGoogle Generative AI Vector Store.\\nThe GenAI Semantic Retriever API is a managed end-to-end service that allows\\ndevelopers to create a corpus of documents to perform semantic search on\\nrelated passages given a user query. For more information visit:\\nhttps://developers.generativeai.google/guide\\nClasses\\n\\ngoogle_vector_store.DoesNotExistsException(*,\\xa0...)\\n\\ngoogle_vector_store.GoogleVectorStore(*,\\xa0...)\\nGoogle GenerativeAI Vector Store.\\n\\ngoogle_vector_store.ServerSideEmbedding()\\nDo nothing embedding model where the embedding is done by the server.\\n\\n\\n## Class Objects: langchain_google_genai.google_vector_store.ServerSideEmbedding\\nlangchain-google-genai: 2.1.4\\ngoogle_vector_store\\nServerSideEmbedding\\n\\nServerSideEmbedding#\\n\\nclass langchain_google_genai.google_vector_store.ServerSideEmbedding[source]#\\nDo nothing embedding model where the embedding is done by the server.\\nMethods\\n\\naembed_documents(texts)\\nAsynchronous Embed search docs.\\n\\naembed_query(text)\\nAsynchronous Embed query text.\\n\\nembed_documents(texts)\\nEmbed search docs.\\n\\nembed_query(text)\\nEmbed query text.\\n\\nasync aembed_documents(\\n\\ntexts: list[str],\\n\\n) → list[list[float]]#\\nAsynchronous Embed search docs.\\n\\nParameters:\\ntexts (list[str]) – List of text to embed.\\n\\nReturns:\\nList of embeddings.\\n\\nReturn type:\\nlist[list[float]]\\n\\nasync aembed_query(text: str) → list[float]#\\nAsynchronous Embed query text.\\n\\nParameters:\\ntext (str) – Text to embed.\\n\\nReturns:\\nEmbedding.\\n\\nReturn type:\\nlist[float]\\n\\nembed_documents(\\n\\ntexts: List[str],\\n\\n) → List[List[float]][source]#\\nEmbed search docs.\\n\\nParameters:\\ntexts (List[str]) – List of text to embed.\\n\\nReturns:\\nList of embeddings.\\n\\nReturn type:\\nList[List[float]]\\n\\nembed_query(\\n\\ntext: str,\\n\\n) → List[float][source]#\\nEmbed query text.\\n\\nParameters:\\ntext (str) – Text to embed.\\n\\nReturns:\\nEmbedding.\\n\\nReturn type:\\nList[float]\\n\\n On this page\\n  \\n\\nServerSideEmbedding\\naembed_documents()\\naembed_query()\\nembed_documents()\\nembed_query()\\n\\n\\n## Class Objects: langchain_google_genai.google_vector_store.GoogleVectorStore\\nlangchain-google-genai: 2.1.4\\ngoogle_vector_store\\nGoogleVectorStore\\n\\nGoogleVectorStore#\\n\\nclass langchain_google_genai.google_vector_store.GoogleVectorStore(\\n\\n*,\\ncorpus_id: str,\\ndocument_id: str | None = None,\\n**kwargs: Any,\\n\\n)[source]#\\nGoogle GenerativeAI Vector Store.\\nCurrently, it computes the embedding vectors on the server side.\\nExample: Add texts to an existing corpus.\\n\\nstore = GoogleVectorStore(corpus_id=”123”)\\nstore.add_documents(documents, document_id=”456”)\\n\\nExample: Create a new corpus.\\n\\nstore = GoogleVectorStore.create_corpus(corpus_id=”123”, display_name=”My Google corpus”)\\n\\nExample: Query the corpus for relevant passages.\\n\\nstore.as_retriever()             .get_relevant_documents(“Who caught the gingerbread man?”)\\n\\nExample: Ask the corpus for grounded responses!\\n\\naqa = store.as_aqa()\\nresponse = aqa.invoke(“Who caught the gingerbread man?”)\\nprint(response.answer)\\nprint(response.attributed_passages)\\nprint(response.answerability_probability)\\n\\nYou can also operate at Google’s Document level.\\nExample: Add texts to an existing Google Vector Store Document.\\n\\ndoc_store = GoogleVectorStore(corpus_id=”123”, document_id=”456”)\\ndoc_store.add_documents(documents)\\n\\nExample: Create a new Google Vector Store Document.\\n\\ndoc_store = GoogleVectorStore.create_document(corpus_id=”123”, document_id=”456”, display_name=”My Google document”)\\n\\nExample: Query the Google document.\\n\\ndoc_store.as_retriever()             .get_relevant_documents(“Who caught the gingerbread man?”)\\n\\nFor more details, see the class’s methods.\\nReturns an existing Google Semantic Retriever corpus or document.\\nIf just the corpus ID is provided, the vector store operates over all\\ndocuments within that corpus.\\nIf the document ID is provided, the vector store operates over just that\\ndocument.\\n\\nRaises:\\nDoesNotExistsException if the IDs do not match to anything on Google – server. In this case, consider using create_corpus or\\n    create_document to create one.\\n\\nParameters:\\n\\ncorpus_id (str)\\ndocument_id (str | None)\\nkwargs (Any)\\n\\nAttributes\\n\\ncorpus_id\\nReturns the corpus ID managed by this vector store.\\n\\ndocument_id\\nReturns the document ID managed by this vector store.\\n\\nembeddings\\nAccess the query embedding object if available.\\n\\nname\\nReturns the name of the Google entity.\\n\\nMethods\\n\\n__init__(*,\\xa0corpus_id[,\\xa0document_id])\\nReturns an existing Google Semantic Retriever corpus or document.\\n\\naadd_documents(documents,\\xa0**kwargs)\\nAsync run more documents through the embeddings and add to the vectorstore.\\n\\naadd_texts(texts[,\\xa0metadatas,\\xa0ids])\\nAsync run more texts through the embeddings and add to the vectorstore.\\n\\nadd_documents(documents,\\xa0**kwargs)\\nAdd or update documents in the vectorstore.\\n\\nadd_texts(texts[,\\xa0metadatas,\\xa0document_id])\\nAdd texts to the vector store.\\n\\nadelete([ids])\\nAsync delete by vector ID or other criteria.\\n\\nafrom_documents(documents,\\xa0embedding,\\xa0**kwargs)\\nAsync return VectorStore initialized from documents and embeddings.\\n\\nafrom_texts(texts,\\xa0embedding[,\\xa0metadatas,\\xa0ids])\\nAsync return VectorStore initialized from texts and embeddings.\\n\\naget_by_ids(ids,\\xa0/)\\nAsync get documents by their IDs.\\n\\namax_marginal_relevance_search(query[,\\xa0k,\\xa0...])\\nAsync return docs selected using the maximal marginal relevance.\\n\\namax_marginal_relevance_search_by_vector(...)\\nAsync return docs selected using the maximal marginal relevance.\\n\\nas_aqa(**kwargs)\\nConstruct a Google Generative AI AQA engine.\\n\\nas_retriever(**kwargs)\\nReturn VectorStoreRetriever initialized from this VectorStore.\\n\\nasearch(query,\\xa0search_type,\\xa0**kwargs)\\nAsync return docs most similar to query using a specified search type.\\n\\nasimilarity_search(query[,\\xa0k])\\nAsync return docs most similar to query.\\n\\nasimilarity_search_by_vector(embedding[,\\xa0k])\\nAsync return docs most similar to embedding vector.\\n\\nasimilarity_search_with_relevance_scores(query)\\nAsync return docs and relevance scores in the range [0, 1].\\n\\nasimilarity_search_with_score(*args,\\xa0**kwargs)\\nAsync run similarity search with distance.\\n\\ncreate_corpus([corpus_id,\\xa0display_name])\\nCreate a Google Semantic Retriever corpus.\\n\\ncreate_document(corpus_id[,\\xa0document_id,\\xa0...])\\nCreate a Google Semantic Retriever document.\\n\\ndelete([ids])\\nDelete chunnks.\\n\\nfrom_documents(documents,\\xa0embedding,\\xa0**kwargs)\\nReturn VectorStore initialized from documents and embeddings.\\n\\nfrom_texts(texts[,\\xa0embedding,\\xa0metadatas,\\xa0...])\\nReturns a vector store of an existing document with the specified text.\\n\\nget_by_ids(ids,\\xa0/)\\nGet documents by their IDs.\\n\\nmax_marginal_relevance_search(query[,\\xa0k,\\xa0...])\\nReturn docs selected using the maximal marginal relevance.\\n\\nmax_marginal_relevance_search_by_vector(...)\\nReturn docs selected using the maximal marginal relevance.\\n\\nsearch(query,\\xa0search_type,\\xa0**kwargs)\\nReturn docs most similar to query using a specified search type.\\n\\nsimilarity_search(query[,\\xa0k,\\xa0filter])\\nSearch the vector store for relevant texts.\\n\\nsimilarity_search_by_vector(embedding[,\\xa0k])\\nReturn docs most similar to embedding vector.\\n\\nsimilarity_search_with_relevance_scores(query)\\nReturn docs and relevance scores in the range [0, 1].\\n\\nsimilarity_search_with_score(query[,\\xa0k,\\xa0filter])\\nRun similarity search with distance.\\n\\n__init__(\\n\\n*,\\ncorpus_id: str,\\ndocument_id: str | None = None,\\n**kwargs: Any,\\n\\n)[source]#\\nReturns an existing Google Semantic Retriever corpus or document.\\nIf just the corpus ID is provided, the vector store operates over all\\ndocuments within that corpus.\\nIf the document ID is provided, the vector store operates over just that\\ndocument.\\n\\nRaises:\\nDoesNotExistsException if the IDs do not match to anything on Google – server. In this case, consider using create_corpus or\\n    create_document to create one.\\n\\nParameters:\\n\\ncorpus_id (str)\\ndocument_id (str | None)\\nkwargs (Any)\\n\\nasync aadd_documents(\\n\\ndocuments: list[Document],\\n**kwargs: Any,\\n\\n) → list[str]#\\nAsync run more documents through the embeddings and add to the vectorstore.\\n\\nParameters:\\n\\ndocuments (list[Document]) – Documents to add to the vectorstore.\\nkwargs (Any) – Additional keyword arguments.\\n\\nReturns:\\nList of IDs of the added texts.\\n\\nRaises:\\nValueError – If the number of IDs does not match the number of documents.\\n\\nReturn type:\\nlist[str]\\n\\nasync aadd_texts(\\n\\ntexts: Iterable[str],\\nmetadatas: list[dict] | None = None,\\n*,\\nids: list[str] | None = None,\\n**kwargs: Any,\\n\\n) → list[str]#\\nAsync run more texts through the embeddings and add to the vectorstore.\\n\\nParameters:\\n\\ntexts (Iterable[str]) – Iterable of strings to add to the vectorstore.\\nmetadatas (Optional[list[dict]]) – Optional list of metadatas associated with the texts.\\nDefault is None.\\nids (Optional[list[str]]) – Optional list\\n**kwargs (Any) – vectorstore specific parameters.\\n\\nReturns:\\nList of ids from adding the texts into the vectorstore.\\n\\nRaises:\\n\\nValueError – If the number of metadatas does not match the number of texts.\\nValueError – If the number of ids does not match the number of texts.\\n\\nReturn type:\\nlist[str]\\n\\nadd_documents(\\n\\ndocuments: list[Document],\\n**kwargs: Any,\\n\\n) → list[str]#\\nAdd or update documents in the vectorstore.\\n\\nParameters:\\n\\ndocuments (list[Document]) – Documents to add to the vectorstore.\\nkwargs (Any) – Additional keyword arguments.\\nif kwargs contains ids and documents contain ids,\\nthe ids in the kwargs will receive precedence.\\n\\nReturns:\\nList of IDs of the added texts.\\n\\nRaises:\\nValueError – If the number of ids does not match the number of documents.\\n\\nReturn type:\\nlist[str]\\n\\nadd_texts(\\n\\ntexts: Iterable[str],\\nmetadatas: List[Dict[str, Any]] | None = None,\\n*,\\ndocument_id: str | None = None,\\n**kwargs: Any,\\n\\n) → List[str][source]#\\nAdd texts to the vector store.\\nIf the vector store points to a corpus (instead of a document), you must\\nalso provide a document_id.\\n\\nReturns:\\nChunk’s names created on Google servers.\\n\\nParameters:\\n\\ntexts (Iterable[str])\\nmetadatas (List[Dict[str, Any]] | None)\\ndocument_id (str | None)\\nkwargs (Any)\\n\\nReturn type:\\nList[str]\\n\\nasync adelete(\\n\\nids: List[str] | None = None,\\n**kwargs: Any,\\n\\n) → bool | None[source]#\\nAsync delete by vector ID or other criteria.\\n\\nParameters:\\n\\nids (List[str] | None) – List of ids to delete. If None, delete all. Default is None.\\n**kwargs (Any) – Other keyword arguments that subclasses might use.\\n\\nReturns:\\nTrue if deletion is successful,\\nFalse otherwise, None if not implemented.\\n\\nReturn type:\\nOptional[bool]\\n\\nasync classmethod afrom_documents(\\n\\ndocuments: list[Document],\\nembedding: Embeddings,\\n**kwargs: Any,\\n\\n) → Self#\\nAsync return VectorStore initialized from documents and embeddings.\\n\\nParameters:\\n\\ndocuments (list[Document]) – List of Documents to add to the vectorstore.\\nembedding (Embeddings) – Embedding function to use.\\nkwargs (Any) – Additional keyword arguments.\\n\\nReturns:\\nVectorStore initialized from documents and embeddings.\\n\\nReturn type:\\nVectorStore\\n\\nasync classmethod afrom_texts(\\n\\ntexts: list[str],\\nembedding: Embeddings,\\nmetadatas: list[dict] | None = None,\\n*,\\nids: list[str] | None = None,\\n**kwargs: Any,\\n\\n) → Self#\\nAsync return VectorStore initialized from texts and embeddings.\\n\\nParameters:\\n\\ntexts (list[str]) – Texts to add to the vectorstore.\\nembedding (Embeddings) – Embedding function to use.\\nmetadatas (list[dict] | None) – Optional list of metadatas associated with the texts.\\nDefault is None.\\nids (list[str] | None) – Optional list of IDs associated with the texts.\\nkwargs (Any) – Additional keyword arguments.\\n\\nReturns:\\nVectorStore initialized from texts and embeddings.\\n\\nReturn type:\\nVectorStore\\n\\nasync aget_by_ids(\\n\\nids: Sequence[str],\\n/,\\n\\n) → list[Document]#\\nAsync get documents by their IDs.\\nThe returned documents are expected to have the ID field set to the ID of the\\ndocument in the vector store.\\nFewer documents may be returned than requested if some IDs are not found or\\nif there are duplicated IDs.\\nUsers should not assume that the order of the returned documents matches\\nthe order of the input IDs. Instead, users should rely on the ID field of the\\nreturned documents.\\nThis method should NOT raise exceptions if no documents are found for\\nsome IDs.\\n\\nParameters:\\nids (Sequence[str]) – List of ids to retrieve.\\n\\nReturns:\\nList of Documents.\\n\\nReturn type:\\nlist[Document]\\n\\nAdded in version 0.2.11.\\n\\nasync amax_marginal_relevance_search(\\n\\nquery: str,\\nk: int = 4,\\nfetch_k: int = 20,\\nlambda_mult: float = 0.5,\\n**kwargs: Any,\\n\\n) → list[Document]#\\nAsync return docs selected using the maximal marginal relevance.\\nMaximal marginal relevance optimizes for similarity to query AND diversity\\namong selected documents.\\n\\nParameters:\\n\\nquery (str) – Text to look up documents similar to.\\nk (int) – Number of Documents to return. Defaults to 4.\\nfetch_k (int) – Number of Documents to fetch to pass to MMR algorithm.\\nDefault is 20.\\nlambda_mult (float) – Number between 0 and 1 that determines the degree\\nof diversity among the results with 0 corresponding\\nto maximum diversity and 1 to minimum diversity.\\nDefaults to 0.5.\\n**kwargs (Any) – Arguments to pass to the search method.\\n\\nReturns:\\nList of Documents selected by maximal marginal relevance.\\n\\nReturn type:\\nlist[Document]\\n\\nasync amax_marginal_relevance_search_by_vector(\\n\\nembedding: list[float],\\nk: int = 4,\\nfetch_k: int = 20,\\nlambda_mult: float = 0.5,\\n**kwargs: Any,\\n\\n) → list[Document]#\\nAsync return docs selected using the maximal marginal relevance.\\nMaximal marginal relevance optimizes for similarity to query AND diversity\\namong selected documents.\\n\\nParameters:\\n\\nembedding (list[float]) – Embedding to look up documents similar to.\\nk (int) – Number of Documents to return. Defaults to 4.\\nfetch_k (int) – Number of Documents to fetch to pass to MMR algorithm.\\nDefault is 20.\\nlambda_mult (float) – Number between 0 and 1 that determines the degree\\nof diversity among the results with 0 corresponding\\nto maximum diversity and 1 to minimum diversity.\\nDefaults to 0.5.\\n**kwargs (Any) – Arguments to pass to the search method.\\n\\nReturns:\\nList of Documents selected by maximal marginal relevance.\\n\\nReturn type:\\nlist[Document]\\n\\nas_aqa(\\n\\n**kwargs: Any,\\n\\n) → Runnable[str, AqaOutput][source]#\\nConstruct a Google Generative AI AQA engine.\\nAll arguments are optional.\\n\\nParameters:\\n\\nanswer_style – See\\ngoogle.ai.generativelanguage.GenerateAnswerRequest.AnswerStyle.\\nsafety_settings – See google.ai.generativelanguage.SafetySetting.\\ntemperature – 0.0 to 1.0.\\nkwargs (Any)\\n\\nReturn type:\\nRunnable[str, AqaOutput]\\n\\nas_retriever(\\n\\n**kwargs: Any,\\n\\n) → VectorStoreRetriever#\\nReturn VectorStoreRetriever initialized from this VectorStore.\\n\\nParameters:\\n**kwargs (Any) – Keyword arguments to pass to the search function.\\nCan include:\\nsearch_type (Optional[str]): Defines the type of search that\\n\\nthe Retriever should perform.\\nCan be “similarity” (default), “mmr”, or\\n“similarity_score_threshold”.\\n\\nsearch_kwargs (Optional[Dict]): Keyword arguments to pass to the\\nsearch function. Can include things like:k: Amount of documents to return (Default: 4)\\nscore_threshold: Minimum relevance threshold\\n\\nfor similarity_score_threshold\\n\\nfetch_k: Amount of documents to pass to MMR algorithm(Default: 20)\\n\\nlambda_mult: Diversity of results returned by MMR;1 for minimum diversity and 0 for maximum. (Default: 0.5)\\n\\nfilter: Filter by document metadata\\n\\nReturns:\\nRetriever class for VectorStore.\\n\\nReturn type:\\nVectorStoreRetriever\\n\\nExamples:\\n# Retrieve more documents with higher diversity\\n# Useful if your dataset has many similar documents\\ndocsearch.as_retriever(\\n    search_type=\"mmr\",\\n    search_kwargs={\\'k\\': 6, \\'lambda_mult\\': 0.25}\\n)\\n\\n# Fetch more documents for the MMR algorithm to consider\\n# But only return the top 5\\ndocsearch.as_retriever(\\n    search_type=\"mmr\",\\n    search_kwargs={\\'k\\': 5, \\'fetch_k\\': 50}\\n)\\n\\n# Only retrieve documents that have a relevance score\\n# Above a certain threshold\\ndocsearch.as_retriever(\\n    search_type=\"similarity_score_threshold\",\\n    search_kwargs={\\'score_threshold\\': 0.8}\\n)\\n\\n# Only get the single most similar document from the dataset\\ndocsearch.as_retriever(search_kwargs={\\'k\\': 1})\\n\\n# Use a filter to only retrieve documents from a specific paper\\ndocsearch.as_retriever(\\n    search_kwargs={\\'filter\\': {\\'paper_title\\':\\'GPT-4 Technical Report\\'}}\\n)\\n\\nasync asearch(\\n\\nquery: str,\\nsearch_type: str,\\n**kwargs: Any,\\n\\n) → list[Document]#\\nAsync return docs most similar to query using a specified search type.\\n\\nParameters:\\n\\nquery (str) – Input text.\\nsearch_type (str) – Type of search to perform. Can be “similarity”,\\n“mmr”, or “similarity_score_threshold”.\\n**kwargs (Any) – Arguments to pass to the search method.\\n\\nReturns:\\nList of Documents most similar to the query.\\n\\nRaises:\\nValueError – If search_type is not one of “similarity”,\\n    “mmr”, or “similarity_score_threshold”.\\n\\nReturn type:\\nlist[Document]\\n\\nasync asimilarity_search(\\n\\nquery: str,\\nk: int = 4,\\n**kwargs: Any,\\n\\n) → list[Document]#\\nAsync return docs most similar to query.\\n\\nParameters:\\n\\nquery (str) – Input text.\\nk (int) – Number of Documents to return. Defaults to 4.\\n**kwargs (Any) – Arguments to pass to the search method.\\n\\nReturns:\\nList of Documents most similar to the query.\\n\\nReturn type:\\nlist[Document]\\n\\nasync asimilarity_search_by_vector(\\n\\nembedding: list[float],\\nk: int = 4,\\n**kwargs: Any,\\n\\n) → list[Document]#\\nAsync return docs most similar to embedding vector.\\n\\nParameters:\\n\\nembedding (list[float]) – Embedding to look up documents similar to.\\nk (int) – Number of Documents to return. Defaults to 4.\\n**kwargs (Any) – Arguments to pass to the search method.\\n\\nReturns:\\nList of Documents most similar to the query vector.\\n\\nReturn type:\\nlist[Document]\\n\\nasync asimilarity_search_with_relevance_scores(\\n\\nquery: str,\\nk: int = 4,\\n**kwargs: Any,\\n\\n) → list[tuple[Document, float]]#\\nAsync return docs and relevance scores in the range [0, 1].\\n0 is dissimilar, 1 is most similar.\\n\\nParameters:\\n\\nquery (str) – Input text.\\nk (int) – Number of Documents to return. Defaults to 4.\\n**kwargs (Any) – kwargs to be passed to similarity search. Should include:\\nscore_threshold: Optional, a floating point value between 0 to 1 to\\n\\nfilter the resulting set of retrieved docs\\n\\nReturns:\\nList of Tuples of (doc, similarity_score)\\n\\nReturn type:\\nlist[tuple[Document, float]]\\n\\nasync asimilarity_search_with_score(\\n\\n*args: Any,\\n**kwargs: Any,\\n\\n) → list[tuple[Document, float]]#\\nAsync run similarity search with distance.\\n\\nParameters:\\n\\n*args (Any) – Arguments to pass to the search method.\\n**kwargs (Any) – Arguments to pass to the search method.\\n\\nReturns:\\nList of Tuples of (doc, similarity_score).\\n\\nReturn type:\\nlist[tuple[Document, float]]\\n\\nclassmethod create_corpus(\\n\\ncorpus_id: str | None = None,\\ndisplay_name: str | None = None,\\n\\n) → GoogleVectorStore[source]#\\nCreate a Google Semantic Retriever corpus.\\n\\nParameters:\\n\\ncorpus_id (str | None) – The ID to use to create the new corpus. If not provided,\\nGoogle server will provide one.\\ndisplay_name (str | None) – The title of the new corpus. If not provided, Google\\nserver will provide one.\\n\\nReturns:\\nAn instance of vector store that points to the newly created corpus.\\n\\nReturn type:\\nGoogleVectorStore\\n\\nclassmethod create_document(\\n\\ncorpus_id: str,\\ndocument_id: str | None = None,\\ndisplay_name: str | None = None,\\nmetadata: Dict[str, Any] | None = None,\\n\\n) → GoogleVectorStore[source]#\\nCreate a Google Semantic Retriever document.\\n\\nParameters:\\n\\ncorpus_id (str) – ID of an existing corpus.\\ndocument_id (str | None) – The ID to use to create the new Google Semantic\\nRetriever document. If not provided, Google server will provide\\none.\\ndisplay_name (str | None) – The title of the new document. If not provided, Google\\nserver will provide one.\\nmetadata (Dict[str, Any] | None)\\n\\nReturns:\\nAn instance of vector store that points to the newly created\\ndocument.\\n\\nReturn type:\\nGoogleVectorStore\\n\\ndelete(\\n\\nids: List[str] | None = None,\\n**kwargs: Any,\\n\\n) → bool | None[source]#\\nDelete chunnks.\\nNote that the “ids” are not corpus ID or document ID. Rather, these\\nare the entity names returned by add_texts.\\n\\nReturns:\\nTrue if successful. Otherwise, you should get an exception anyway.\\n\\nParameters:\\n\\nids (List[str] | None)\\nkwargs (Any)\\n\\nReturn type:\\nbool | None\\n\\nclassmethod from_documents(\\n\\ndocuments: list[Document],\\nembedding: Embeddings,\\n**kwargs: Any,\\n\\n) → Self#\\nReturn VectorStore initialized from documents and embeddings.\\n\\nParameters:\\n\\ndocuments (list[Document]) – List of Documents to add to the vectorstore.\\nembedding (Embeddings) – Embedding function to use.\\nkwargs (Any) – Additional keyword arguments.\\n\\nReturns:\\nVectorStore initialized from documents and embeddings.\\n\\nReturn type:\\nVectorStore\\n\\nclassmethod from_texts(\\n\\ntexts: List[str],\\nembedding: Embeddings | None = None,\\nmetadatas: List[dict[str, Any]] | None = None,\\n*,\\ncorpus_id: str | None = None,\\ndocument_id: str | None = None,\\n**kwargs: Any,\\n\\n) → GoogleVectorStore[source]#\\nReturns a vector store of an existing document with the specified text.\\n\\nParameters:\\n\\ncorpus_id (str | None) – REQUIRED. Must be an existing corpus.\\ndocument_id (str | None) – REQUIRED. Must be an existing document.\\ntexts (List[str]) – Texts to be loaded into the vector store.\\nembedding (Embeddings | None)\\nmetadatas (List[dict[str, Any]] | None)\\nkwargs (Any)\\n\\nReturns:\\nA vector store pointing to the specified Google Semantic Retriever\\nDocument.\\n\\nRaises:\\nDoesNotExistsException if the IDs do not match to anything at – Google server.\\n\\nReturn type:\\nGoogleVectorStore\\n\\nget_by_ids(\\n\\nids: Sequence[str],\\n/,\\n\\n) → list[Document]#\\nGet documents by their IDs.\\nThe returned documents are expected to have the ID field set to the ID of the\\ndocument in the vector store.\\nFewer documents may be returned than requested if some IDs are not found or\\nif there are duplicated IDs.\\nUsers should not assume that the order of the returned documents matches\\nthe order of the input IDs. Instead, users should rely on the ID field of the\\nreturned documents.\\nThis method should NOT raise exceptions if no documents are found for\\nsome IDs.\\n\\nParameters:\\nids (Sequence[str]) – List of ids to retrieve.\\n\\nReturns:\\nList of Documents.\\n\\nReturn type:\\nlist[Document]\\n\\nAdded in version 0.2.11.\\n\\nmax_marginal_relevance_search(\\n\\nquery: str,\\nk: int = 4,\\nfetch_k: int = 20,\\nlambda_mult: float = 0.5,\\n**kwargs: Any,\\n\\n) → list[Document]#\\nReturn docs selected using the maximal marginal relevance.\\nMaximal marginal relevance optimizes for similarity to query AND diversity\\namong selected documents.\\n\\nParameters:\\n\\nquery (str) – Text to look up documents similar to.\\nk (int) – Number of Documents to return. Defaults to 4.\\nfetch_k (int) – Number of Documents to fetch to pass to MMR algorithm.\\nDefault is 20.\\nlambda_mult (float) – Number between 0 and 1 that determines the degree\\nof diversity among the results with 0 corresponding\\nto maximum diversity and 1 to minimum diversity.\\nDefaults to 0.5.\\n**kwargs (Any) – Arguments to pass to the search method.\\n\\nReturns:\\nList of Documents selected by maximal marginal relevance.\\n\\nReturn type:\\nlist[Document]\\n\\nmax_marginal_relevance_search_by_vector(\\n\\nembedding: list[float],\\nk: int = 4,\\nfetch_k: int = 20,\\nlambda_mult: float = 0.5,\\n**kwargs: Any,\\n\\n) → list[Document]#\\nReturn docs selected using the maximal marginal relevance.\\nMaximal marginal relevance optimizes for similarity to query AND diversity\\namong selected documents.\\n\\nParameters:\\n\\nembedding (list[float]) – Embedding to look up documents similar to.\\nk (int) – Number of Documents to return. Defaults to 4.\\nfetch_k (int) – Number of Documents to fetch to pass to MMR algorithm.\\nDefault is 20.\\nlambda_mult (float) – Number between 0 and 1 that determines the degree\\nof diversity among the results with 0 corresponding\\nto maximum diversity and 1 to minimum diversity.\\nDefaults to 0.5.\\n**kwargs (Any) – Arguments to pass to the search method.\\n\\nReturns:\\nList of Documents selected by maximal marginal relevance.\\n\\nReturn type:\\nlist[Document]\\n\\nsearch(\\n\\nquery: str,\\nsearch_type: str,\\n**kwargs: Any,\\n\\n) → list[Document]#\\nReturn docs most similar to query using a specified search type.\\n\\nParameters:\\n\\nquery (str) – Input text\\nsearch_type (str) – Type of search to perform. Can be “similarity”,\\n“mmr”, or “similarity_score_threshold”.\\n**kwargs (Any) – Arguments to pass to the search method.\\n\\nReturns:\\nList of Documents most similar to the query.\\n\\nRaises:\\nValueError – If search_type is not one of “similarity”,\\n    “mmr”, or “similarity_score_threshold”.\\n\\nReturn type:\\nlist[Document]\\n\\nsimilarity_search(\\n\\nquery: str,\\nk: int = 4,\\nfilter: Dict[str, Any] | None = None,\\n**kwargs: Any,\\n\\n) → List[Document][source]#\\nSearch the vector store for relevant texts.\\n\\nParameters:\\n\\nquery (str)\\nk (int)\\nfilter (Dict[str, Any] | None)\\nkwargs (Any)\\n\\nReturn type:\\nList[Document]\\n\\nsimilarity_search_by_vector(\\n\\nembedding: list[float],\\nk: int = 4,\\n**kwargs: Any,\\n\\n) → list[Document]#\\nReturn docs most similar to embedding vector.\\n\\nParameters:\\n\\nembedding (list[float]) – Embedding to look up documents similar to.\\nk (int) – Number of Documents to return. Defaults to 4.\\n**kwargs (Any) – Arguments to pass to the search method.\\n\\nReturns:\\nList of Documents most similar to the query vector.\\n\\nReturn type:\\nlist[Document]\\n\\nsimilarity_search_with_relevance_scores(\\n\\nquery: str,\\nk: int = 4,\\n**kwargs: Any,\\n\\n) → list[tuple[Document, float]]#\\nReturn docs and relevance scores in the range [0, 1].\\n0 is dissimilar, 1 is most similar.\\n\\nParameters:\\n\\nquery (str) – Input text.\\nk (int) – Number of Documents to return. Defaults to 4.\\n**kwargs (Any) – kwargs to be passed to similarity search. Should include:\\nscore_threshold: Optional, a floating point value between 0 to 1 to\\n\\nfilter the resulting set of retrieved docs.\\n\\nReturns:\\nList of Tuples of (doc, similarity_score).\\n\\nReturn type:\\nlist[tuple[Document, float]]\\n\\nsimilarity_search_with_score(\\n\\nquery: str,\\nk: int = 4,\\nfilter: Dict[str, Any] | None = None,\\n**kwargs: Any,\\n\\n) → List[Tuple[Document, float]][source]#\\nRun similarity search with distance.\\n\\nParameters:\\n\\nquery (str)\\nk (int)\\nfilter (Dict[str, Any] | None)\\nkwargs (Any)\\n\\nReturn type:\\nList[Tuple[Document, float]]\\n\\n On this page\\n  \\n\\nGoogleVectorStore\\n__init__()\\naadd_documents()\\naadd_texts()\\nadd_documents()\\nadd_texts()\\nadelete()\\nafrom_documents()\\nafrom_texts()\\naget_by_ids()\\namax_marginal_relevance_search()\\namax_marginal_relevance_search_by_vector()\\nas_aqa()\\nas_retriever()\\nasearch()\\nasimilarity_search()\\nasimilarity_search_by_vector()\\nasimilarity_search_with_relevance_scores()\\nasimilarity_search_with_score()\\ncreate_corpus()\\ncreate_document()\\ndelete()\\nfrom_documents()\\nfrom_texts()\\nget_by_ids()\\nmax_marginal_relevance_search()\\nmax_marginal_relevance_search_by_vector()\\nsearch()\\nsimilarity_search()\\nsimilarity_search_by_vector()\\nsimilarity_search_with_relevance_scores()\\nsimilarity_search_with_score()\\n\\n\\n## Class Objects: langchain_google_genai.google_vector_store.DoesNotExistsException\\nlangchain-google-genai: 2.1.4\\ngoogle_vector_store\\nDoesNotExistsException\\n\\nDoesNotExistsException#\\n\\nclass langchain_google_genai.google_vector_store.DoesNotExistsException(\\n\\n*,\\ncorpus_id: str,\\ndocument_id: str | None = None,\\n\\n)[source]#\\n\\nParameters:\\n\\ncorpus_id (str)\\ndocument_id (str | None)\\n\\n On this page\\n  \\n\\nDoesNotExistsException\\n\\n\\n## Class Objects: embeddings\\nlangchain-google-genai: 2.1.4\\nembeddings\\n\\nembeddings#\\nClasses\\n\\nembeddings.GoogleGenerativeAIEmbeddings\\nGoogle Generative AI Embeddings.\\n\\n\\n## Class Objects: langchain_google_genai.genai_aqa.AqaOutput\\nlangchain-google-genai: 2.1.4\\ngenai_aqa\\nAqaOutput\\n\\nAqaOutput#\\n\\nclass langchain_google_genai.genai_aqa.AqaOutput[source]#\\nBases: BaseModel\\nOutput from GenAIAqa.invoke.\\n\\nanswer#\\nThe answer to the user’s inquiry.\\n\\nattributed_passages#\\nA list of passages that the LLM used to construct\\nthe answer.\\n\\nanswerable_probability#\\nThe probability of the question being answered\\nfrom the provided passages.\\n\\nCreate a new model by parsing and validating input data from keyword arguments.\\nRaises [ValidationError][pydantic_core.ValidationError] if the input data cannot be\\nvalidated to form a valid model.\\nself is explicitly positional-only to allow self as a field name.\\n\\nparam answer: str [Required]#\\n\\nparam answerable_probability: float [Required]#\\n\\nparam attributed_passages: List[str] [Required]#\\n\\n On this page\\n  \\n\\nAqaOutput\\nanswer\\nattributed_passages\\nanswerable_probability\\nanswer\\nanswerable_probability\\nattributed_passages\\n\\n\\n## Class Objects: langchain_google_genai.genai_aqa.AqaInput\\nlangchain-google-genai: 2.1.4\\ngenai_aqa\\nAqaInput\\n\\nAqaInput#\\n\\nclass langchain_google_genai.genai_aqa.AqaInput[source]#\\nBases: BaseModel\\nInput to GenAIAqa.invoke.\\n\\nprompt#\\nThe user’s inquiry.\\n\\nsource_passages#\\nA list of passage that the LLM should use only to\\nanswer the user’s inquiry.\\n\\nCreate a new model by parsing and validating input data from keyword arguments.\\nRaises [ValidationError][pydantic_core.ValidationError] if the input data cannot be\\nvalidated to form a valid model.\\nself is explicitly positional-only to allow self as a field name.\\n\\nparam prompt: str [Required]#\\n\\nparam source_passages: List[str] [Required]#\\n\\n On this page\\n  \\n\\nAqaInput\\nprompt\\nsource_passages\\nprompt\\nsource_passages'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset['google_genai']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "-7lqAj6SEcOk",
      "metadata": {
        "id": "-7lqAj6SEcOk"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n",
        "max_tokens = 4096\n",
        "\n",
        "def tokenize_len(text):\n",
        "    return len(tokenizer.tokenize(text))\n",
        "\n",
        "def split_by_class_sections(text):\n",
        "    pattern = r'\\n{2,}## Class Objects: (.+?)\\n'\n",
        "    parts = re.split(pattern, text)\n",
        "    # parts = ['', first_module, first_text, second_module, second_text, ...]\n",
        "    it = iter(parts[1:])  # Skip first empty string\n",
        "    return list(zip(it, it))  # [(module1, text1), (module2, text2), ...]\n",
        "\n",
        "final_chunks = []\n",
        "\n",
        "for i, (topic, full_doc) in enumerate(dataset.items(), start=1):\n",
        "    sections = split_by_class_sections(full_doc)\n",
        "    for j, (module, content) in enumerate(sections, start=1):\n",
        "        header = f\"### Instruction: Learn about the {topic} LangChain API.\\n\\n### Part {i} - Module:{module}(chunk{j})\\n\\n\"\n",
        "        full_text = header + content.strip()\n",
        "        if tokenize_len(full_text) <= max_tokens:\n",
        "            final_chunks.append({\"text\": full_text})\n",
        "        else:\n",
        "            words = full_text.split()\n",
        "            chunk = \"\"\n",
        "            for word in words:\n",
        "                chunk += word + \" \"\n",
        "                if tokenize_len(chunk) >= 3000:\n",
        "                    final_chunks.append({\"text\": chunk.strip()})\n",
        "                    chunk = \"\"\n",
        "            if chunk:\n",
        "                final_chunks.append({\"text\": chunk.strip()})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "F8iNQZpJS9iV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8iNQZpJS9iV",
        "outputId": "e39042bd-2411-4d4e-96c2-e0fec348f78a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1143\n",
            "{'text': '### Instruction: Learn about the nvidia_ai_endpoints LangChain API.\\n\\n### Part 8 - Module:langchain_nvidia_ai_endpoints.callbacks.get_usage_callback(chunk11)\\n\\nlangchain-nvidia-ai-endpoints: 0.3.10\\ncallbacks\\nget_usage_callback\\n\\nget_usage_callback#\\n\\nlangchain_nvidia_ai_endpoints.callbacks.get_usage_callback(\\n\\nprice_map: dict = {},\\ncallback: UsageCallbackHandler | None = None,\\n\\n) → Generator[UsageCallbackHandler, None, None][source]#\\nGet the OpenAI callback handler in a context manager.\\nwhich conveniently exposes token and cost information.\\n\\nReturns:\\nThe OpenAI callback handler.\\n\\nReturn type:\\nOpenAICallbackHandler\\n\\nParameters:\\n\\nprice_map (dict)\\ncallback (UsageCallbackHandler | None)\\n\\nExample\\n>>> with get_openai_callback() as cb:\\n...     # Use the OpenAI callback handler\\n\\n On this page\\n  \\n\\nget_usage_callback()'}\n"
          ]
        }
      ],
      "source": [
        "print(len(final_chunks))\n",
        "print(final_chunks[80])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "SCPTD78xYTV4",
      "metadata": {
        "id": "SCPTD78xYTV4"
      },
      "outputs": [],
      "source": [
        "data= open('LangDatasetChunked.pickle', 'wb')\n",
        "pickle.dump(final_chunks, data)\n",
        "data.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "lEs1HFoEB0c9",
      "metadata": {
        "id": "lEs1HFoEB0c9"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "documentation = Dataset.from_list(final_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "33639be0",
      "metadata": {
        "id": "33639be0"
      },
      "outputs": [],
      "source": [
        "model_id = \"google/gemma-2-2b\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "ad2df6a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "990a82ec00084ab9a502f87e8c702226",
            "e4d641b82b0a4c5fa5c96343176c867a",
            "67e074de5665427bb27fb1948464cb83",
            "441e989ac7d3495aae2adf3db6817fab",
            "f27b0e3777c74fe7aa460bb65cedadf3",
            "b30f5a421ee7490192cec220f79e89b5",
            "5787fa47a89a465e981aa7d7338affbc",
            "ba5a4756e1f44eb3b7995c623e5c48d6",
            "0974972f26f84defb0f34c2c80a19b1d",
            "df030c3d971149c2877d964bc944d13d",
            "4089e52294a347209477bab68e880e12"
          ]
        },
        "id": "ad2df6a7",
        "outputId": "79231c40-e592-4a74-e27e-8831d87e57a0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "990a82ec00084ab9a502f87e8c702226",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=False,\n",
        "    attn_implementation='eager',\n",
        "    use_cache=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "e6b8938e",
      "metadata": {
        "id": "e6b8938e"
      },
      "outputs": [],
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    target_modules=['q_proj', \"o_proj\", \"k_proj\", \"v_proj\", 'gate_proj', 'up_proj', \"down_proj\"],\n",
        "    task_type='CAUSAL_LM',\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "GV49HxwpCZ7K",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "5a92604d4d5440f3925ad7265acdb86a",
            "70dc0892c2b04727b1183839ad84a7db",
            "ba8817e8e85645b39192efcdbd217ded",
            "b6cd2b731f0f4d3d98a8a3824cf6bd4c",
            "6c3b443caaea406eaf60300a42223b54",
            "1e4b7f4cef584b37a6405edb84a658c0",
            "2b4179498fe641cdb545a78de3ab0b8d",
            "01f6c677eda246ff992f14ecebe87a17",
            "261f45473c2b491ea07f0b86944df9bf",
            "466c5710fec04b20be86708dc54caea3",
            "b0511d19bdac489aa4eb7787cbc6435a"
          ]
        },
        "id": "GV49HxwpCZ7K",
        "outputId": "34e1f76f-9074-4233-d4d1-d630302a181d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5a92604d4d5440f3925ad7265acdb86a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1143 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "def tokenize(example):\n",
        "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=4096)\n",
        "\n",
        "tokenized_dataset = documentation.map(tokenize, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "bj9BgmY6KW6C",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bj9BgmY6KW6C",
        "outputId": "e61692bb-3d13-40ae-aff2-55719fdba9aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainable parameters: 590,065,920\n",
            "Total parameters: 1,602,203,904\n",
            "Trainable ratio: 36.8284%\n"
          ]
        }
      ],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    trainable = 0\n",
        "    total = 0\n",
        "    for param in model.parameters():\n",
        "        num_params = param.numel()\n",
        "        total += num_params\n",
        "        if param.requires_grad:\n",
        "            trainable += num_params\n",
        "    print(f\"Trainable parameters: {trainable:,}\")\n",
        "    print(f\"Total parameters: {total:,}\")\n",
        "    print(f\"Trainable ratio: {100 * trainable / total:.4f}%\")\n",
        "\n",
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "08f14586",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "d086003bb0124215b95beaa5c26ed45f",
            "0713788da3484b269e08756a369a4933",
            "dd5f9a2c028b4f1f9626687b713a02a7",
            "2b12caba0fd243b6a905b2c4474a5814",
            "b5791dd7c05847f6a5f15e4b51cf3a0a",
            "8c2c714e7d9b41329bd4a5181e4aa9bc",
            "68973cb358854657a484417993f32ea7",
            "45c6ff81c276474dabe2d5af18320e12",
            "937d264403ed4852ab8560bbda45c210",
            "59a556ace00448b78459353f4a7468b3",
            "f805f7ba48cc4f3e9bf07342aaa3a166"
          ]
        },
        "id": "08f14586",
        "outputId": "4ce5798e-0e5a-4615-c75f-8b0fd954d50c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d086003bb0124215b95beaa5c26ed45f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Truncating train dataset:   0%|          | 0/1143 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        }
      ],
      "source": [
        "tuner = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=8,\n",
        "        num_train_epochs=3,\n",
        "        learning_rate=5e-5,\n",
        "        warmup_steps=50,\n",
        "        logging_steps=5,\n",
        "        fp16=True,\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        gradient_checkpointing=True,\n",
        "        output_dir=\"outputs2\"\n",
        "    ),\n",
        "    peft_config=lora_config\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "0fTv4Rs6FXVb",
      "metadata": {
        "id": "0fTv4Rs6FXVb"
      },
      "outputs": [],
      "source": [
        "os.environ['WANDB_DISABLED'] = \"false\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "7iEFmqqoGahW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "7iEFmqqoGahW",
        "outputId": "603c0084-7bd7-486a-ac5b-230d8a8a77e7"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mprincedastan\u001b[0m (\u001b[33mprincedastan-mbm-university-jodhpur\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250520_104939-ttj2owpt</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/princedastan-mbm-university-jodhpur/lang-tuner/runs/ttj2owpt' target=\"_blank\">light-surf-2</a></strong> to <a href='https://wandb.ai/princedastan-mbm-university-jodhpur/lang-tuner' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/princedastan-mbm-university-jodhpur/lang-tuner' target=\"_blank\">https://wandb.ai/princedastan-mbm-university-jodhpur/lang-tuner</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/princedastan-mbm-university-jodhpur/lang-tuner/runs/ttj2owpt' target=\"_blank\">https://wandb.ai/princedastan-mbm-university-jodhpur/lang-tuner/runs/ttj2owpt</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/princedastan-mbm-university-jodhpur/lang-tuner/runs/ttj2owpt?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x782b46ee0fd0>"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "\n",
        "wandb.init(project=\"lang-tuner\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "4bd99dc7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4bd99dc7",
        "outputId": "a4960a0e-cb58-4603-8a37-5e371daa92fb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='213' max='213' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [213/213 1:42:07, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>27.291200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>23.562300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>13.402400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>4.611600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>3.986300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>3.399100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>2.847100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.237700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>1.591300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.107900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>1.638900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.687500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.577300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.324500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.555500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.475200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.395600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.379700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.332500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.331100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>0.337700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.307800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>0.286100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.295400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.287100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.279000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>0.309400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.283200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>0.277900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.280400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>0.288800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.277400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165</td>\n",
              "      <td>0.286500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.297600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.298100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.308000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>185</td>\n",
              "      <td>0.305600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.288900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>195</td>\n",
              "      <td>0.291900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.284800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>205</td>\n",
              "      <td>0.278500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.293900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=213, training_loss=2.2850887517973852, metrics={'train_runtime': 6157.4986, 'train_samples_per_second': 0.557, 'train_steps_per_second': 0.035, 'total_flos': 4.238324877754368e+16, 'train_loss': 2.2850887517973852})"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tuner.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "pno2OmpnFGia",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pno2OmpnFGia",
        "outputId": "e476ab0b-762f-4748-b246-add036e991d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " What is langchain_google_genai?\n",
            "\n",
            "langchain_google_genai is a library for building large language models (LLMs) using the Google Cloud AI Platform. It provides a simple and easy-to-use interface for training and deploying LLMs, as well as a set of tools for managing and monitoring your LLMs.\n",
            "\n",
            "What are the benefits of using langchain_google_genai?\n",
            "\n",
            "langchain_google_genai offers a number of benefits, including:\n",
            "\n",
            "* Easy to use: langchain_google_genai provides a simple and easy-to-use interface for training and deploying LLMs, making it accessible to developers of all skill levels.\n",
            "* Scalable: langchain_google_genai can be scaled to handle large amounts of data and complex models, making it ideal for use in production environments.\n",
            "* Cost-effective: langchain_google_genai is a cost-effective solution for training and deploying LLMs, as it uses the Google Cloud AI Platform to provide a low-cost, high-performance environment for training and deploying LLMs.\n",
            "* Flexible: langchain_google_genai provides a flexible and customizable interface for training and deploying LLMs, allowing you to tailor the model to your specific needs.\n",
            "\n",
            "What are the requirements for using langchain_google_genai?\n",
            "\n",
            "To use langchain_google_genai, you will need to have a Google Cloud account and a Google Cloud AI Platform account. You will also need\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "text = \" What is langchain_google_genai\"\n",
        "device = 'cuda:0'\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "for k, v in inputs.items():\n",
        "    if v.dtype == torch.float:\n",
        "        inputs[k] = v.half().to(device)\n",
        "    else:\n",
        "        inputs[k] = v.to(device)\n",
        "with torch.amp.autocast('cuda'):\n",
        "    outputs = model.generate(**inputs, max_new_tokens=300)\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "SrXyluTjRPKa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrXyluTjRPKa",
        "outputId": "81ec21a5-acb4-4f50-a629-dd0fdd94aa29"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('fine-tuned-gemma/tokenizer_config.json',\n",
              " 'fine-tuned-gemma/special_tokens_map.json',\n",
              " 'fine-tuned-gemma/tokenizer.model',\n",
              " 'fine-tuned-gemma/added_tokens.json',\n",
              " 'fine-tuned-gemma/tokenizer.json')"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.save_pretrained(\"fine-tuned-gemma\")\n",
        "tokenizer.save_pretrained(\"fine-tuned-gemma\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "QIm4bTOaQY_D",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257,
          "referenced_widgets": [
            "e7eaa344466b4235b384b5449e6958f6",
            "e084a85c0fb845b58714cd509131de35",
            "cae22fc229914feba84c4d19047c21c2",
            "cfdd17ef2f0e4b2b855a525378bd2bcb",
            "fb0dfc0586cc4d94af3a65d95e1d62a7",
            "eaa6d1a260914e46bc9248479d91db51",
            "036a352496b34e3aa475edd7450f0942",
            "0b270a7696cd43268fb00688a63e0501",
            "b706af89b0de45df9b0188cb2fa19e2d",
            "115ecd671deb46ff82b9a06c39d0c938",
            "c08b7ad86fc24c3eae9a10a3ff563d8b"
          ]
        },
        "collapsed": true,
        "id": "QIm4bTOaQY_D",
        "outputId": "75eccc67-71ae-41b0-ac1a-03498b2450d0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e7eaa344466b4235b384b5449e6958f6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/3.54G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import HfApi\n",
        "api = HfApi()\n",
        "\n",
        "for file in os.listdir(r'/content/fine-tuned-gemma'):\n",
        "  api.upload_file(path_or_fileobj=f\"fine-tuned-gemma/{file}\", path_in_repo=f\"{file}\", repo_id=\"Prince-Dastan/gemma-2-2b-langchain-finetuned\", repo_type=\"model\",token=os.environ['HF_TOKEN'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ow6L9-ZPUP6c",
      "metadata": {
        "id": "Ow6L9-ZPUP6c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01f6c677eda246ff992f14ecebe87a17": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "036a352496b34e3aa475edd7450f0942": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0713788da3484b269e08756a369a4933": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c2c714e7d9b41329bd4a5181e4aa9bc",
            "placeholder": "​",
            "style": "IPY_MODEL_68973cb358854657a484417993f32ea7",
            "value": "Truncating train dataset: 100%"
          }
        },
        "0974972f26f84defb0f34c2c80a19b1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0b270a7696cd43268fb00688a63e0501": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "115ecd671deb46ff82b9a06c39d0c938": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e4b7f4cef584b37a6405edb84a658c0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "261f45473c2b491ea07f0b86944df9bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2b12caba0fd243b6a905b2c4474a5814": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59a556ace00448b78459353f4a7468b3",
            "placeholder": "​",
            "style": "IPY_MODEL_f805f7ba48cc4f3e9bf07342aaa3a166",
            "value": " 1143/1143 [00:00&lt;00:00, 49840.83 examples/s]"
          }
        },
        "2b4179498fe641cdb545a78de3ab0b8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4089e52294a347209477bab68e880e12": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "441e989ac7d3495aae2adf3db6817fab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df030c3d971149c2877d964bc944d13d",
            "placeholder": "​",
            "style": "IPY_MODEL_4089e52294a347209477bab68e880e12",
            "value": " 3/3 [01:02&lt;00:00, 17.54s/it]"
          }
        },
        "45c6ff81c276474dabe2d5af18320e12": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "466c5710fec04b20be86708dc54caea3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5787fa47a89a465e981aa7d7338affbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59a556ace00448b78459353f4a7468b3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a92604d4d5440f3925ad7265acdb86a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_70dc0892c2b04727b1183839ad84a7db",
              "IPY_MODEL_ba8817e8e85645b39192efcdbd217ded",
              "IPY_MODEL_b6cd2b731f0f4d3d98a8a3824cf6bd4c"
            ],
            "layout": "IPY_MODEL_6c3b443caaea406eaf60300a42223b54"
          }
        },
        "67e074de5665427bb27fb1948464cb83": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba5a4756e1f44eb3b7995c623e5c48d6",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0974972f26f84defb0f34c2c80a19b1d",
            "value": 3
          }
        },
        "68973cb358854657a484417993f32ea7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6c3b443caaea406eaf60300a42223b54": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70dc0892c2b04727b1183839ad84a7db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e4b7f4cef584b37a6405edb84a658c0",
            "placeholder": "​",
            "style": "IPY_MODEL_2b4179498fe641cdb545a78de3ab0b8d",
            "value": "Map: 100%"
          }
        },
        "8c2c714e7d9b41329bd4a5181e4aa9bc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "937d264403ed4852ab8560bbda45c210": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "990a82ec00084ab9a502f87e8c702226": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e4d641b82b0a4c5fa5c96343176c867a",
              "IPY_MODEL_67e074de5665427bb27fb1948464cb83",
              "IPY_MODEL_441e989ac7d3495aae2adf3db6817fab"
            ],
            "layout": "IPY_MODEL_f27b0e3777c74fe7aa460bb65cedadf3"
          }
        },
        "b0511d19bdac489aa4eb7787cbc6435a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b30f5a421ee7490192cec220f79e89b5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5791dd7c05847f6a5f15e4b51cf3a0a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6cd2b731f0f4d3d98a8a3824cf6bd4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_466c5710fec04b20be86708dc54caea3",
            "placeholder": "​",
            "style": "IPY_MODEL_b0511d19bdac489aa4eb7787cbc6435a",
            "value": " 1143/1143 [00:05&lt;00:00, 199.00 examples/s]"
          }
        },
        "b706af89b0de45df9b0188cb2fa19e2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ba5a4756e1f44eb3b7995c623e5c48d6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba8817e8e85645b39192efcdbd217ded": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01f6c677eda246ff992f14ecebe87a17",
            "max": 1143,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_261f45473c2b491ea07f0b86944df9bf",
            "value": 1143
          }
        },
        "c08b7ad86fc24c3eae9a10a3ff563d8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cae22fc229914feba84c4d19047c21c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b270a7696cd43268fb00688a63e0501",
            "max": 3540634326,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b706af89b0de45df9b0188cb2fa19e2d",
            "value": 3540634326
          }
        },
        "cfdd17ef2f0e4b2b855a525378bd2bcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_115ecd671deb46ff82b9a06c39d0c938",
            "placeholder": "​",
            "style": "IPY_MODEL_c08b7ad86fc24c3eae9a10a3ff563d8b",
            "value": " 3.54G/3.54G [02:24&lt;00:00, 25.3MB/s]"
          }
        },
        "d086003bb0124215b95beaa5c26ed45f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0713788da3484b269e08756a369a4933",
              "IPY_MODEL_dd5f9a2c028b4f1f9626687b713a02a7",
              "IPY_MODEL_2b12caba0fd243b6a905b2c4474a5814"
            ],
            "layout": "IPY_MODEL_b5791dd7c05847f6a5f15e4b51cf3a0a"
          }
        },
        "dd5f9a2c028b4f1f9626687b713a02a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45c6ff81c276474dabe2d5af18320e12",
            "max": 1143,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_937d264403ed4852ab8560bbda45c210",
            "value": 1143
          }
        },
        "df030c3d971149c2877d964bc944d13d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e084a85c0fb845b58714cd509131de35": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eaa6d1a260914e46bc9248479d91db51",
            "placeholder": "​",
            "style": "IPY_MODEL_036a352496b34e3aa475edd7450f0942",
            "value": "model.safetensors: 100%"
          }
        },
        "e4d641b82b0a4c5fa5c96343176c867a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b30f5a421ee7490192cec220f79e89b5",
            "placeholder": "​",
            "style": "IPY_MODEL_5787fa47a89a465e981aa7d7338affbc",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "e7eaa344466b4235b384b5449e6958f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e084a85c0fb845b58714cd509131de35",
              "IPY_MODEL_cae22fc229914feba84c4d19047c21c2",
              "IPY_MODEL_cfdd17ef2f0e4b2b855a525378bd2bcb"
            ],
            "layout": "IPY_MODEL_fb0dfc0586cc4d94af3a65d95e1d62a7"
          }
        },
        "eaa6d1a260914e46bc9248479d91db51": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f27b0e3777c74fe7aa460bb65cedadf3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f805f7ba48cc4f3e9bf07342aaa3a166": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb0dfc0586cc4d94af3a65d95e1d62a7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
