{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "id": "84d95532"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from dotenv import load_dotenv\n",
    "import transformers\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig, GemmaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "id": "04e753c5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "497226f8",
    "outputId": "035abf9f-60cd-4def-be4f-2f54e5d2ec74"
   },
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "id": "c4b933a0"
   },
   "outputs": [],
   "source": [
    "def loadData(file):\n",
    "    dbfile = open(file, 'rb')\n",
    "    db = pickle.load(dbfile)\n",
    "\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "id": "c856e44b"
   },
   "outputs": [],
   "source": [
    "dataset = loadData('LangDatasetBetter.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6AsfLoY8AVSt",
    "outputId": "933776c4-036a-4300-e3e6-ae33d4be6361"
   },
   "outputs": [],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "id": "x2nd5s-PAvat",
    "outputId": "2bda10cc-ded7-44cc-f95c-193dbf6c9d93"
   },
   "outputs": [],
   "source": [
    "dataset['google_genai']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "id": "-7lqAj6SEcOk"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n",
    "max_tokens = 4096\n",
    "\n",
    "def tokenize_len(text):\n",
    "    return len(tokenizer.tokenize(text))\n",
    "\n",
    "def split_by_class_sections(text):\n",
    "    pattern = r'\\n{2,}## Class Objects: (.+?)\\n'\n",
    "    parts = re.split(pattern, text)\n",
    "    it = iter(parts[1:]) \n",
    "    return list(zip(it, it))  \n",
    "\n",
    "final_chunks = []\n",
    "\n",
    "for i, (topic, full_doc) in enumerate(dataset.items(), start=1):\n",
    "    sections = split_by_class_sections(full_doc)\n",
    "    for j, (module, content) in enumerate(sections, start=1):\n",
    "        header = f\"### Instruction: Learn about the {topic} LangChain API.\\n\\n### Part {i} - Module:{module}(chunk{j})\\n\\n\"\n",
    "        full_text = header + content.strip()\n",
    "        if tokenize_len(full_text) <= max_tokens:\n",
    "            final_chunks.append({\"text\": full_text})\n",
    "        else:\n",
    "            words = full_text.split()\n",
    "            chunk = \"\"\n",
    "            for word in words:\n",
    "                chunk += word + \" \"\n",
    "                if tokenize_len(chunk) >= 3000:\n",
    "                    final_chunks.append({\"text\": chunk.strip()})\n",
    "                    chunk = \"\"\n",
    "            if chunk:\n",
    "                final_chunks.append({\"text\": chunk.strip()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F8iNQZpJS9iV",
    "outputId": "e39042bd-2411-4d4e-96c2-e0fec348f78a"
   },
   "outputs": [],
   "source": [
    "print(len(final_chunks))\n",
    "print(final_chunks[80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "id": "SCPTD78xYTV4"
   },
   "outputs": [],
   "source": [
    "data= open('LangDatasetChunked.pickle', 'wb')\n",
    "pickle.dump(final_chunks, data)\n",
    "data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "id": "lEs1HFoEB0c9"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "documentation = Dataset.from_list(final_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "id": "33639be0"
   },
   "outputs": [],
   "source": [
    "model_id = \"google/gemma-2-2b\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "990a82ec00084ab9a502f87e8c702226",
      "e4d641b82b0a4c5fa5c96343176c867a",
      "67e074de5665427bb27fb1948464cb83",
      "441e989ac7d3495aae2adf3db6817fab",
      "f27b0e3777c74fe7aa460bb65cedadf3",
      "b30f5a421ee7490192cec220f79e89b5",
      "5787fa47a89a465e981aa7d7338affbc",
      "ba5a4756e1f44eb3b7995c623e5c48d6",
      "0974972f26f84defb0f34c2c80a19b1d",
      "df030c3d971149c2877d964bc944d13d",
      "4089e52294a347209477bab68e880e12"
     ]
    },
    "id": "ad2df6a7",
    "outputId": "79231c40-e592-4a74-e27e-8831d87e57a0"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=False,\n",
    "    attn_implementation='eager',\n",
    "    use_cache=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "id": "e6b8938e"
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=['q_proj', \"o_proj\", \"k_proj\", \"v_proj\", 'gate_proj', 'up_proj', \"down_proj\"],\n",
    "    task_type='CAUSAL_LM',\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "5a92604d4d5440f3925ad7265acdb86a",
      "70dc0892c2b04727b1183839ad84a7db",
      "ba8817e8e85645b39192efcdbd217ded",
      "b6cd2b731f0f4d3d98a8a3824cf6bd4c",
      "6c3b443caaea406eaf60300a42223b54",
      "1e4b7f4cef584b37a6405edb84a658c0",
      "2b4179498fe641cdb545a78de3ab0b8d",
      "01f6c677eda246ff992f14ecebe87a17",
      "261f45473c2b491ea07f0b86944df9bf",
      "466c5710fec04b20be86708dc54caea3",
      "b0511d19bdac489aa4eb7787cbc6435a"
     ]
    },
    "id": "GV49HxwpCZ7K",
    "outputId": "34e1f76f-9074-4233-d4d1-d630302a181d"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "def tokenize(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=4096)\n",
    "\n",
    "tokenized_dataset = documentation.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bj9BgmY6KW6C",
    "outputId": "e61692bb-3d13-40ae-aff2-55719fdba9aa"
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable = 0\n",
    "    total = 0\n",
    "    for param in model.parameters():\n",
    "        num_params = param.numel()\n",
    "        total += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable += num_params\n",
    "    print(f\"Trainable parameters: {trainable:,}\")\n",
    "    print(f\"Total parameters: {total:,}\")\n",
    "    print(f\"Trainable ratio: {100 * trainable / total:.4f}%\")\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86,
     "referenced_widgets": [
      "d086003bb0124215b95beaa5c26ed45f",
      "0713788da3484b269e08756a369a4933",
      "dd5f9a2c028b4f1f9626687b713a02a7",
      "2b12caba0fd243b6a905b2c4474a5814",
      "b5791dd7c05847f6a5f15e4b51cf3a0a",
      "8c2c714e7d9b41329bd4a5181e4aa9bc",
      "68973cb358854657a484417993f32ea7",
      "45c6ff81c276474dabe2d5af18320e12",
      "937d264403ed4852ab8560bbda45c210",
      "59a556ace00448b78459353f4a7468b3",
      "f805f7ba48cc4f3e9bf07342aaa3a166"
     ]
    },
    "id": "08f14586",
    "outputId": "4ce5798e-0e5a-4615-c75f-8b0fd954d50c"
   },
   "outputs": [],
   "source": [
    "tuner = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=8,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=5e-5,\n",
    "        warmup_steps=50,\n",
    "        logging_steps=5,\n",
    "        fp16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        gradient_checkpointing=True,\n",
    "        output_dir=\"outputs2\"\n",
    "    ),\n",
    "    peft_config=lora_config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "id": "0fTv4Rs6FXVb"
   },
   "outputs": [],
   "source": [
    "os.environ['WANDB_DISABLED'] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "7iEFmqqoGahW",
    "outputId": "603c0084-7bd7-486a-ac5b-230d8a8a77e7"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project=\"lang-tuner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4bd99dc7",
    "outputId": "a4960a0e-cb58-4603-8a37-5e371daa92fb"
   },
   "outputs": [],
   "source": [
    "tuner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pno2OmpnFGia",
    "outputId": "e476ab0b-762f-4748-b246-add036e991d2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "text = \" What is langchain_google_genai\"\n",
    "device = 'cuda:0'\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "for k, v in inputs.items():\n",
    "    if v.dtype == torch.float:\n",
    "        inputs[k] = v.half().to(device)\n",
    "    else:\n",
    "        inputs[k] = v.to(device)\n",
    "with torch.amp.autocast('cuda'):\n",
    "    outputs = model.generate(**inputs, max_new_tokens=300)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SrXyluTjRPKa",
    "outputId": "81ec21a5-acb4-4f50-a629-dd0fdd94aa29"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"fine-tuned-gemma\")\n",
    "tokenizer.save_pretrained(\"fine-tuned-gemma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257,
     "referenced_widgets": [
      "e7eaa344466b4235b384b5449e6958f6",
      "e084a85c0fb845b58714cd509131de35",
      "cae22fc229914feba84c4d19047c21c2",
      "cfdd17ef2f0e4b2b855a525378bd2bcb",
      "fb0dfc0586cc4d94af3a65d95e1d62a7",
      "eaa6d1a260914e46bc9248479d91db51",
      "036a352496b34e3aa475edd7450f0942",
      "0b270a7696cd43268fb00688a63e0501",
      "b706af89b0de45df9b0188cb2fa19e2d",
      "115ecd671deb46ff82b9a06c39d0c938",
      "c08b7ad86fc24c3eae9a10a3ff563d8b"
     ]
    },
    "id": "QIm4bTOaQY_D",
    "outputId": "75eccc67-71ae-41b0-ac1a-03498b2450d0"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "\n",
    "for file in os.listdir(r'/content/fine-tuned-gemma'):\n",
    "  api.upload_file(path_or_fileobj=f\"fine-tuned-gemma/{file}\", path_in_repo=f\"{file}\", repo_id=\"Prince-Dastan/gemma-2-2b-langchain-finetuned\", repo_type=\"model\",token=os.environ['HF_TOKEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "id": "Ow6L9-ZPUP6c"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
